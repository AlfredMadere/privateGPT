{
    "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
    "authors": "Brian Lester^{*} Rami Al-Rfou Noah Constant\nGoogle Research\n{brianlester,rmyeid,nconstant}@google.com",
    "body": [
        {
            "section_title": "1 Introduction",
            "text": "With the wide success of pre-trained large language models, a range of\ntechniques has arisen to adapt these general-purpose models to downstream tasks.\nELMo (Peters et al., 2018) proposed freezing the pre-trained model and learning\na task-specific weighting of its per-layer representations. However, since GPT\n(Radford et al., 2018) and BERT (Devlin et al., 2019), the dominant adaptation\ntechnique has been model tuning (or \"fine-tuning\"), where all model parameters\nare tuned during adaptation, as proposed by Howard and Ruder (2018).\n\nMore recently, Brown et al. (2020) showed that prompt design (or \"priming\") is\nsurprisingly effective at modulating a frozen GPT-3 model's behavior through\ntext prompts. Prompts are typically composed of a task description and/or\nseveral canonical examples. This return to \"freezing\" pre-trained models is\nappealing, especially as model size continues to increase. Rather than requiring\na separate copy of the model for each downstream task, a single generalist model\ncan simultaneously serve many different tasks. Unfortunately, prompt-based\nadaptation has several key drawbacks. Task description is error-prone and\nrequires human involvement, and the effectiveness of a prompt is limited by how\nmuch conditioning text can fit into the model's input. As a result, downstream\ntask quality still lags far behind that of tuned models. For instance, GPT-3\n175B few-shot performance on SuperGLUE is 17.5 points below fine-tuned T5-XXL\n(Raffel et al., 2020) (71.8 vs. 89.3) despite using 16 times more parameters.\nSeveral efforts to automate prompt design have been recently proposed. Shin et\nal. (2020) propose a search algorithm over the discrete space of words, guided\nby the downstream application training data. While this technique outperforms\nmanual prompt design, there is still a gap relative to model tuning.\n\nLi and Liang (2021) propose \"prefix tuning\" and show strong results on\ngenerative tasks. This method freezes the model parameters and back-propagates\nthe error during tuning to prefix activations prepended to each layer in the\nencoder stack, including the input layer. Hambardzumyan et al. (2021) simplify\nthis recipe by restricting the trainable parameters to the input and output\nsubnetworks of a masked language model, and show reasonable results on\nclassifications tasks. In this paper, we propose prompt tuning as a further\nsimplification for adapting language models. We freeze the entire pre-trained\nmodel and only allow an additional k tunable tokens per downstream task to be\nprepended to the input text. This \"soft prompt\" is trained end-to-end and can\ncondense the signal from a full labeled dataset, allowing our method to\noutperform few-shot prompts and close the quality gap with model tuning (Figure\n1). At the same time, since a single pre-trained model is recycled for all\ndownstream tasks, we retain the efficient serving benefits of frozen models\n(Figure 2). While we developed our method concurrently with Li and Liang (2021)\nand Hambardzumyan et al. (2021), we are the first to show that prompt tuning\nalone (with no intermediate-layer prefixes or task-specific output layers) is\nsufficient to be competitive with model tuning. Through detailed experiments in\nsections 2-3, we demonstrate that language model capacity is a key ingredient\nfor these approaches to succeed. As Figure 1 shows, prompt tuning becomes more\ncompetitive with scale. We compare with similar approaches in Section 4.\nExplicitly separating task-specific parameters from the \"generalist\" parameters\nneeded for general language-understanding has a range of additional benefits. We\nshow in Section 5 that by capturing the task definition in the prompt while\nkeeping the generalist parameters fixed, we are able to achieve better\nresilience to domain shifts. In Section 6, we show that \"prompt ensembling\",\nlearning multiple prompts for the same task, can boost quality and is more\nefficient than classic model ensembling. Finally, in Section 7, we investigate\nthe interpretability of our learned soft prompts. In sum, our key contributions\nare:\n\n1. Proposing prompt tuning and showing its competitiveness with model tuning in\nthe regime of large language models.\n\n2. Ablating many design choices, and showing quality and robustness improve with\nscale.\n\n3. Showing prompt tuning outperforms model tuning on domain shift problems.\n\n4. Proposing \"prompt ensembling\" and showing its effectiveness."
        },
        {
            "section_title": "2 Prompt Tuning",
            "text": "Following the \"text-to-text\" approach of T5 (Raffel et al., 2020), we cast all\ntasks as text generation. Instead of modeling classification as the probability\nof an output class given some input, Pr(y|X), where X is a series of tokens and\ny is a single class label, we now model it as conditional generation, where Y is\na sequence of tokens that represent a class label. T5 models classification as\nPr_{\u03b8}(Y |X), parameterized by the weights, \u03b8, of the transformers (Vaswani et\nal., 2017) that make up its encoder and decoder. Prompting is the approach of\nadding extra information for the model to condition on during its generation of\nY . Normally, prompting is done by prepending a series of tokens, P, to the\ninput X, such that the model maximizes the likelihood of the correct Y ,\nPr_{\u03b8}(Y |[P; X]), while keeping the model parameters, \u03b8, fixed. In GPT-3, the\nrepresentations of the prompt tokens, P = {p_{1}, p_{2}, . . . , p_{n}}, are\npart of the model's embedding table, parameterized by the frozen \u03b8. Finding an\noptimal prompt thus requires the selection of prompt tokens, through either\nmanual search or non-differentiable search methods (Jiang et al., 2020; Shin et\nal., 2020). Prompt tuning removes the restriction that the prompt P be\nparameterized by \u03b8; instead the prompt has its own dedicated parameters, \u03b8_{P} ,\nthat can be updated. While prompt design involves selecting prompt tokens from a\nfixed vocabulary of frozen embeddings, prompt tuning can be thought of as using\na fixed prompt of special tokens, where only the embeddings of these prompt\ntokens can be updated. Our new conditional generation is now Pr_{\u03b8;\u03b8P} (Y |[P;\nX]) and can be trained by maximizing the likelihood of Y via backpropagation,\nwhile only applying gradient updates to \u03b8_{P} . Given a series of n tokens,\n{x_{1}, x_{2}, . . . , x_{n}}, the first thing T5 does is embed the tokens,\nforming a matrix X_{e} \u2208 R^{n\u00d7e} where e is the dimension of the embedding\nspace. Our soft-prompts are represented as a parameter P_{e} \u2208 R^{p\u00d7e}, where p\nis the length of the prompt. Our prompt is then concatenated to the embedded\ninput forming a single matrix [P_{e}; X_{e}] \u2208 R^{(p+n)\u00d7e} which then flows\nthough the encoder-decoder as normal. Our models are trained to maximize the\nprobability of Y , but only the prompt parameters P_{e} are updated."
        },
        {
            "section_title": "2.1 Design Decisions",
            "text": "There are many possible ways to initialize the prompt representations. The\nsimplest is to train from scratch, using random initialization. A more\nsophisticated option is to initialize each prompt token to an embedding drawn\nfrom the model's vocabulary. Conceptually, our soft-prompt modulates the frozen\nnetwork's behavior in the same way as text preceding the input, so it follows\nthat a word-like representation might serve as a good initialization spot. For\nclassification tasks, a third option is to initialize the prompt with embeddings\nthat enumerate the output classes, similar to the \"verbalizers\" of Schick and\nSch\u00fctze (2021). Since we want the model to produce these tokens in the output,\ninitializing the prompt with the embeddings of the valid target tokens should\nprime the model to restrict its output to the legal output classes. Another\ndesign consideration is the length of the prompt. The parameter cost of our\nmethod is EP, where E is the token embedding dimension and P is the prompt\nlength. The shorter the prompt, the fewer new parameters must be tuned, so we\naim to find a minimal length that still performs well."
        },
        {
            "section_title": "2.2 Unlearning Span Corruption",
            "text": "Unlike autoregressive language models like GPT-3, the T5 models we experiment\nwith use an encoder-decoder architecture and pre-train on a span corruption\nobjective. Specifically, T5 is tasked with \"reconstructing\" masked spans in the\ninput text, which are marked with unique sentinel tokens. The target output text\nconsists of all the masked content, separated by sentinels, plus a final\nsentinel. For instance, from the text \"Thank you for inviting me to your party\nlast week\" we might construct a pre-training example where the input is \"Thank\nyou \u27e8X\u27e9 me to your party \u27e8Y\u27e9 week\" and the target output is \"\u27e8X\u27e9 for inviting\n\u27e8Y\u27e9 last \u27e8Z\u27e9\". While Raffel et al. (2020) find this architecture and\npre-training objective more effective than traditional language modeling, we\nhypothesize that this setup is not a good fit for producing a frozen model that\ncan be readily controlled through prompt tuning. In particular, a T5 model\npre-trained exclusively on span corruption, such as T5.1.1, has never seen truly\nnatural input text (free of sentinel tokens), nor has it ever been asked to\npredict truly natural targets. In fact, due to the details of T5's span\ncorruption preprocessing, every pre-training target will begin with a sentinel.\nWhile this \"unnatural\" tendency to output sentinels is easy to overcome through\nfine-tuning, we suspect that it would be much harder to override through a\nprompt alone, as the decoder priors cannot be adjusted. Given these concerns, we\nexperiment with T5 models in three settings. (1) \"Span Corruption\": We use\npre-trained T5 off-the-shelf as our frozen model, and test its ability to output\nthe expected text for downstream tasks. (2) \"Span Corruption + Sentinel\": We use\nthe same model, but prepend all downstream targets with a sentinel, so as to\nmore closely resemble the targets seen in pretraining. (3) \"LM Adaptation\": We\ncontinue T5's self-supervised training for a small number of additional steps,\nbut using the \"LM\" objective discussed by Raffel et al. (2020); given a natural\ntext prefix as input, the model must produce the natural text continuation as\noutput. Crucially, this adaptation happens only once, producing a single frozen\nmodel that we can reuse for prompt tuning across any number of downstream tasks.\nThrough LM adaptation, we hope to \"quickly\" transform T5 into a model more\nsimilar to GPT-3, which always outputs realistic text, and is known to respond\nwell to prompts as a \"few-shot learner\". It is not obvious how successful this\nlate-stage transformation will be compared to pre-training from scratch, and it\nhas not been investigated previously to our knowledge. As such, we experiment\nwith various lengths of adaptation up to 100K steps."
        },
        {
            "section_title": "3 Results",
            "text": "Our frozen models are built on top of pre-trained T5 checkpoints of all sizes\n(Small, Base, Large, XL, XXL). We leverage the public T5.1.1 checkpoints, which\ninclude improvements over the original T5.(These improvements are (1) the\nremoval of all supervised data from pre-training, (2) adjustments to\nhyperparameters d_{model} and d_{ff}, and (3) the use of GeGLU (Shazeer, 2020)\nover ReLU (Nair and Hinton, 2010) activations.)\n\nOur \"default\" configuration, plotted with a green '\u00d7' ( ) throughout, uses an\nLM-adapted version of T5 trained for an additional 100K steps, initializes using\nclass labels (see Section 3.2), and uses a prompt length of 100 tokens. While\nthis is longer than the default 10-token prefix used by Li and Liang (2021), our\nmethod still uses fewer task-specific parameters, as we only tune the input\nlayer, as opposed to overwriting activations in all network layers. See Figure 4\nfor a detailed comparison. We will also see shortly that even much shorter\nprompts are viable as model size increases. We measure performance on the\nSuperGLUE benchmark (Wang et al., 2019a), a collection of eight challenging\nEnglish language understanding tasks.(The tasks are BoolQ (Clark et al., 2019),\nCB (De Marneff et al., 2019), COPA (Roemmele et al., 2011), MultiRC (Khashabi et\nal., 2018), ReCoRD (Zhang et al., 2018), RTE (Dagan et al., 2005; Bar-Haim et\nal., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), WiC (Pilehvar and\nCamacho-Collados, 2018), and WSC (Levesque et al., 2012).) We report metrics on\nthe development set associated with each dataset. Each of our prompts train on a\nsingle SuperGLUE task; there was no multi-task setup or mixing of training data\nacross tasks. We translate each SuperGLUE dataset into a text-to-text format\nfollowing Raffel et al. (2020), except that we omit the task names prepended to\ninputs indicating which SuperGLUE task an example belongs to. We train our\nprompts for 30,000 steps using T5's standard cross-entropy loss, with a constant\nlearning rate of 0.3 and a batch size of 32. Checkpoints are selected via early\nstopping on the development set, where the stopping metric is the default metric\nfor the dataset, or the average of metrics for datasets evaluated with multiple\nmetrics. All experiments were run in JAX (Bradbury et al., 2018) using the\nAdafactor optimizer (Shazeer and Stern, 2018) with weight decay 1e\u22125, \u03b2_{2}\ndecay 0.8, and parameter scaling off. The models were implemented in Flax (Heek\net al., 2020). More details are available in Appendix A."
        },
        {
            "section_title": "3.1 Closing the Gap",
            "text": "To compare our method with standard model tuning, we tune the public T5.1.1\ncheckpoints on SuperGLUE using the default hyperparameters specified in the T5\nlibrary (learning rate 0.001, and Adafactor optimizer with pre-training\nparameter states restored). We consider two baselines. (1) \"Model Tuning\": For\nan apples-to-apples comparison, we tune on each task separately, as in our\nprompt tuning setup.(To improve this baseline, we performed a sweep over the\nbatch size hyperparameter and selected 2^{16} tokens per batch.) (2) \"Model\nTuning (Multitask)\": We use T5's multi-task tuning setup to achieve a more\ncompetitive baseline.(The T5 SuperGLUE submission used a more complex setup,\nfirst mixing multi-task supervised data into pre-training, and then performing\nsingle-task fine-tuning. Since we use T5.1.1 throughout, this setup is\nunavailable, as the pre-training phase is fully self-supervised. We follow\nRaffel et al. (2020) in using 2^{20} tokens per batch and including DPR data in\nthe multi-task mixture, which is known to boost WSC task performance (Kocijan et\nal., 2019).) In this case, a single model is tuned on all tasks jointly, with a\ntext prefix indicating the task name. In Figure 1 (p. 1), we see that prompt\ntuning becomes more competitive with model tuning as scale increases. At the XXL\nsize (11 billion parameters), prompt tuning matches even the stronger multi-task\nmodel tuning baseline, despite having over 20,000 times fewer task-specific\nparameters. To compare with prompt design, we include GPT-3 few-shot performance\non the SuperGLUE dev split, as reported by Brown et al. (2020).(We also\nexperimented with using GPT-3's manual text prompts directly with our LM-adapted\nT5 checkpoints. However performance was far below GPT-3 for comparable model\nsizes. This may be due to differences in pre-training data and model\narchitecture, as well as T5's shorter sequence length.)\n\nFigure 1 shows that prompt tuning beats GPT-3 prompt design by a large margin,\nwith prompt-tuned T5-Small matching GPT-3 XL (over 16 times larger), and\nprompt-tuned T5-Large beating GPT-3 175B (over 220 times larger)."
        },
        {
            "section_title": "3.2 Ablation Study",
            "text": "Prompt Length We train prompts for each model size while varying the prompt\nlength in {1, 5, 20, 100, 150} and fixing other settings to our default\nconfiguration. Figure 3(a) shows that for most model sizes, increasing prompt\nlength beyond a single token is critical to achieve good performance. Notably,\nthe XXL model still gives strong results with a single-token prompt, suggesting\nthat the larger the model, the less conditioning signal is needed to achieve a\ntarget behavior. Across all models, increasing beyond 20 tokens only yields\nmarginal gains.(Going past 100 tokens appears mildly detrimental for larger\nmodels. A similar pattern of diminishing performance past a certain prefix\nlength is observed by Li and Liang (2021).)\n\nPrompt Initialization We ablate the effect of prompt initialization by training\nmodels at all sizes while fixing other hyperparameters to their default values.\nFor random initialization, we sample uniformly from the range [\u22120.5, 0.5]. When\ninitializing from sampled vocabulary, we restrict to the 5,000 most \"common\"\ntokens in T5's Sentence-Piece vocabulary (Kudo and Richardson, 2018), which is\nordered by likelihood in the pre-training corpus. For \"class label\"\ninitialization, we take the embeddings for the string representations of each\nclass in the downstream task and use them to initialize one of the tokens in the\nprompt. When a class label is multi-token, we average the token embeddings. At\nlonger prompt lengths, we often run out of class labels before we have\ninitialized all of the prompt tokens. In this case we fall back to our sampled\nvocab strategy to fill in the prompt.(T5's handling of the ReCoRD and WSC tasks\nrequires the model to generate short, free-form text. In these cases, we\ninitialize the prompts with words related to the task: commonsense, reasoning,\nreading, and comprehension for ReCoRD and commonsense, pronoun, and resolution\nfor WSC.)\n\nFigure 3(b) shows our ablation of initialization strategy across model sizes,\nwhere we find that the class based initialization performs best. At smaller\nmodel sizes, there are large gaps between the different initializations, but\nonce the model is scaled to XXL size, those differences disappear. With \"class\nlabel\" initialization, we observe that the class labels typically persist in the\nlearned prompts, such that the nearest token embeddings (in cosine distance)\nmatch the tokens used for initialization. Beyond this, we did not find our\nlearned prompts to be interpretable, similar to those of Shin et al. (2020). See\nSection 7 for details.\n\nPre-training Objective In Figures 3(c) and 3(d), we see pre-training objective\nhas a clear effect on prompt tuning quality. As hypothesized in Section 2.2,\nT5's default \"span corruption\" objective is not well-suited for training frozen\nmodels to be later conditioned by prompts. Intuitively, models pre-trained to\nread and write sentinel tokens are hard to apply directly to tasks of reading\nand writing text without sentinels. As seen in Figure 3(c), even the\n\"workaround\" of adding a sentinel to the downstream targets has little benefit.\nWhile LM adaptation adds value across all model sizes, we note our largest XXL\nmodel is the most forgiving and gives strong results even with span corruption.\nGiven the benefit of LM adaptation, we also explore how long of an adaptation is\nhelpful. Figure 3(d) shows that longer adaptation provides additional gains, up\nto 100K steps. This suggests that the \"transition\" from span corruption to a\nlanguage modeling objective is not a trivial change, and making an effective\nswitch takes an investment of training resources (10% of the steps of the\noriginal T5 pre-training). At the same time, as in our other ablations, we\nobserve that the XXL model is robust to even non-ideal configurations. At this\nsize, the gains from adaptation are quite modest. In the non-optimal \"span\ncorruption\" setting, we observe instability across model sizes, with the Small\nmodel outperforming the larger Base, Large, and XL models. On inspection, we\nfind that for many tasks, these mid-sized models never learn to output a legal\nclass label and thus score 0%. The two most common error modes are copying\nsubspans from the input and predicting an empty string. Furthermore, this poor\nperformance is not due to random variance in prompt tuning, as we observe low\nvariance across 3 runs for each size. These results indicate that using models\npre-trained with the \"span corruption\" objective can be unreliable, with only 2\nout of 5 models working well, whereas the LM adapated versions work reliably\nacross all model sizes. We have released T5 1.1 checkpoints adapted using the LM\nobjective for 100K steps for all model\nsizes.(https://github.com/google-research/ text-to-text-transfer-transformer/\nblob/main/released_checkpoints.md# lm-adapted-t511lm100k)"
        },
        {
            "section_title": "4 Comparison to Similar Approaches",
            "text": "In this section, we review recent work on learning continuous prompts, and draw\ncomparisons with our method. One important axis of comparison is the number of\ntask-specific parameters each method requires, as shown in Figure 4. Among\nmethods with learnable parameters, prompt tuning is the most parameter\nefficient, requiring less than 0.01% task-specific parameters for models over a\nbillion parameters.(To compare with prompt design, we count each token ID in the\nprompt as a parameter, and assume a prompt of between 500-2000 tokens to match\nthe GPT-3 setting. While this technique is by far the most parameter efficient,\nit comes at the cost of task quality.)\n\nLi and Liang (2021) propose \"prefix tuning\": learning a sequence of prefixes\nthat are prepended at every transformer layer. This is akin to learning\ntransformer activations that are fixed across examples at every network layer.\nIn contrast, prompt tuning uses a single prompt representation that is prepended\nto the embedded input. Beyond requiring fewer parameters, our approach allows\nthe transformer to update the intermediate-layer task representations, as\ncontextualized by an input example. Their work builds on GPT-2 (Radford et al.,\n2019) and BART (Lewis et al., 2020), while ours focuses on T5 and examines\nchanges in performance and robustness to design choices as model size increases.\nWhen using BART, prefix tuning includes prefixes on both the encoder and decoder\nnetwork, while prompt tuning only requires prompts on the encoder. Li and Liang\n(2021) also rely on a reparameterization of the prefix to stabilize learning,\nwhich adds a large number of parameters during training, whereas our\nconfiguration does not require this reparameterization and is robust across\nSuperGLUE tasks and model sizes.\n\nHambardzumyan et al. (2021) propose \"WARP\", where prompt parameters are added to\nthe input layer. This method works with masked language models, relying on a\n[MASK] token and a learnable output layer to project the mask to class logits.\nThis formulation restricts the model to producing a single output, limiting it\nto classification. Prompt tuning does not require any changes to the input or a\ntask-specific head. The performance of prompt tuning is also considerably closer\nto the strong performance of model tuning.\n\nLiu et al. (2021) propose \"P-tuning\" where learnable continuous prompts are\ninterleaved throughout the embedded input, using patterns based on human design.\nOur approach removes this complication by simply prepending the prompt to the\ninput. To achieve strong SuperGLUE results, P-tuning has to be used in\nconjunction with model tuning, that is, models jointly update both the prompt\nand the main model parameters, whereas our approach keeps the original language\nmodel frozen.(As another difference, P-tuning requires the addition of \"anchor\"\ntokens in the input (e.g. a question mark following the hypothesis in the RTE\ntask) to achieve strong performance, while prompt tuning leaves inputs\nuntouched.)\n\nQin and Eisner (2021) use \"soft words\" to learn prompts to extract knowledge\nfrom pre-trained LMs. Prompts are positioned in relation to the input based on\nhand-designed prompt prototypes, and a learned \u2206^{\u2113} _{i} parameter is included\nfor each layer, so parameter cost scales with model depth.\n\nLogeswaran et al. (2020) use a learnable prepended token to adapt transformer\nmodels to various tasks, but focus on small synthetic datasets designed to\naccommodate a compositional task representation, as opposed to larger real-world\ndatasets. Their base models are small transformers trained from scratch jointly\nwith the task representations, whereas we keep the base model frozen and\ninvestigate scaling laws using larger transformers. More generally, work on task\nprompts is closely aligned with work on \"adapters\" (Rebuffi et al., 2017;\nHoulsby et al., 2019), small bottleneck layers inserted between frozen\npre-trained network layers. Adapters offer another means of reducing\ntask-specific parameters, with Houlsby et al. (2019) achieving GLUE performance\nclose to full model tuning when freezing BERT-Large and only adding 2-4%\nadditional parameters. Pfeiffer et al. (2020) use multiple adapters in a\nmultilingual context to explicitly separate language understanding from task\nspecification, similar to our approach. A core difference between adapters and\nprompt tuning is how the approaches change model behavior. Adapters modify the\nactual function that acts on the input representation, parameterized by the\nneural network, by allowing the rewriting of activations at any given layer.\nPrompt tuning modifies behavior by leaving the function fixed and adding new\ninput representations that can affect how subsequent input is processed."
        },
        {
            "section_title": "5 Resilience to Domain Shift",
            "text": "By freezing the core language model parameters, prompt tuning prevents the model\nfrom modifying its general understanding of language. Instead, prompt\nrepresentations indirectly modulate the representation of the input. This\nreduces the model's ability to overfit to a dataset by memorizing specific\nlexical cues and spurious correlations. This restriction suggests that prompt\ntuning may improve robustness to domain shifts, where the distribution of inputs\ndiffers between training and evaluation. We investigate zero-shot domain\ntransfer on two tasks: question answering (QA) and paraphrase detection. For\nquestion answering, we use the MRQA 2019 shared task on generalization (Fisch et\nal., 2019). This task collects extractive QA datasets in a unified format and\ntests how models trained on \"in-domain\" datasets perform when evaluated on\n\"out-of-domain\" datasets. For our experiments, we train on SQuAD (Rajpurkar et\nal., 2016) and evaluate on each of the out-of-domain datasets.(We select\ncheckpoints based on SQuAD validation F1. The out-of-domain datasets are\nTextbookQA (Kembhavi et al., 2017), RACE (Lai et al., 2017), BioASQ\n(http://bioasq. org/), RE (Levy et al., 2017), DuoRC (Saha et al., 2018), and\nDROP (Dua et al., 2019).)\n\nTable 1 shows that prompt tuning outperforms model tuning on the majority of\nout-of-domain datasets, with a remarkable 12.5 point F1 gap between the two\napproaches on TextbookQA. We observe larger gains from prompt tuning in cases of\nlarger domain shifts (e.g. to Biomedical in BioASQ or to Textbooks in\nTextbookQA). Of the datasets where model tuning is better, we see that DROP\nshares a domain (Wikipedia) with SQuAD and is thus one of the smallest domain\ntransfers. As a second test of robustness to domain shift, we explore transfer\nbetween two paraphrase detection tasks from GLUE (Wang et al., 2019b). The first\ntask is QQP (Iyer et al., 2017), which asks if two questions from the community\nQ&A site Quora are \"duplicates\". The second task is MRPC (Dolan and Brockett,\n2005), which asks if two sentences drawn from news articles are paraphrases. We\ntest transfer in both directions (QQP \u21d4 MRPC). As before, we train on the\n\"in-domain\" task, select checkpoints using in-domain validation, and evaluate\nzero-shot on the \"out-of-domain\" task. Table 2 shows that training a lightweight\nprompt on the QQP data and evaluating on MRPC gives much better performance than\ntuning the entire model (+3.2 accuracy and +3.1 F1). The results are much closer\nin the other direction, with prompt tuning showing a small improvement in\naccuracy and a small drop in F1. These results support the view that model\ntuning may be over-parameterized and more prone to overfit the training task, to\nthe detriment of similar tasks in different domains."
        },
        {
            "section_title": "6 Prompt Ensembling",
            "text": "Ensembles of neural models trained from different initializations on the same\ndata are widely observed to improve task performance (Hansen and Salamon, 1990)\nand are useful for estimating model uncertainty (Lakshminarayanan et al., 2017).\nHowever, as model size increases, ensembling can become impractical. Beyond the\nspace required to store N models (e.g. 42 GiB for each copy of T5-XXL), there is\na substantial inference cost to running N distinct models, whether in parallel\nor in series. Prompt tuning provides a more efficient way to ensemble multiple\nadaptations of a pre-trained language model. By training N prompts on the same\ntask, we create N separate \"models\" for a task, while still sharing the core\nlanguage modeling parameters throughout. Beyond drastically reducing storage\ncosts, the prompt ensemble makes inference more efficient. To process one\nexample, rather than computing forward passes of N different models, we can\nexecute a single forward pass with a batch size of N, replicating the example\nacross the batch and varying the prompt. These savings mirror those seen for\nmulti-tasking in Figure 2. To demonstrate the viability of prompt ensembling, we\ntrain five prompts for each SuperGLUE task, using a single frozen T5-XXL model\nwith our default hyperparameters. We use simple majority voting to compute\npredictions from the ensemble. Table 3 shows that across all tasks, the ensemble\nbeats the single-prompt average and beats, or matches, the best individual\nprompt."
        },
        {
            "section_title": "7 Interpretability",
            "text": "An ideally interpretable prompt would consist of natural language that clearly\ndescribes the task at hand, explicitly asks the model for some result or action,\nand makes it easy to understand why the prompt elicited such behavior from the\nmodel. As prompt tuning works in the continuous embedding space rather than the\ndiscrete token space, interpreting prompts becomes more difficult. To test the\ninterpretability of our learned soft prompts, we compute the nearest neighbors\nto each prompt token from the frozen model's vocabulary. We use cosine distance\nbetween the vocabulary embedding vector and the prompt token representation as\nthe similarity metric. We observe that for a given learned prompt token, the\ntop-5 nearest neighbors form tight semantic clusters. For example, we see\nlexically similar clusters such as { Technology / technology / Technologies /\ntechnological / technologies }, as well as more diverse but still strongly\nrelated clusters such as { entirely / completely / totally / altogether / 100%\n}. The nature of these clusters suggests that the prompts are in fact learning\n\"word-like\" representations. We found that random vectors drawn from the\nembedding space do not show this sort of semantic clustering. When initializing\nthe prompts using the \"class-label\" strategy, we often find that the class\nlabels persist through training. Specifically, if a prompt token is initialized\nto a given label, that label is often among the learned token's nearest\nneighbors after tuning. When initializing with the \"Random Uniform\" or \"Sampled\nVocab\" methods, the class labels can also be found in the nearest neighbors of\nthe prompts; however they tend to appear as neighbors to multiple prompt tokens.\nThis suggests that the model is learning to store the expected output classes in\nthe prompts as reference, and initializing the prompt to outputs classes makes\nthis easier and more centralized. When examining longer prompts (e.g. size 100),\nwe often find several prompt tokens with the same nearest neighbors. This\nsuggests there is either excess capacity in the prompt, or that the lack of\nsequential structure in the prompt representation makes it difficult for the\nmodel to localize information to a specific position. While the learned prompts\ntaken as sequences show little interpretability, we do observe a high frequency\nof words like science, technology and engineering as the nearest neighbors for\nprompts trained on the BoolQ dataset and approximately 20% of the questions are\nin the \"Nature/Science\" category. While more investigation is needed, this\nsuggests that one role of the prompt may be to prime the model to interpret\ninputs in a specific domain or context (e.g. \"scientific\")."
        },
        {
            "section_title": "8 Conclusion",
            "text": "In this paper, we showed that prompt tuning is a competitive technique for\nadapting frozen pretrained language models to downstream tasks. On the popular\nSuperGLUE benchmark, its task performance rivals that of traditional model\ntuning, with the gap vanishing as model size increases. On zero-shot domain\ntransfer, we found that prompt tuning leads to improved generalization. This\nplausibly indicates that freezing general-purpose language understanding\nparameters and restricting downstream learning to a lightweight parameter\nfootprint can help to avoid overfitting to a specific domain. Beyond task\nquality metrics, we discussed the appeal of moving to frozen pre-trained models\nin terms of storage and serving costs. This move enables both efficient\nmulti-task serving, as well as efficient high-performing prompt ensembling.\nLooking forward, we believe that factoring out task-defining parameters as\ndistinct from general language-modeling parameters is an exciting step that\nopens up many avenues for new research."
        }
    ],
    "addenda": [
        {
            "section_title": "A Reproducibility",
            "text": ""
        },
        {
            "section_title": "A.1 Experimental Settings",
            "text": "We evaluate each GLUE and SuperGLUE dataset using the metric specified in the\nbenchmark. We reuse the evaluation code from the publicly available T5\nopen-source release to compute metrics.(https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/ master/t5/evaluation/metrics.py)\n\nFor the SQuAD and MRQA datasets, we evaluate using F1, one of the metrics used\nby the SQuAD benchmark, where partial answer spans are considered. Again, we use\nthe T5 open-source release for metric\ncalculation.(https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/ master/t5/evaluation/metrics.py#L151)\nAll of our models use T5 1.1 as the base frozen model, additional details and\npretrained checkpoints can be found on GitHub.^{1415}\n\nAll prompts for T5 Small and Base models were trained on 4 TPU v2 chips, while\nprompts for larger models were trained on 16 TPU v3 chips. Parameter counts for\neach prompt can be found in Table 4. Average runtimes until convergence can be\nfound in Table 5."
        },
        {
            "section_title": "A.2 Hyperparameter Search",
            "text": "This work used 77 hyperparameter search trials (40 for prompt tuning and 37 for\nsingle-task model tuning), and 3 training runs (with validation evaluation) for\neach baseline configuration and ablation setting, for a total of 195 runs for\nour main result and ablations. There were an additional 18 runs for the domain\nshift experiments and 24 extra runs to create the ensemble. Hyperparameter\nbounds can be found in Table 6. Hyperparameter tuning was done via manual tuning\nand settings were selected based on the SuperGLUE score. All experiments in this\nwork, outside of the hyperparameter being ablated, use our default configuration\nof 100K steps of LM Adapation, a prompt length of 100, and \"class-label\"\ninitialization. All graphs of our experimental results plot the mean and\nstandard deviation over 3 runs as computed by Seaborn (Waskom, 2021). Some\nsettings have such low variance that the standard deviation is hidden behind the\nline itself, such as \"Model Tuning (Multi-task)\" in Figure 1 and the Base,\nLarge, and XL prompts trained on the \"Span Corruption\" pretraining objective in\nFigure 3(b). Figure 4 also shows mean and standard deviation for the number of\nparameters each method uses as the prompt length varies from 1-100. The \"Prefix\nTuning (Train)\" curves appears to have no standard deviation because the\nparameter count is so strongly dominated by the cost of the reparameterization\nparameters that the standard deviation bands are occluded. For our experiments\non domain transfer, we report mean and standard deviation over 3 runs."
        },
        {
            "section_title": "A.3 Datasets",
            "text": "All datasets used are in English. For the GLUE^{16,17} and\nSuperGLUE(https://www.tensorflow.org/datasets/ catalog/super_glue) datasets, we\nused the training, validation, and test splits that ship with TensorFlow\nDatasets. We used version 1.0.0 for GLUE and 1.0.2 for SuperGLUE datasets. For\nSQuAD(https://www.tensorflow.org/datasets/\ncatalog/squad#squadv11_default_config) we used v1.1:3.0.0 from Tensorflow\nDatasets and follow the provided training, validation, and test splits. For the\nout-of-domain datasets we used the development splits distributed as part of the\nMRQA shared task.(https://github.com/mrqa/ MRQA-Shared-Task-2019#out-of-domain)\nDataset sizes can be found in Table 7. The label distributions for each dataset\ncan be found in Table 8 (BoolQ), Table 9 (CB), Table 10 (COPA), Table 11\n(MultiRC), Table 14 (RTE), Table 12 (WiC), Table 13 (WSC), Table 15 (MRPC) and\nTable 16 (QQP). The question answering datasets are extractive datasets with a\nvariety of answers, so there isn't a label distribution to report. Similarly,\nthe ReCoRD dataset is a multiple choice dataset where the model must predict the\nmasked out entity from a list of possible entities. Due to this formulation\nthere isn't a meaningful label distribution. We followed the open-source T5\npreprocessing procedure(https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/ master/t5/data/preprocessors.py) for\neach dataset, except that we omit the dataset prefix denoting which SuperGLUE\ndataset an example belongs to. For the SQuAD and MRQA datasets we used the T5\nSQuAD preprocessing code(https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/ master/t5/data/preprocessors.py#L264).\nBy following the T5 preprocessing and text-to-text format, we recast the WSC\ndataset as a text generation task. Instead of predicting whether a supplied\nreferent is correct for a highlighted span, our model predicts the correct\nreferent directly. As such, we can only learn from training examples where the\nreferent is correct, so WSC training data where the supplied referent is\nincorrect are omitted. No new data was collected for this work."
        }
    ],
    "abstract": "In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for\nlearning \"soft prompts\" to condition frozen language models to perform specific\ndownstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts\nare learned through back-propagation and can be tuned to incorporate signals\nfrom any number of labeled examples. Our end-to-end learned approach outperforms\nGPT-3's few-shot learning by a large margin. More remarkably, through ablations\non model size using T5, we show that prompt tuning becomes more competitive with\nscale: as models exceed billions of parameters, our method \"closes the gap\" and\nmatches the strong performance of model tuning (where all model weights are\ntuned). This finding is especially relevant because large models are costly to\nshare and serve and the ability to reuse one frozen model for multiple\ndownstream tasks can ease this burden. Our method can be seen as a\nsimplification of the recently proposed \"prefix tuning\" of Li and Liang (2021)\nand we provide a comparison to this and other similar approaches. Finally, we\nshow that conditioning a frozen model with soft prompts confers benefits in\nrobustness to domain transfer and enables efficient \"prompt ensembling.\"",
    "acknowledgements": "We thank Lucas Dixon, Waleed Ammar, Slav Petrov and Sebastian Ruder for comments\non an earlier draft, and the following people for helpful discussion: Colin\nRaffel, Adam Roberts, and Noam Shazeer. We thank Linting Xue for help with the\nLM adaptation training.",
    "references": "Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo\nMagnini, and Idan Szpektor. 2006. The second PASCAL recognising textual\nentailment challenge. In Proceedings of the second PASCAL challenges workshop on\nrecognising textual entailment, volume 6, pages 6-4. Venice.\n\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The\nfifth PASCAL recognizing textual entailment challenge. In TAC.\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary,\nDougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye\nWanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of\nPython+NumPy programs.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learners. In Advances in Neural\nInformation Processing Systems, volume 33, pages 1877-1901. Curran Associates,\nInc.\n\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins,\nand Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of\nnatural yes/no questions. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 2924-2936,\nMinneapolis, Minnesota. Association for Computational Linguistics.\n\nIdo Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising\ntextual entailment challenge. In Machine Learning Challenges Workshop, pages\n177-190. Springer.\n\nMarie-Catherine De Marneff, Mandy Simons, and Judith Tonhauser. 2019. The\nCommitmentBank: Investigating projection in naturally occurring discourse.\nProceedings of Sinn und Bedeutung 23.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of deep bidirectional transformers for language understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\n\nWilliam B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of\nsentential paraphrases. In Proceedings of the Third International Workshop on\nParaphrasing (IWP2005).\n\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and\nMatt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete\nreasoning over paragraphs. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1\n\n(Long and Short Papers), pages 2368-2378, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\n\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen.\n2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension.\nIn Proceedings of 2nd Machine Reading for Reading Comprehension (MRQA) Workshop\nat EMNLP.\n\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third\nPASCAL recognizing textual entailment challenge. In Proceedings of the\nACL-PASCAL workshop on textual entailment and paraphrasing, pages 1-9.\nAssociation for Computational Linguistics.\n\nKaren Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. WARP: Word-level\nAdversarial ReProgramming. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers), pages\n4921-4933, Online. Association for Computational Linguistics.\n\nL. K. Hansen and P. Salamon. 1990. Neural network ensembles. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 12(10):993-1001.\n\nJonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand\nRondepierre, Andreas Steiner, and Marc van Zee. 2020. Flax: A neural network\nlibrary and ecosystem for JAX.\n\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De\nLaroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Proceedings of the 36th\nInternational Conference on Machine Learning, volume 97 of Proceedings of\nMachine Learning Research, pages 2790-2799. PMLR.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning\nfor text classification. In Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages\n328-339, Melbourne, Australia. Association for Computational Linguistics.\n\nShankar Iyer, Nikhil Dandekar, and Kornel Csernai. 2017. First Quora dataset\nrelease: Question pairs.\n\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know\nwhat language models know? Transactions of the Association for Computational\nLinguistics, 8:423-438.\n\nA. Kembhavi, M. Seo, D. Schwenk, J. Choi, A. Farhadi, and H. Hajishirzi. 2017.\nAre you smarter than a sixth grader? textbook question answering for multi-modal\nmachine comprehension. In 2017 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5376-5384.\n\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth.\n2018. Looking beyond the surface: A challenge set for reading comprehension over\nmultiple sentences. In Proceedings of North American Chapter of the Association\nfor Computational Linguistics (NAACL).\n\nVid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas\nLukasiewicz. 2019. A surprisingly robust trick for the Winograd schema\nchallenge. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4837-4842, Florence, Italy. Association for\nComputational Linguistics.\n\nTaku Kudo and John Richardson. 2018. SentencePiece: A simple and language\nindependent subword tokenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 66-71, Brussels, Belgium. Association\nfor Computational Linguistics.\n\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE:\nLarge-scale ReAding comprehension dataset from examinations. In Proceedings of\nthe 2017 Conference on Empirical Methods in Natural Language Processing, pages\n785-794, Copenhagen, Denmark. Association for Computational Linguistics.\n\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple\nand scalable predictive uncertainty estimation using deep ensembles. In Advances\nin Neural Information Processing Systems, volume 30. Curran Associates, Inc.\n\nHector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The Winograd schema\nchallenge. In Thirteenth International Conference on the Principles of Knowledge\nRepresentation and Reasoning.\n\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-shot\nrelation extraction via reading comprehension. In Proceedings of the 21st\nConference on Computational Natural Language Learning (CoNLL 2017), pages\n333-342, Vancouver, Canada. Association for Computational Linguistics.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising\nsequence-to-sequence pretraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 7871-7880, Online. Association for\nComputational Linguistics.\n\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous\nprompts for generation. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers), pages\n\n4582-4597, Online. Association for Computational Linguistics.\n\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie\nTang. 2021. GPT understands, too. CoRR, abs/2103.10385.\n\nLajanugen Logeswaran, Ann Lee, Myle Ott, Honglak Lee, Marc'Aurelio Ranzato, and\nArthur Szlam. 2020. Few-shot sequence learning with transformers. CoRR,\nabs/2012.09543.\n\nVinod Nair and Geoffrey E. Hinton. 2010. Rectified linear units improve\nrestricted Boltzmann machines. In Proceedings of the 27th International\nConference on International Conference on Machine Learning, ICML'10, page\n807-814, Madison, WI, USA. Omnipress.\n\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,\nKenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word\nrepresentations. In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227-2237, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\n\nJonas Pfeiffer, Ivan Vuli\u00b4c, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X:\nAn Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 7654-7673, Online. Association for Computational Linguistics.\n\nMohammad Taher Pilehvar and Jose Camacho-Collados. 2018. WiC: 10,000 example\npairs for evaluating context-sensitive representations. CoRR, abs/1808.09121.\n\nGuanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with\nmixtures of soft prompts. In Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, pages 5203-5212, Online. Association for Computational\nLinguistics.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.\nImproving language understanding by generative pre-training.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language models are unsupervised multitask learners. OpenAI\nBlog.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of\ntransfer learning with a unified text-to-text transformer. Journal of Machine\nLearning Research, 21(140):1-67.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD:\n100,000+ questions for machine comprehension of text. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing, pages 2383-2392,\nAustin, Texas. Association for Computational Linguistics.\n\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning\nmultiple visual domains with residual adapters. In Advances in Neural\nInformation Processing Systems, volume 30. Curran Associates, Inc.\n\nMelissa Roemmele, Cosmin Adrian Bejan, and An-drew S Gordon. 2011. Choice of\nplausible alternatives: An evaluation of commonsense causal reasoning. In 2011\nAAAI Spring Symposium Series.\n\nAmrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan.\n2018. DuoRC: Towards complex language understanding with paraphrased reading\ncomprehension. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1683-1693, Melbourne,\nAustralia. Association for Computational Linguistics.\n\nTimo Schick and Hinrich Sch\u00fctze. 2021. Exploiting cloze-questions for few-shot\ntext classification and natural language inference. In Proceedings of the 16th\nConference of the European Chapter of the Association for Computational\nLinguistics: Main Volume, pages 255-269, Online. Association for Computational\nLinguistics.\n\nNoam Shazeer. 2020. GLU variants improve transformer. CoRR, abs/2002.05202.\n\nNoam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with\nsublinear memory cost. In Proceedings of the 35th International Conference on\nMachine Learning, volume 80 of Proceedings of Machine Learning Research, pages\n4596-4604. PMLR.\n\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer\nSingh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), pages 4222-4235,\nOnline. Association for Computational Linguistics.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In\nAdvances in Neural Information Processing Systems, volume 30, pages 5998-6008.\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel Bowman. 2019a. SuperGLUE: A stickier benchmark\nfor general-purpose language understanding systems. In Advances in Neural\nInformation Processing Systems, volume 32. Curran Associates, Inc.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.\nBowman. 2019b. GLUE: A multi-task benchmark and analysis platform for natural\nlanguage understanding. In the Proceedings of ICLR.\n\nMichael L. Waskom. 2021. seaborn: statistical data visualization. Journal of\nOpen Source Software, 6(60):3021.\n\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin\nVan Durme. 2018. ReCoRD: Bridging the gap between human and machine commonsense\nreading comprehension. CoRR, abs/1810.12885.",
    "images": {
        "Figure_1_block_18_89e795f6": {
            "source_page_num": 0,
            "caption": "Figure 1: Standard model tuning of T5 achieves strong performance, but requires\nstoring separate copies of the model for each end task. Our prompt tuning of T5\nmatches the quality of model tuning as size increases, while enabling the reuse\nof a single frozen model for all tasks. Our approach significantly outperforms\nfew-shot prompt design using GPT-3. We show mean and standard deviation across 3\nruns for tuning methods.",
            "box": [
                300.78302001953125,
                167.03863525390625,
                531.0662841796875,
                388.8856506347656
            ],
            "adjacent_neighbor": 1,
            "image_file": "images/Figure_1_block_18_89e795f6.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/4dc86ab1c415b20bbac5438f11edf5a6.png"
        },
        "Figure_2_block_25_e6e2f34f": {
            "source_page_num": 1,
            "caption": "Figure 2: Model tuning requires making a task-specific copy of the entire\npre-trained model for each downstream task and inference must be performed in\nseparate batches. Prompt tuning only requires storing a small task-specific\nprompt for each task, and enables mixed-task inference using the original\npretrained model. With a T5 \"XXL\" model, each copy of the tuned model requires\n11 billion parameters. By contrast, our tuned prompts would only require 20,480\nparameters per task-a reduction of over five orders of magnitude-assuming a\nprompt length of 5 tokens.",
            "box": [
                65.55156707763672,
                40,
                295.7903747558594,
                186.00662231445312
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Figure_2_block_25_e6e2f34f.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/fb145c172dcd01be465c5d0c3bbe8d5e.png"
        },
        "Figure_3_block_52_d1364b52": {
            "source_page_num": 4,
            "caption": "Figure 3: Ablations of various hyperparameters on prompt tuning performance\n(mean and stddev across 3 runs). In our \"default\" ( ) configuration, quality\nimproves stably with model size. Across all ablations, the largest (XXL) model\nis the most robust to hyperparameter choice. (a) Prompt length: Increasing to\n20+ tokens generally confers a large boost, but XXL performs well even with\nsingle-token prompts. (b) Prompt initialization: Random uniform initialization\nlags behind more \"advanced\" initializations using sampled vocabulary or class\nlabel embeddings, but the difference vanishes at XXL size. (c) Pre-training\nobjective: LM adaptation outperforms span corruption, even when a sentinel is\nadded to downstream task targets, but XXL works well with any method. (d) LM\nadaptation: Longer adaptation generally gives larger gains, but XXL is robust to\neven short adaptation.",
            "box": [
                65.507080078125,
                40,
                530.794677734375,
                367.9986572265625
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Figure_3_block_52_d1364b52.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/1787fc399123cf0afef892d61d0d140d.png"
        },
        "Figure_4_block_18_b7e40df7": {
            "source_page_num": 5,
            "caption": "Figure 4: Parameter usage of various adaptation techniques, fixing architecture\nto T5.1.1 and prompt/prefix length to 1-100 tokens (bands show mean and stddev).\nModel Tuning: All parameters are task-specific. Prefix Tuning: Activations are\ntuned in the prefix of each layer, requiring 0.1-1% task-specific parameters for\ninference, but more are used for training. WARP: Task parameters are reduced to\nunder 0.1% by only tuning input and output layers. Prompt Tuning: Only prompt\nembeddings are tuned, reaching under 0.01% for most model sizes. Prompt Design:\nOnly a sequence of prompt IDs (500-2000 tokens) is required.",
            "box": [
                301.14190673828125,
                40,
                531.1576538085938,
                250.39962768554688
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Figure_4_block_18_b7e40df7.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/991151e0b893c9019c5f93b9c41ed7c8.png"
        },
        "Table_1_block_8_1c66fb1f": {
            "source_page_num": 6,
            "caption": "Table 1: F1 mean and stddev for models trained on SQuAD and evaluated on\nout-of-domain datasets from the MRQA 2019 shared task. Prompt tuning tends to\ngive stronger zero-shot performance than model tuning, especially on datasets\nwith large domain shifts like TextbookQA.",
            "box": [
                297.998836517334,
                74.4723129272461,
                526.3236694335938,
                163.49563598632812
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Table_1_block_8_1c66fb1f.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/7c39fe484cfd6a95257528594003b077.png"
        },
        "Table_2_block_3_e1d04733": {
            "source_page_num": 7,
            "caption": "Table 2: Mean and stddev of zero-shot domain transfer between two paraphrase\ndetection tasks.",
            "box": [
                70.35299682617188,
                74.69535827636719,
                298.28944396972656,
                140.54562377929688
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Table_2_block_3_e1d04733.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/2b12e0a90ead66e5e08aa0d6f70aeb10.png"
        },
        "Table_3_block_10_5be56145": {
            "source_page_num": 7,
            "caption": "Table 3: Performance of a five-prompt ensemble built from a single frozen T5-XXL\nmodel exceeds both the average and the best among the five prompts.",
            "box": [
                298.28944396972656,
                74.5856704711914,
                526.2258911132812,
                180.75466918945312
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Table_3_block_10_5be56145.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/0c1ee566f3475fca948d8562a8daf104.png"
        },
        "Table_4_block_2_a118d29c": {
            "source_page_num": 13,
            "caption": "Table 4: Number of parameters used for various prompt lengths and T5 model\nsizes. Trainable parameters is the number of parameters in the prompt itself,\nwhile total parameters includes the prompt plus the original T5 parameters. The\nT5 parameters are frozen and shared across all tasks, and include the\nSentencePiece lookup table parameters. The final column is the percentage of\ntotal parameters that are trainable.",
            "box": [
                70.4729995727539,
                75.02851104736328,
                526.06640625,
                402.9996337890625
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Table_4_block_2_a118d29c.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/5a9873da6b5879f406c03097f3c0659b.png"
        },
        "Table_5_block_7_d7e1ca5e": {
            "source_page_num": 13,
            "caption": "Table 5: Mean and standard deviation of the runtime until convergence for the\nBoolQ dataset and various prompt lengths and model sizes. Convergence is defined\nas reaching a performance within 1% of the mean value for that model\nconfiguration. A few configurations have been omitted because their runtimes\nwere artificially extended due to preemption.",
            "box": [
                298.26970291137695,
                491.8565368652344,
                526.06640625,
                670.388671875
            ],
            "adjacent_neighbor": 2,
            "image_file": "images/Table_5_block_7_d7e1ca5e.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/366f47a2a136a69f1e55ecb173c1aac0.png"
        },
        "Table_6_block_2_16d0d46f": {
            "source_page_num": 14,
            "caption": "Table 6: Search space for each hyperparameter considered. Parameter Scaling\nrefers to the Adafactor setting where an update is scaled by the norm of the\nparameter it will be applied to. Warmup Steps is the number of steps before a\nlinearly increasing learning rate reaches the Learning Rate value, starting from\nzero. Decay Factor is the reduction in Learning Rate size that occurs every\n\"Steps per Decay\" steps.",
            "box": [
                70.39800262451172,
                75.93555450439453,
                298.232234954834,
                174.76565551757812
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Table_6_block_2_16d0d46f.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/b1a10e73bcc6dfa42d6619dfaa632e66.png"
        },
        "Table_7_block_5_091d1f1a": {
            "source_page_num": 14,
            "caption": "Table 7: Sizes for training, validation, and testing splits of each dataset\nused. ^{*}Following T5, our casting of WSC as a text generation problems means\nwe can only train on examples where the supplied referent is correct. This means\nour training dataset is smaller than the normal WSC training dataset, which has\n554 examples.",
            "box": [
                70.39800262451172,
                283.62353515625,
                298.232234954834,
                482.08062744140625
            ],
            "adjacent_neighbor": 2,
            "image_file": "images/Table_7_block_5_091d1f1a.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/a437ced9b790763ad2fcf4d3b2c65eab.png"
        },
        "Table_8_block_8_735721a9": {
            "source_page_num": 14,
            "caption": "Table 8: Label distribution for the BoolQ dataset.",
            "box": [
                70.39800262451172,
                566.96533203125,
                298.232234954834,
                616.045654296875
            ],
            "adjacent_neighbor": 5,
            "image_file": "images/Table_8_block_8_735721a9.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/d48a5b2bdf304211459aa576613e2f03.png"
        },
        "Table_9_block_11_b5cc7a24": {
            "source_page_num": 14,
            "caption": "Table 9: Label distribution for the CB dataset.",
            "box": [
                70.39800262451172,
                640.6201782226562,
                298.232234954834,
                684.70166015625
            ],
            "adjacent_neighbor": 8,
            "image_file": "images/Table_9_block_11_b5cc7a24.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/187ed0f6218c81652e28a126e8446bdf.png"
        },
        "Table_10_block_14_59e85e39": {
            "source_page_num": 14,
            "caption": "Table 10: Label distribution for the COPA dataset.",
            "box": [
                70.39800262451172,
                709.810302734375,
                298.232234954834,
                758.8916625976562
            ],
            "adjacent_neighbor": 11,
            "image_file": "images/Table_10_block_14_59e85e39.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/72f55d6c1b6b4e88b885d6087eb301fb.png"
        },
        "Table_11_block_17_73d1964e": {
            "source_page_num": 14,
            "caption": "Table 11: Label distribution for the MultiRC dataset.",
            "box": [
                298.232234954834,
                90.81430053710938,
                526.0664672851562,
                139.89462280273438
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Table_11_block_17_73d1964e.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/00ef8fbea89b9a3ace7292c886bfbbdb.png"
        },
        "Table_12_block_20_a4946fec": {
            "source_page_num": 14,
            "caption": "Table 12: Label distribution for the WiC dataset.",
            "box": [
                298.232234954834,
                194.88729858398438,
                526.0664672851562,
                243.96865844726562
            ],
            "adjacent_neighbor": 17,
            "image_file": "images/Table_12_block_20_a4946fec.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/8309318c06aa1b715084846f740fd10a.png"
        },
        "Table_13_block_23_edbc1bfb": {
            "source_page_num": 14,
            "caption": "Table 13: Label distribution for the WSC dataset. Following T5, we cast the WSC\ndataset to a free-form text generation task where the model generates the\nreferent to the highlighted span instead predicting if the supplied entity is\nthe correct referent of the highlighted span. Thus, we only use training data\nwhere the supplied referent is correct making our training label distribution\nfocused entirely on True.",
            "box": [
                298.232234954834,
                298.9613342285156,
                526.0664672851562,
                348.0416259765625
            ],
            "adjacent_neighbor": 20,
            "image_file": "images/Table_13_block_23_edbc1bfb.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/30a1f401f04e59b1da752e86470d8bbf.png"
        },
        "Table_14_block_26_936a0222": {
            "source_page_num": 14,
            "caption": "Table 14: Label distribution for the RTE dataset.",
            "box": [
                298.232234954834,
                486.7213134765625,
                526.0664672851562,
                535.8016967773438
            ],
            "adjacent_neighbor": 23,
            "image_file": "images/Table_14_block_26_936a0222.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/8b21a5451921aaa4e024461956dfbc2d.png"
        },
        "Table_15_block_29_1bbea29c": {
            "source_page_num": 14,
            "caption": "Table 15: Label distribution for the MRPC dataset.",
            "box": [
                298.232234954834,
                590.7943115234375,
                526.0664672851562,
                639.8756713867188
            ],
            "adjacent_neighbor": 26,
            "image_file": "images/Table_15_block_29_1bbea29c.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/c585944579c08f7af22d48065d726ab7.png"
        },
        "Table_16_block_32_e43bd926": {
            "source_page_num": 14,
            "caption": "Table 16: Label distribution for the QQP dataset.",
            "box": [
                298.232234954834,
                694.8682861328125,
                526.0664672851562,
                743.9486694335938
            ],
            "adjacent_neighbor": 29,
            "image_file": "images/Table_16_block_32_e43bd926.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/41263a1aa9c22b6115068c463304fdf5.png"
        }
    }
}