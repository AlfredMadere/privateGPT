{
    "title": "AUTOPEFT: Automatic Configuration Search for\nParameter-Efficient Fine-Tuning",
    "authors": "Han Zhou^{1,*}\nXingchen Wan^{2,*}\nIvan Vuli\u00b4c^{1}\nAnna Korhonen^{1}\n^{1}Language Technology Lab, University of Cambridge\n^{2}Machine Learning Research Group, University of Oxford\n{hz416, iv250, alk23}@cam.ac.uk\nxwan@robots.ox.ac.uk",
    "body": [
        {
            "section_title": "1 Introduction and Motivation",
            "text": "Pretrained language models (PLM) are used in downstream tasks via the standard\ntransfer learning paradigm, where they get fine-tuned for particular tasks\n(Devlin et al., 2019; Liu et al., 2019b).\n\nThis achieves state-of-the-art results in a wide spectrum of NLP tasks, becoming\na prevalent modelling paradigm in NLP (Raffel et al., 2020). Fine-tuning the\nPLMs typically requires a full update of their original parameters (i.e. the\nso-called full-model fine-tuning (FFT)); however, this is 1) computationally\nexpensive and also 2) storage-wise expensive as it requires saving a separate\nfull model copy for each task-tuned model. With the ever-growing size of the\nPLMs (Brown et al., 2020; Sanh et al., 2022), the cost of full model FT becomes\na major bottleneck, due to its increasing demands as well as computational (time\nand space) non-efficiency. Parameter-Efficient Fine-Tuning (PEFT) delivers a\nsolution for alleviating the issues with full-model FT (Houlsby et al., 2019).\nBy freezing the majority of pretrained weights of PLMs, PEFT approaches only\nupdate a small portion of parameters for efficiently adapting the PLM to a new\ndownstream task. Recent studies have shown that PEFT can achieve competitive\ntask performance while being modular, adaptable, and preventing catastrophic\nforgetting in comparison to traditional FFT (Wang et al., 2022; Pfeiffer et al.,\n2023).\n\nRecent developments have created diverse PEFT modules with distinctive\ncharacteristics (Pfeiffer et al., 2020b; Li and Liang, 2021), with one of the\ntwo main aims in focus: 1) to improve task performance over other PEFT\napproaches while maintaining the same parameter budget as the competitor PEFT\nmethods; or 2) to maintain task performance while reducing the parameter budget\nneeded. Existing PEFT modules, optimising for one of the two aims, have been\nsuccessfully applied to transfer learning tasks (Chen et al., 2022b; Pfeiffer et\nal., 2022). However, different tasks, with different complexity, show distinct\nsensitivity to the allocated parameter budget and even to the chosen PEFT\napproach (He et al., 2022). At the same time, most PEFT applications are limited\nto a single PEFT architecture (e.g. serial adapters, prefix-tuning) with fixed\ndecisions on its components (e.g. hidden size dimensionality, insertion layers)\nresulting in potentially suboptimal PEFT configurations across many tasks.\nTherefore, in this work, we propose a new, versatile and unified framework that\nautomatically searches for improved and task-adapted PEFT configurations, aiming\nto effectively balance between the two (often colliding goals) of improving\nperformance and keeping the desired low parameter budget for PEFT.\n\nWhile recent research has started exploring more dynamic PEFT configurations,\nthe prior studies remain limited across several dimensions, including how they\ndefine the configuration search space. Namely, they typically focus only on a\nsingle PEFT architecture (e.g. adapters) or their simple combinations, or a\nsingle property (e.g. insertion layers - where to insert the module); the\nreaders are referred to a short overview later in \u00a73. Here, we propose a unified\nand more comprehensive framework for improved configuration search. It covers\nmultiple standard PEFT modules (serial adapters, parallel adapters, and\nprefix-tuning) as building blocks, combined with the critical parameter\nbudget-related decisions: the size of each constituent module and the insertion\nlayers for the modules.\n\nOur defined comprehensive search space is huge; as a consequence, traversing it\neffectively and efficiently is extremely challenging. To enable search over the\nlarge configuration space, we thus propose the novel AUTOPEFT framework. It\nautomatically configures multiple PEFT modules along with their\nefficiency-oriented design decisions, relying on a high-dimensional Bayesian\noptimisation (BO) approach. Crucially, within the search space, we propose a\nmulti-objective optimisation which learns to simultaneously balance between\nmaximising the searched configurations' task performance and parameter\nefficiency. We conduct extensive experiments on the standard GLUE and SuperGLUE\nbenchmarks (Wang et al., 2018, 2019). We first study the transferability of the\nAUTOPEFT-searched architecture by running AUTOPEFT on a single task with a\nlow-fidelity proxy (aiming to reduce computational cost), followed by\ntransferring the found architecture to other tasks. Experimental results show\nthat this architecture can outperform existing PEFT baselines while achieving\non-par performance with the standard FFT. Further slight gains can be achieved\nwith a larger computation budget for training, where we run AUTOPEFT per each\nsingle task to find a task-adapted PEFT configuration. As demonstrated in Figure\n1, AUTOPEFT is able to find configurations that offer a solid trade-off between\ntask performance and parameter efficiency, even outperforming FFT. We also\nprovide ablation studies over the search space, validating that the AUTOPEFT\nframework is versatile and portable to different search spaces.\n\nContributions. 1) We propose the AUTOPEFT search space containing diverse and\nexpressive combinations of PEFT configurations from three representative PEFT\nmodules as foundational building blocks and the binary decisions concerning\nTransformer layers for inserting these modules as searchable dimensions. 2) To\nnavigate the vast AUTOPEFT search space and to discover a set of transferable\nPEFT configurations that optimally trade performance against cost across various\nparameter ranges in a single run, we further propose an effective search method\nbased on multi-dimensional Bayesian optimisation. 3) We demonstrate that the\none-time search cost of AUTOPEFT is cheap, and AUTOPEFT yields task-shareable\nconfigurations, outperforming existing PEFT modules while being transferable\nacross tasks. The AUTOPEFT framework can also be easily extended to other and\nnew PEFT modules."
        },
        {
            "section_title": "2 ^{AUTO}PEFT Framework",
            "text": ""
        },
        {
            "section_title": "2.1 Designing the AUTOPEFT Search Space",
            "text": "Inspired by the success of neural architecture search (NAS), we similarly start\nby designing a large and expressive configuration space. We additionally provide\nthe motivation behind each decision to include a particular module and its\ncomponents in the configuration space, along with a mathematical formulation.\n\nThe search space is known to be one of the most important factors in the\nperformance of the configurations to be discovered subsequently (Ru et al.,\n2020; Xie et al., 2019; Li and Talwalkar, 2019; Dong and Yang, 2020; Yang et\nal., 2020). In order to simultaneously maximise task performance along with\nparameter efficiency, it is necessary to first define a 'parameter-reducible'\nsearch space, where each dimension within the space potentially contributes to\nreducing the parameter budget. Similarly, each dimension might potentially bring\na positive impact on the task performance without introducing redundancy in the\nspace (Wan et al., 2022). Therefore, as shown in Figure 2, we propose the search\nspace with representative PEFT modules, as follows, spanning a plethora of\n(non-redundant) configurations.\n\nPEFT Modules. Inspired by common practices in NAS of using known well-performing\nmodules as building blocks, we include three distinctive PEFT designs to\nefficiently adapt different forwarding stages of hidden states in the PLM\nlayers. We combine Serial Adapters (SA), Parallel Adapters (PA), and\nPrefix-Tuning (PT) as the three representative modules in the search space as\nthe building blocks, where the PT module adapts the multi-head attention layer,\nand SA and PA interact with the FFN layer (Figure 2). Each configuration makes\ntwo decisions on each of the PEFT modules in the insertion layer: the binary\ndecision on whether it is 'switched' on or off, and, when it is switched on, its\nactual module size (see the next paragraph). As we empirically validate later,\nthe resultant search space spanned by the building blocks is extremely\nexpressive and flexible, and enables the discovery of configurations that\noutscore any of the individual building blocks.\n\nSize. Previous studies show that PEFT methods are highly sensitive to the number\nof tunable parameters: adaptively setting their capacity in accordance with the\ntarget task is therefore essential for achieving good performance (Chen et al.,\n2022a). The number of tunable parameters depends on each particular module. The\nadditional parameters introduced by both SA and PA are dominated by their\nbottleneck dimension D. Similarly, the size of the PT module is defined by its\nprefix length L_{PT}. Thus, we introduce a searchable dimension for each of\nD_{SA}, D_{PA}, and L_{PT} whose possible values span from 0, which indicates\nthe module is 'switched off' or disabled, to D_{h}, where D_{h} is the\ndimensionality of the output embedding of the PLM (e.g. D_{h}=768 for\nBERT_{base}).\n\nInsertion Layers. Prior work has also shown that different layers in the PLMs\nstore different semantic information (Vuli\u00b4c et al., 2020), where the higher\nlayers produce more task-specific and contextualised representations (Tenney et\nal., 2019). Therefore, adapting all layers may potentially result in overfitting\nto the target task while being suboptimal. We then introduce another set of\nsearchable dimensions which control the 'insertion' decision at each layer l_{i}\n- the aim is to search for configurations that are high-performing yet\nparsimonious in inserting PEFT modules.\n\nCombining PEFT Modules. The SA module and the PA module share a bottleneck\narchitecture. The SA receives hidden states from the FFN output as its inputs,\nadapts it with a down-projection matrix W^{down} SA \u2208 R^{Dh\u00d7DSA}, which is\nfollowed by a non-linear activation function, and an up-projection matrix W^{up}\n_{SA} \u2208 R^{DSA\u00d7Dh}:\n\nf_{SA}(h) = ReLU(hW^{down} SA )W^{up} _{SA}. (1)\n\nPA, on the other hand, receives its inputs from hidden states before the FFN\nlayer with the same formulation:\n\nf_{PA}(x) = ReLU(xW^{down} PA )W^{up} _{PA}. (2)\n\nTherefore, it is able to act in parallel with the SA without interference. Note\nthat the FFN hidden states h = F(x) contain the task-specific bias learned in\nits pretrained weights. Therefore, by combining SA with PA, the following\ncomposition of functions is achieved:\n\nf_{SAPA}(x) =ReLU(F(x)W^{down} SA )W^{up} SA +ReLU(xW^{down} PA )W^{up} _{PA}.\n(3)\n\nThe final composition should provide an effective adaptation to both\nbias-influence hidden states and the original inputs before the pretrained FFN\nlayer.(The PA module also acts as the low-rank reparametrization of the learned\nSA together with the frozen FFN layer to further match the intrinsic\ndimensionality of the target task.)\n\nFurther, applying PEFT modules to interact both with FFNs and multi-head\nattention should have a positive impact on task performance (Mao et al., 2022;\nHe et al., 2022). PT learns two prefix vectors, P_{k} and P_{v} \u2208 R^{LPT\u00d7Dh},\nthat are concatenated with the original multi-head attention's key and value\nvectors, which efficiently adapts the multi-head attention layer to fit the\ntarget task. We thus finally combine the SA and the PA (i.e. SAPA from above)\nwith PT. In sum, the overview of the dimensions spanning the final configuration\nspace is provided in Figure 2. The combination of the different 'configuration\ndimensions' outlined above gives rise to a total of e.g. 5,451,776 possible\nconfigurations with BERT_{base} and \u223c 3\u00d710^{10} configurations with\nRoBERTa_{large} (i.e. the number of configurations is\n2^{|l|}\u00d7|D_{SA}|\u00d7|D_{PA}|\u00d7|L_{PT}|). While a large search space is crucial for\nexpressiveness and to ensure that good-performing configurations are contained,\nit also increases the difficulty for search strategies to navigate the search\nspace well while remaining sample- and thus computationally efficient.\nFurthermore, in the PEFT setting, we are also often interested in discovering a\nfamily of configurations that trade off between performance and efficiency for\ngeneral application in various scenarios with different resource constraints,\nthus giving rise to a multi-objective optimisation problem where we\nsimultaneously aim to maximise performance while minimising costs. In what\nfollows, we propose a search framework that satisfies all those criteria."
        },
        {
            "section_title": "2.2 Pareto-Optimal Configuration Search",
            "text": "The ultimate goal of AUTOPEFT is to discover promising PEFT configuration(s)\nfrom the expressive search space designed in \u00a72.1, which is itself challenging.\nIn this paper, we focus on an even more challenging but practical goal: instead\nof aiming to find a single, best-performing PEFT configuration, we aim to\ndiscover a family of Pareto-optimal PEFT configurations that trade performance\nagainst parameter-efficiency (or parameter cost) optimally: one of the most\nimpactful use cases of PEFT is its ability to allow fine-tuning of massive\nlanguage models even with modest computational resources, and thus we argue that\nsearching Pareto-optimal configurations is key as it allows tailored user- and\nscenario-specific PEFT deployment depending on the computational budget.\n\nTo this end, we adopt a Bayesian optimisation (BO) approach, illustrated in\nFigure 3. On a high level, BO consists of a surrogate model that sequentially\napproximates the objective function based on the observations so far, and an\nacquisition function, for exploitation-exploration trade-off that is optimised\nat each iteration to actively select the next configuration to evaluate. For a\ndetailed overview of BO, we refer the readers to Garnett (2023) and Frazier\n(2018). We argue that BO is particularly desirable, as 1) BO is sample-efficient\nand zeroth-order. It treats the model as a black box and requires no\ndifferentiable objectives, allowing cost-efficient optimisation without assuming\nstructures of the objectives nor the model itself, and BO also has proven\nsuccess in NAS and automated machine learning in general (Snoek et al., 2012;\nWhite et al., 2021a; Ru et al., 2021; Kandasamy et al., 2018); 2) BO may\ngeneralise to the multi-objective setup in a search space-agnostic manner while\nthe competing methods, such as supernet-based NAS methods, are typically used to\ndiscover a single best-performing configuration (Eriksson et al., 2021;\nIzquierdo et al., 2021); and 3) BO is more parallelisable and during search, its\nlargest memory use, which is particularly important for PEFT given its main\npromise on parameter efficiency, is upper-bounded by the largest PEFT\nconfiguration in the search space. Competing methods such as those relying on\nover-parameterised supernets typically involve a much larger memory burden.\n\nAdapting BO to the high-dimensional and combinatorial AUTOPEFT search space is\nnon-trivial. To address the challenges, we adopt the SAAS-GP (Eriksson and\nJankowiak, 2021) model as the surrogate function: SAAS-GP places strong,\nsparsity-inducing priors to alleviate the difficulty in modelling\nhigh-dimensional data by assuming that despite the high nominal dimensionality,\nthe effective dimensionality is much lower - this assumption is shown to be the\ncase in NAS (Wan et al., 2022), and we expect similar findings in our particular\ncase. For the acquisition function, we use the noisy expected hypervolume\nimprovement (NEHVI) (Daulton et al., 2021) to handle the multi-objective\nsetting. Lastly, we additionally use low-fidelity approximations, a popular\nlow-cost performance estimation strategy in NAS (Elsken et al., 2019), to manage\nthe search cost: at search-time, instead of fine-tuning each candidate PEFT\nconfiguration in full, we only fine-tune with a much smaller number of\niterations (5% of full) - this is possible as we are only interested in the\nrelative ranking (rather than the performance itself) of the different\nconfigurations during the search. Consistent with NAS literature, we also find\nthe low-fidelity estimate to provide a reliable ranking, with the\nbest-performing configurations in low fidelity also performing the best under\nfine-tuning with the full iterations. As we will show in \u00a75, using the\nlow-fidelity search pipeline, in combination with the strong transferability of\nthe discovered configurations, AUTOPEFT only incurs an additional one-off, 1.9%\nof the total GLUE fine-tuning cost, but delivers significant performance gains."
        },
        {
            "section_title": "3 Related Work",
            "text": "PEFT Methods in NLP. Standard PEFT methods can be divided into two main groups\n(Pfeiffer et al., 2023). 1) Some methods fine-tune a small portion of pretrained\nparameters (Zhao et al., 2020; Guo et al., 2021). For instance, Ben Zaken et al.\n(2022) propose to fine-tune the PLM's bias terms, while Sung et al. (2021) and\nAnsell et al. (2022) fine-tune sparse subnetworks withing the original PLM for a\nparticular task. 2) Other methods fine-tune an additional set of parameters (Liu\net al., 2022). Since there is no interference with the pretrained parameters,\nthis class of PEFT modules, besides offering strong task performance, is\narguably more modular; we thus focus on this class of PEFT methods in this work.\nThe original adapter modules (Houlsby et al., 2019; Pfeiffer et al., 2020b) have\na bottleneck serial architecture which can be inserted into every Transformer\nlayer, see Figure 2. LoRA (Hu et al., 2022a) assumes the low-rank intrinsic\ndimensionality of the target task and performs low-rank updates (Mahabadi et\nal., 2021). Li and Liang (2021) propose the Prefix-Tuning method that appends a\nlearnable vector to the attention heads at each Transformer layer. Similarly,\nprompt-tuning (Lester et al., 2021) only appends this vector to the input\nembedding. UniPELT (Mao et al., 2022) integrates multiple PEFT modules with a\ndynamic gating mechanism. He et al. (2022) provide a unified formulation of\nexisting PEFT modules and propose a parallel adapter module, along with a\ncombined 'Mix-and-Match Adapter (MAM)' architecture that blends parallel\nadapters and prefix-tuning. Wang et al. (2022) propose the\nmixture-of-adaptations (AdaMix) combined architecture that leverages weight\naveraging for a mixture of adapters.\n\nOptimising Parameter Efficiency in PEFT. Recent work further aims to optimise\nthe parameter efficiency of existing PEFT modules while maintaining task\nperformance. The standard approach is to insert (typically serial) adapters into\nall Transformer layers, which still requires a sizeable parameter budget. R\u00fcckl\u00e9\net al. (2021) address this question by performing random dropout of adapters\nfrom lower-level layers, displaying only a small decrease in task performance.\nAdaptable Adapters (AA) (Moosavi et al., 2022) generalise this idea by learning\ngates that switch on or off adapters in particular Transformer layers. Neural\nArchitecture Search (NAS) methods aim to automate the design of neural net\narchitectures themselves, and NAS has seen great advances recently, with\nperformance often surpassing human expert-designed architectures in various\ntasks (Zoph and Le, 2017; Ren et al., 2021; Elsken et al., 2019). Concerning NLP\ntasks and PEFT, Hu et al. (2022b) propose S_{3}PET, which adapts Differentiable\nArchitecture Search (DARTS) (Liu et al., 2019a) to learn the positions for\ninserting the PEFT modules. Concurrent works (Valipour et al., 2022; Zhang et\nal., 2023) also approach the same problem by dynamic budget allocation\nmechanisms on a single PEFT module within a limited search space. This field\nstill lacks a compact solution for automatically configuring a complex space of\nPEFT modules (Chen et al., 2023).\n\nOur method, discussed in detail in \u00a72, offers a spectrum of advantages over\nrelated PEFT works. Relying on multi-objective optimisation, unlike DARTS, we\ncan automatically discover a family of configurations at different parameter\nefficiency levels in a single search run, effectively balancing between task\nperformance and parameter efficiency, without the need to set the 'parameter\nbudget' in advance; similarly, we enable an automatic search over multiple\nconstituent modules over the desirable range of parameter budget and effective\nlayers, whereas previous work can only support one architecture per each search\nrun. Further, previous work indicated that weight-sharing NAS such as DARTS may\nsuffer with the reliability of prediction (White et al., 2021b), large memory\nusage due to supernet construction (which could be particularly problematic for\nPEFT given the emphasis on memory efficiency), and, as discussed in \u00a72, its\nsuccess often hinges heavily on the design of the actual search space. While\nweight-sharing NAS is often perceived to be more computationally efficient, as\nwe discussed in \u00a72.2 and will show empirically in \u00a75, AUTOPEFT can be similar,\nif not more, efficient in discovering effective PEFT configurations even in\nterms of search costs while arguably more parameter-efficient."
        },
        {
            "section_title": "4 Experimental Setup",
            "text": "Evaluation Data. We follow prior PEFT research and base our evaluation on the\nstandard and established GLUE and SuperGLUE benchmarks. For GLUE, we include 4\ntypes of text classification tasks, including linguistic acceptability: CoLA;\nsimilarity and paraphrase: STS-B, MRPC, QQP; sentiment analysis: SST-2; natural\nlanguage inference: RTE, QNLI, MNLI. We exclude WNLI following previous work\n(Houlsby et al., 2019; Mao et al., 2022). We also include CB, COPA, WiC, and\nBoolQ from SuperGLUE to further validate the transferability of AUTOPEFT-found\nconfiguration across different tasks and datasets.\n\nBaselines. We compare the performance of the AUTOPEFT-found configurations to\nthe standard full model FT and each individual PEFT module (SA, PA, PT) from the\nsearch space used in their default setup from respective original work. We also\ncompare with the LoRA module, to provide a comparison to low-rank decomposition\nmethods. In order to provide comparisons with recently proposed methods that\nalso integrate multiple PEFT modules (see \u00a73), we further include the UniPELT\nand the MAM adapter in their default settings. We reproduce AdaMix for a\ncomparison to a mixture of homogeneous adaptations. In ablations on insertion\nlayers, we also include the Adaptable Adapter (AA) as a baseline that proposes a\ndifferentiable gate learning method to select the insertion layer for PEFT\nmodules (i.e. serial adapters originally).\n\nImplementation Details. Following previous work on the GLUE benchmark, we report\nthe best GLUE dev set performance (Ben Zaken et al., 2022) and use 20 training\nepochs with an early stopping scheme of 10 epochs for all per-task experiments.\nWe use AdapterHub (Pfeiffer et al., 2020a) as the codebase and conduct extensive\nexperiments with the uncased BERT_{base} (Devlin et al., 2019) as the main\nbackbone model. We report main experiments with the mean and standard deviation\nover 5 different random seeds. Following Pfeiffer et al. (2020b), we use a\nrecommended learning rate of 10^{\u22124} for all PEFT experiments. We use the\nlearning rate of 2 \u00d7 10^{\u22125} for full model FT according to Mao et al. (2022).\nWe use the batch size of 32 and 16 for all BERT and RoBERTa experiments,\nrespectively. The optimiser settings for each PEFT module follow the default\nsettings in AdapterHub (Pfeiffer et al., 2020a). We implement the BO search\nalgorithm in BoTorch (Balandat et al., 2020) and use the recommended settings\nfrom Eriksson and Jankowiak (2021) for the surrogate. For acquisition function\noptimisation, we use a local search method similar to previous literature with a\nsimilar setup (Wan et al., 2021; Eriksson et al., 2021): at each search\niteration (after the initial randomly sampled points), we collect the\nPareto-optimal architectures up to this point. From this collection of\nPareto-optimal architectures, we perform a local search by evaluating the\nacquisition function values of their neighbours, and move the current point to a\nneighbour with a higher acquisition function value and this process is repeated\nuntil convergence. Due to the relatively noisy nature of the problem, we use 100\nrandom initialisation points for all experiments followed by 100 BO iterations.\nWe further show results using RoBERTa_{large} (Liu et al., 2019b) in Table 4,\nwhich shows findings that are consistent with the BERT_{base}. In experiments\nwith RoBERTa_{large} as the underlying PLM, we report the RTE results with a\nlearning rate of 2 \u00d7 10^{\u22125} for_{ AUTO}PEFT^{MRPC} and_{ AUTO}PEFT^{CoLA};\n10^{\u22124} for AUTOPEFT^{RTE}."
        },
        {
            "section_title": "5 Results and Discussion",
            "text": "Discussion of Main Results. The main results are summarised in Table 1 where we\nevaluate the AUTOPEFT-found configurations searched from RTE, the most\nlow-resource and challenging task, on the full GLUE suite. For simplicity, we\nreport a single configuration that leads to the highest task performance in a\npredefined, user-specified parameter budget from the discovered Pareto-optimal\nset in Table 1, whereas the full Pareto-optimal set is evaluated in Figure 4.\nFirst, using only 0.76% of parameters, AUTOPEFT^{RTE} outperforms all the PEFT\nbaselines (more than 2% on RTE). The AUTOPEFT-found configuration also\noutperforms the full-model FT baseline on the RTE task by more than 1%. These\nresults indicate the effectiveness of the AUTOPEFT framework in optimising both\ntask performance and parameter efficiency. Transferring the RTE-based\nconfigurations to other tasks, we find that strong performance is maintained\nacross the target tasks, with more benefits on the medium-resource tasks (MRPC,\nSTS-B, CoLA), but the configuration remains competitive also for higher-resource\ntasks (e.g. QQP, MNLI).\n\nTable 2 specifies the composition of the found configuration, indicating the\nexact task-active layers while allocating more parameter budget to the efficient\nand effective PA module. On average, the_{ AUTO}PEFT^{RTE} configuration shows a\ncomparable fine-tuning performance (83.17), to FFT (83.15), by only updating\n0.76% of parameters. With strong transferability across similar tasks, AUTOPEFT\nprovides distinct advantages in parameter efficiency; the search algorithm\nitself coupled with transfer becomes more sample-efficient within limited\ntraining resources.\n\nScalability to More Tasks and Efficiency. We next 'stress-test' the ability of\nAUTOPEFT-found configuration in a more challenging scenario, carrying out\nexperiments on a completely new set of dissimilar tasks. Table 3 reports the\nresults of transferring AUTOPEFT^{RTE} from Table 1 to four SuperGLUE tasks. In\nterms of parameter efficiency, we observe consistent patterns as in Table 1\nbefore, where our plug-and-play PEFT configuration outperforms existing PEFT\nbaselines by a substantial margin (2%) on average while being comparable to the\ncostly full-model FT.(With the^{ AUTO}PEFT-found off-the-shelf configuration,)\nIn terms of search cost, we recall that through the use of low-fidelity proxy\nand the strong transferability,_{ AUTO}PEFT^{RTE} in Table 1 only requires an\nadditional, one-off 1.9% in terms of training time (or equivalently the number\nof fine-tuning steps) of that of single-seed training of the GLUE training sets.\nFurthermore, Figure 5 demonstrates the robustness of our framework to the choice\nof the source task to search on. Therefore, our framework is task-agnostic with\na cheap one-time cost but yields 'permanent' improvement towards all efficiency\nmetrics for PEFT: space, time, and memory.\n\nPer-Task Configuration Search. We further conduct full-resource per-task\nAUTOPEFT searches. While naturally more expensive, we argue this setup is useful\nif, for example, one is interested in finding absolutely the best configurations\nfor that particular task and where search cost is less of a concern. Due to\ncomputational constraints, we search per-task on RTE, MPRC, STS-B and CoLA then\nport the small set of best configurations to the remaining higher-resource tasks\n(SST-2, QNLI, QQP, MNLI). We observe consistent gains in all tasks we search on\nover the best-performing PEFT baselines, e.g. MRPC (87.16% (best baseline) to\n87.45% (ours)) and CoLA (60.13% to 60.92%), and also the transferred\nconfiguration _{AUTO}PEFT^{RTE} in Table 1. One interpretation is that while\nconfigurations are highly transferable, the optimal configurations may\nnonetheless differ slightly across tasks such that while transferred AUTOPEFT\nconfigurations (e.g. the one reported in Table 1) perform well, searching\nper-task per-this requires no additional search cost and enables a more\nefficient and effective tuning approach for new tasks.\n\nforms the best. Crucially, we also find per-task AUTOPEFT in this setup to even\noutperform FFT, despite only using 1.4% of all parameters, except for the\nhigh-resources task where we mostly perform on par; this is consistent with our\nobservations that similar to the baselines, due to the richness of training\nresources, the performance may be mostly saturated and PEFT methods often\nachieve on-par performance to FFT at most.\n\nAnalysing the 'Behaviour' of Bayesian Optimisation. Figure 6 shows the\ndistribution of AUTOPEFT-found configurations when we conduct its search\nexperiment on RTE. Recalling that the search strategy (\u00a72.2) starts with the\nrandom initialisation, we compare the behaviours of the random explorations and\nthe BO-suggested configurations: whereas the random search baseline is purely\nexploratory and discovers less parameter-efficient configurations, BO succeeds\nin discovering configurations towards the regions with improved parameter\nefficiency. BO eventually discovers a rich family of PEFT configurations across\na wide range of parameters, whereas previous approaches typically fail to\nexplore the entire Pareto front. This is a critical strength motivating our BO\nsearch strategy.\n\nAblation of the Configuration Space. To provide a finer-grained analysis of\nfactors that bring positive impact to AUTOPEFT, we ablate the AUTOPEFT search\nspace from the full configuration space: 1) to the basic enumeration of the\nbottleneck size D_{SA} of the SA only (the SA space). We then include the\nTransformer layer and the SA size together into the search space (the SA-Layer\nspace) to validate the usefulness of using layer selection as one configuration\ndimension. We can then also expand the search space by adding another module\n(e.g. PA yields the SA-PA-Layer space). Figure 7 plots the performance over the\nablated configuration spaces and over different parameter budgets. Several key\nfindings emerge. First, combining multiple single PEFT modules has a positive\nimpact on AUTOPEFT in general (c.f. full AUTOPEFT vs. SA-PA-Layer vs. SA-Layer).\nRelying on layer selection also brings benefits (c.f. SA vs. SA-Layer). The\ncomparison also indicates that leaving out some Transformer layers while\nincreasing the capacity of the PEFT module is a straightforward method to\nimprove the parameter efficiency and task performance of the PEFT module within\na fixed parameter budget. The ablation results also demonstrate that AUTOPEFT is\nsearch space-agnostic, capable of effectively operating over configuration\nspaces of different granularity.\n\nLayer Selection. The ability to disable some PEFT layers altogether is a key\nnovelty of the AUTOPEFT search space, and to further compare different layer\nselection approaches, we conduct a controlled experiment with the SA module on\nBERT_{large} (24 Transformer layers) under a predefined parameter budget. In\nTable 5, we compare against AdapterDrop, which simply drops the adapters for the\nfirst 11 layers while doubling their bottleneck sizes, and, within the same\narchitecture, we also include the Adaptable Adapter with selected layers from\nswitch learning (3 and 10 layers from the first 12 and the other 12 layers,\nrespectively). We show that AUTOPEFT outperforms existing layer selection\nbaselines activating fewer PEFT layers, leading to better parameter efficiency\n(12.5% fewer parameters in relative terms) yet achieving better performance. It\nindicates that selecting the best insertion layer is non-trivial, and AUTOPEFT\ncan efficiently learn the correlation between layers."
        },
        {
            "section_title": "6 Conclusion",
            "text": "We proposed AUTOPEFT, a novel search framework for automatically configuring\nparameter-efficient fine-tuning (PEFT) modules of pretrained language models.\nAUTOPEFT features both a large and expressive, newly designed configuration\nsearch space and an effective search method featuring Bayesian optimisation that\ndiscovers a Pareto-optimal set of novel PEFT configurations with promising\nperformance-efficiency trade-offs. Empirically, we demonstrated that\nAUTOPEFT-discovered configurations transfer strongly across different GLUE and\nSuperGLUE tasks, outperforming a variety of strong PEFT baselines and being\ncompetitive to full model fine-tuning."
        },
        {
            "section_title": "Limitations and Future Work",
            "text": "AUTOPEFT search inevitably incurs a search cost since it requires iterative\noptimisation at search time. However, we mitigate this by 1) using a\nlow-fidelity proxy of 1-epoch training, and 2) leveraging strong transferability\nby generalising from low-resource and thus quick-to-train tasks. While the\nsearch itself can be seen as a one-time cost yielding a permanent\nwell-performing and shareable configuration for particular tasks, we plan to\ndelve deeper into further optimising the search cost in future work.\nFurthermore, while we conduct extensive experiments on the search space that\ncontains three existing PEFT modules as building blocks, novel PEFT modules may\nemerge. However, AUTOPEFT framework is general where we may easily integrate\nthese forthcoming new modules. We defer thorough investigations to future work."
        }
    ],
    "addenda": [
        {
            "section_title": "A Supplemental Material: Technical Details",
            "text": "PEFT Modules: Architectures and Setup. We implement the serial adapter\narchitecture (SA) following the setup of Pfeiffer et al. (2020b). The parallel\nadapter (PA) architecture is the same as the one proposed by He et al. (2022),\nwhere a scaling factor of 4 is implemented in all PA experiments. The\nprefix-tuning (PT) architecture has an intermediate MLP with a bottleneck size\nof 800, which is trained the same way as in the original wor (Li and Liang,\n2021). We also use the default setting for LoRA with a scaling of 8 and a rank\nof 8. We reproduce the experimental results with the reported setup of the MAM\nadapter He et al. (2022) and UniPELT (Mao et al., 2022). We reproduce the AdaMix\nresults with the reported hyperparameter setup from the original work (Wang et\nal., 2022) in 20 epochs. In the experiments of Figure 4, we control the\nbottleneck size D_{SA} and D_{PA} for SA and PA baselines, respectively, while\nkeeping other setups unchanged to discover their performance across the\nparameter budget. Similarly, we control the prefix length L_{PT} for\nprefix-tuning and the rank r of LoRA without changing other setups.\n\nAUTOPEFT Search Setup. We implement the BO algorithm in BoTorch (Balandat et\nal., 2020). We use the Matern 5/2 kernel as the covariance function, and for the\nMonte Carlo sampling settings of SAAS-BO (Eriksson and Jankowiak, 2021), we use\na warm-up step of 256, the number of samples to retain as 128, and thinning as\n16. For the optimisation of the acquisition function, to adapt to the discrete\nsetup, we use a local search method similar to previous literature involving\nsimilar setup (Wan et al., 2021; Eriksson et al., 2021): at each search\niteration (after the initial randomly sampled points), we collect the\nPareto-optimal architectures up to this point. From this collection of\nPareto-optimal architectures, we perform a local search by evaluating the\nacquisition function values of their neighbours, and move the current point to a\nneighbour with a higher acquisition function value and this process is repeated\nuntil convergence (which is a local minimum in terms of acquisition function),\nor 100 evaluations in acquisition function value are reached. At each search\niteration, we restart this process 10 times and select the top candidate for the\nquery (in this case, fine-tuning) for the next iteration. For all BO\nexperiments, we use 200 total evaluations; given the noisy nature of the\nproblem, we use a relatively large number of random initialisation points (100)\nto ensure that the search results are not overly sensitive to initialisation. We\nuse the same hyperparameter settings as described for all experiments conducted\nin this paper.\n\nCalculation of Fine-tuned Parameters. The uncased BERT_{base} model (109M) has\n12 Transformer layers with a hidden dimension size of 768. The uncased\nBERT_{large} model (335M) and RoBERTa_{large} (355M) both have 24 layers with a\nhidden dimension size of 1, 024. For both SA and PA, their fine-tuned parameters\nare computed by 2 \u00d7 D_{adapter} \u00d7 D_{h} \u00d7 |l|, where D_{h} represents the\ncorresponding hidden dimension of the selected model, and |l| refers to the\ntotal selected number of insertion layers. Similarly, we calculate the\nfine-tuned parameters of PT by 2 \u00d7 L_{PT} \u00d7 D_{h} \u00d7 |l|. Thus, the number of\nfine-tuned parameters of the AUTOPEFT-found configurations is a summation of\nindividual PEFT modules' parameters. We report the default fine-tuned parameters\nfor the remaining PEFT modules as defined in their original papers."
        },
        {
            "section_title": "B Search Space and Discovered Architectures",
            "text": "We analyse the learned configurations in terms of the selected layers over\ndifferent parameter scales in Table 2. They show a common trend in selecting the\nhigher Transformer layers to insert the PEFT modules, which coincides with\nprevious findings that the higher layer contains richer task-specific\nrepresentations, and introducing PEFT modules to these layers is more efficient\nthan other layers. With the AUTOPEFT-found configurations reported in Table 2,\nwe hope future PEFT research and applications can benefit from the architecture\ndesign similar to_{ AUTO}PEFT^{RTE} that we find the most transferable across\ntasks."
        }
    ],
    "abstract": "Large pretrained language models are widely used in downstream NLP tasks via\ntask-specific fine-tuning, but such procedures can be costly. Recently,\nParameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task\nperformance while updating a much smaller number of parameters compared to full\nmodel fine-tuning (FFT). However, it is non-trivial to make informed design\nchoices on the PEFT configurations, such as their architecture, the number of\ntunable parameters, and even the layers in which the PEFT modules are inserted.\nConsequently, it is highly likely that the current, manually designed\nconfigurations are suboptimal in terms of their performance-efficiency\ntrade-off. Inspired by advances in neural architecture search, we propose\nAUTOPEFT for automatic PEFT configuration selection: we first design an\nexpressive configuration search space with multiple representative PEFT modules\nas building blocks. Using multi-objective Bayesian optimisation in a low-cost\nsetup, we then discover a Pareto-optimal set of configurations with strong\nperformance-cost trade-offs across different numbers of parameters that are also\nhighly transferable across different tasks. Empirically, on GLUE and SuperGLUE\ntasks, we show that AUTOPEFT-discovered configurations significantly outperform\nexisting PEFT methods and are on par or better than FFT, without incurring\nsubstantial training efficiency costs.",
    "acknowledgements": "Xingchen Wan is supported by the Clarendon Scholarship at University of Oxford.\nThe work has been supported in part by a personal Royal Society University\nResearch Fellowship (no 221137; 2022-) awarded to Ivan Vuli\u00b4c.",
    "references": "Alan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan Vuli\u00b4c. 2022. Composable\nsparse fine-tuning for cross-lingual transfer. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1778-1796, Dublin, Ireland. Association for Computational\nLinguistics.\n\nMaximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham,\nAndrew G Wilson, and Eytan Bakshy. 2020. Botorch: A framework for efficient\nmonte-carlo bayesian optimization. Advances in neural information processing\nsystems, 33:21524-21538.\n\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022. BitFit: Simple\nparameter-efficient fine-tuning for transformer-based masked language-models. In\nProceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 1-9, Dublin, Ireland. Association\nfor Computational Linguistics.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Rad-ford, Ilya Sutskever, and\nDario Amodei. 2020. Language models are few-shot learners. In Advances in Neural\nInformation Processing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\n\nGuanzheng Chen, Fangyu Liu, Zaiqiao Meng, and Shangsong Liang. 2022a. Revisiting\nparameter-efficient tuning: Are we really there yet? In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing, pages 2612-2626,\nAbu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nJiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. 2023.\nParameter-efficient fine-tuning design spaces. In The Eleventh International\nConference on Learning Representations.\n\nShoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and\nPing Luo. 2022b. Adaptformer: Adapting vision transformers for scalable visual\nrecognition. In Advances in Neural Information Processing Systems.\n\nSamuel Daulton, Maximilian Balandat, and Eytan Bakshy. 2021. Parallel bayesian\noptimization of multiple noisy objectives with expected hypervolume improvement.\nIn Advances in Neural Information Processing Systems 34: Annual Conference on\nNeural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,\nvirtual, pages 2187-2200.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of deep bidirectional transformers for language understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\n\nXuanyi Dong and Yi Yang. 2020. Nas-bench-201: Extending the scope of\nreproducible neural architecture search. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\n\nThomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2019. Neural architecture\nsearch: A survey. The Journal of Machine Learning Research, 20(1):1997-2017.\n\nDavid Eriksson, Pierce I-Jen Chuang, Samuel Daulton, Peng Xia, Akshat\nShrivastava, Arun Babu, Shicong Zhao, Ahmed A Aly, Ganesh Venkatesh, and\nMaximilian Balandat. 2021. Latency-aware neural architecture search with\nmulti-objective bayesian optimization. In 8th ICML Workshop on Automated Machine\nLearning (AutoML).\n\nDavid Eriksson and Martin Jankowiak. 2021. High-dimensional bayesian\noptimization with sparse axis-aligned subspaces. In Uncertainty in Artificial\nIntelligence, pages 493-503. PMLR.\n\nPeter I. Frazier. 2018. A tutorial on bayesian optimization. CoRR,\nabs/1807.02811.\n\nRoman Garnett. 2023. Bayesian Optimization. Cambridge University Press.\n\nDemi Guo, Alexander Rush, and Yoon Kim. 2021.\n\nParameter-efficient transfer learning with diff pruning. In Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing (Volume 1:\nLong Papers), pages 4884-4896, Online. Association for Computational\nLinguistics.\n\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham\nNeubig. 2022. Towards a unified view of parameter-efficient transfer learning.\nIn The Tenth International Conference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022.\n\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de\nLaroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Proceedings of the 36th\nInternational Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, pages 2790-2799.\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2022a. Lora: Low-rank adaptation of large\nlanguage models. In The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022.\n\nShengding Hu, Zhen Zhang, Ning Ding, Yadao Wang, Yasheng Wang, Zhiyuan Liu, and\n\nMaosong Sun. 2022b. Sparse structure search for delta tuning. In Advances in\nNeural Information Processing Systems.\n\nSergio Izquierdo, Julia Guerrero-Viu, Sven Hauns, Guilherme Miotto, Simon\nSchrodi, Andr\u00e9 Biedenkapp, Thomas Elsken, Difan Deng, Marius Lindauer, and Frank\nHutter. 2021. Bag of baselines for multi-objective joint neural architecture\nsearch and hyperparameter optimization. In 8th ICML Workshop on Automated\nMachine Learning (AutoML).\n\nKirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnab\u00e1s P\u00f3czos, and\nEric P. Xing. 2018. Neural architecture search with bayesian optimisation and\noptimal transport. In Advances in Neural Information Processing Systems 31:\nAnnual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,\nDecember 3-8, 2018, Montr\u00e9al, Canada, pages 2020-2029.\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for\nparameter-efficient prompt tuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, pages 3045-3059, Online and\nPunta Cana, Dominican Republic. Association for Computational Linguistics.\n\nLiam Li and Ameet Talwalkar. 2019. Random search and reproducibility for neural\narchitecture search. In Proceedings of the Thirty-Fifth Conference on\nUncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25,\n2019, pages 367-377.\n\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous\nprompts for generation. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers), pages\n4582-4597, Online. Association for Computational Linguistics.\n\nHanxiao Liu, Karen Simonyan, and Yiming Yang. 2019a. DARTS: differentiable\narchitecture search. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Or-leans, LA, USA, May 6-9, 2019.\n\nHaokun Liu, Derek Tam, Muqeeth Mohammed, Jay Mohta, Tenghao Huang, Mohit Bansal,\nand Colin Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and\ncheaper than in-context learning. In Advances in Neural Information Processing\nSystems.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. Roberta: A\nrobustly optimized BERT pretraining approach. CoRR, abs/1907.11692.\n\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. 2021. Compacter:\nEfficient low-rank hypercomplex adapter layers. In Advances in Neural\nInformation Processing Systems 34: Annual Conference on Neural Information\nProcessing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages\n1022-1035.\n\nYuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott\nYih, and Madian Khabsa. 2022. UniPELT: A unified framework for\nparameter-efficient language model tuning. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 6253-6264, Dublin, Ireland. Association for Computational\nLinguistics.\n\nNafise Moosavi, Quentin Delfosse, Kristian Kersting, and Iryna Gurevych. 2022.\nAdaptable adapters. In Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 3742-3753, Seattle, United States. Association for\nComputational Linguistics.\n\nJonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and\nMikel Artetxe. 2022. Lifting the curse of multilinguality by pre-training\nmodular transformers. In Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, pages 3479-3495, Seattle, United States. Association for\nComputational Linguistics.\n\nJonas Pfeiffer, Andreas R\u00fcckl\u00e9, Clifton Poth, Aishwarya Kamath, Ivan Vuli\u00b4c,\nSebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. 2020a. AdapterHub: A\nframework for adapting transformers. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing: System Demonstrations, pages\n46-54, Online. Association for Computational Linguistics.\n\nJonas Pfeiffer, Sebastian Ruder, Ivan Vuli\u00b4c, and Edoardo Maria Ponti. 2023.\nModular deep learning. CoRR, abs/2302.11529.\n\nJonas Pfeiffer, Ivan Vuli\u00b4c, Iryna Gurevych, and Sebastian Ruder. 2020b. MAD-X:\nAn Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 7654-7673, Online. Association for Computational Linguistics.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of\ntransfer learning with a unified text-to-text transformer. J. Mach. Learn. Res.,\n21:140:1-140:67.\n\nPengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen,\nand Xin Wang. 2021. A comprehensive survey of neural architecture search:\nChallenges and solutions. ACM Computing Surveys (CSUR), 54(4):1-34.\n\nBin Xin Ru, Xingchen Wan, Xiaowen Dong, and Michael A. Osborne. 2021.\nInterpretable neural architecture search via bayesian optimisation with\nweisfeiler-lehman kernels. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\n\nRobin Ru, Pedro M. Esperan\u00e7a, and Fabio Maria Carlucci. 2020. Neural\narchitecture generator optimization. In Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\n\nAndreas R\u00fcckl\u00e9, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils\nReimers, and Iryna Gurevych. 2021. AdapterDrop: On the efficiency of adapters in\ntransformers. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 7930-7946, Online and Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid\nAlyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari,\nCanwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim,\nGunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike\nTian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit\nPandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault F\u00e9vry, Jason Alan Fries, Ryan Teehan, Teven Le Scao,\nStella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask\nprompted training enables zero-shot task generalization. In The Tenth\nInternational Conference on Learning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022.\n\nJasper Snoek, Hugo Larochelle, and Ryan P. Adams. 2012. Practical bayesian\noptimization of machine learning algorithms. In Advances in Neural Information\nProcessing Systems 25: 26th Annual Conference on Neural Information Processing\nSystems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe,\nNevada, United States, pages 2960-2968.\n\nYi-Lin Sung, Varun Nair, and Colin Raffel. 2021.\n\nTraining neural networks with fixed sparse masks. In Advances in Neural\nInformation Processing Systems 34: Annual Conference on Neural Information\nProcessing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages\n24193-24205.\n\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\n\nBERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, pages 4593-4601,\nFlorence, Italy. Association for Computational Linguistics.\n\nMojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. 2022.\nDylora: Parameter efficient tuning of pre-trained models using dynamic\nsearch-free low-rank adaptation. CoRR, abs/2210.07558.\n\nIvan Vuli\u00b4c, Edoardo Maria Ponti, Robert Litschko, Goran Glava\u0161, and Anna\nKorhonen. 2020. Probing pretrained language models for lexical semantics. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language\n\nProcessing (EMNLP), pages 7222-7240, Online. Association for Computational\nLinguistics.\n\nXingchen Wan, Vu Nguyen, Huong Ha, Binxin Ru, Cong Lu, and Michael A Osborne.\n2021. Think global and act local: Bayesian optimisation over high-dimensional\ncategorical and mixed search spaces. In International Conference on Machine\nLearning, pages 10663-10674. PMLR.\n\nXingchen Wan, Binxin Ru, Pedro M. Esperan\u00e7a, and Zhenguo Li. 2022. On redundancy\nand diversity in cell-based neural architecture search. In The Tenth\nInternational Conference on Learning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net.\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel R. Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understanding systems. In Advances in\nNeural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 3261-3275.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel\nBowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural\nlanguage understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels,\nBelgium. Association for Computational Linguistics.\n\nYaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed\nHassan Awadallah, and Jianfeng Gao. 2022. Adamix: Mixture-of-adaptations for\nparameter-efficient model tuning. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, pages 5744-5760, Abu Dhabi,\nUnited Arab Emirates. Association for Computational Linguistics.\n\nColin White, Willie Neiswanger, and Yash Savani. 2021a. BANANAS: bayesian\noptimization with neural architectures for neural architecture search. In\nThirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third\nConference on Innovative Applications of Artificial\n\nIntelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages\n10293-10301.\n\nColin White, Arber Zela, Robin Ru, Yang Liu, and Frank Hutter. 2021b. How\npowerful are performance predictors in neural architecture search? In Advances\nin Neural Information Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual,\npages 28454-28469.\n\nSaining Xie, Alexander Kirillov, Ross Girshick, and Kaiming He. 2019. Exploring\nrandomly wired neural networks for image recognition. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages 1284-1293.\n\nAntoine Yang, Pedro M. Esperan\u00e7a, and Fabio Maria Carlucci. 2020. NAS evaluation\nis frustratingly hard. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\n\nQingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu\nChen, and Tuo Zhao. 2023. Adaptive budget allocation for parameter-efficient\nfine-tuning. In The Eleventh International Conference on Learning\nRepresentations.\n\nMengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Sch\u00fctze. 2020. Masking\nas an efficient alternative to finetuning for pretrained language models. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 2226-2241, Online. Association for Computational\nLinguistics.\n\nBarret Zoph and Quoc V. Le. 2017. Neural architecture search with reinforcement\nlearning. In 5th International Conference on Learning Representations, ICLR\n2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
    "images": {
        "Figure_1_block_22_a2c40c19": {
            "source_page_num": 0,
            "caption": "Figure 1: Performance of AUTOPEFT-discovered configurations (AutoPEFT &\nAutoPEFT(per-task); see details in Table 1) compared to other baseline PEFT\nmethods (markers) and full model FT that updates 100% of parameters (dashed\nhorizontal bar), averaged across 8 GLUE tasks. Our approach achieves the best\ntrade-off between task performance and parameter efficiency.",
            "box": [
                302.2760009765625,
                186.66299438476562,
                532.199951171875,
                336.23675537109375
            ],
            "adjacent_neighbor": 2,
            "image_file": "images/Figure_1_block_22_a2c40c19.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/fcd352d5dfd24460f22c4ecee5a501de.png"
        },
        "Figure_2_block_27_d828f171": {
            "source_page_num": 2,
            "caption": "Figure 2: Illustration of the AUTOPEFT search space which combines both\nlayer-level (Layers) and within-layer (Serial, Parallel, Prefix) search, and the\nconnections within a layer (Left). We further show two possible configurations\nin the search space (Right): note that some PEFT layers can be inactive\naltogether and the searchable module sizes (shaded in green), i.e. the\nbottleneck sizes in Serial and Parallel (D_{SA} and D_{PA} respectively) and\nsizes of P_{K}, P_{V} in Prefix (L_{PT}), are dynamic.",
            "box": [
                66.6709976196289,
                40,
                530.7118530273438,
                234.12191772460938
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Figure_2_block_27_d828f171.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/0ad67726ca3f603c0aea522c6f40e99a.png"
        },
        "Figure_3_block_15_afa01953": {
            "source_page_num": 3,
            "caption": "Figure 3: Illustration of the Pareto-optimal search with multi-objective\nBayesian optimisation (BO; \u00a72.2): The BO agent trains on the vector\nrepresentations of the evaluated configurations as inputs and their performance\nunder a low-fidelity setup (e.g. accuracy - obtained by fine-tuning the language\nmodel with the PEFT configuration for a small number of iterations) and cost\n(e.g. number of parameters) as targets. The BO agent then iteratively suggests\nnew configurations until convergence.",
            "box": [
                67.0,
                40,
                371.1874084472656,
                274.8346252441406
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Figure_3_block_15_afa01953.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/63834a8a54ffeb29e72945b187435d1c.png"
        },
        "Table_1_block_11_1dbc54e4": {
            "source_page_num": 6,
            "caption": "Table 1: Results on the GLUE benchmark with BERT_{base} (tasks are ranked in an\nascending order of training resources required from left to right). For^{\nAUTO}PEFT^{RTE}, we search on RTE with a low-fidelity proxy, training for 1\nepoch per iteration, only at a search cost of 1.9% (in terms of additional\nfine-tuning steps required) over the full GLUE experiment. We report the average\nfine-tuned parameters of per-task AUTOPEFT, where we conduct additional per-task\nsearches on RTE, MRPC, STS-B, and CoLA, and take best-found configurations for\nthe remaining tasks. We report Spearman's Correlation for STS-B, Matthew's\nCorrelation for CoLA, and accuracy for all other tasks (matched accuracy for\nMNLI). The percentage of parameters is the ratio of the number of additional\nparameters to the pretrained parameters. We reproduce all baselines and report\nthe mean and standard deviation of all results for 5 random seeds. The best,\nsecond-best, and third-best results are marked in bold fonts and ranked by\ncolour.",
            "box": [
                71.48699951171875,
                67.11968994140625,
                527.451416015625,
                209.23440551757812
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Table_1_block_11_1dbc54e4.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/7c2f37f7bb524133e444e02916ec0d01.png"
        },
        "Table_2_block_15_9896d95a": {
            "source_page_num": 6,
            "caption": "Table 2: Specification of the discovered AUTOPEFT configuration reported in\nTable 1 (^{AUTO}PEFT^{RTE}) using BERT_{base}.",
            "box": [
                71.48699951171875,
                338.9499816894531,
                299.4692077636719,
                407.1556091308594
            ],
            "adjacent_neighbor": 11,
            "image_file": "images/Table_2_block_15_9896d95a.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/7b55dc1fb089b8fd0b2d4e3f4dd48fb5.png"
        },
        "Table_3_block_21_8f6148d2": {
            "source_page_num": 6,
            "caption": "Table 3: Results on SuperGLUE tasks with AUTOPEFT-discovered configurations\nsearched on RTE with BERT_{base} as the underlying PLM. We split 10% of the\ntraining set as the new validation set, and report the ^{AUTO}PEFT^{RTE}-found\nconfiguration transfer results on the evaluation set over five random seeds.",
            "box": [
                71.48699951171875,
                458.5882263183594,
                299.4692077636719,
                527.6416625976562
            ],
            "adjacent_neighbor": 15,
            "image_file": "images/Table_3_block_21_8f6148d2.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/89734f4e5a881df3bf0c6ab1db2e96a9.png"
        },
        "Figure_4_block_38_83906404": {
            "source_page_num": 7,
            "caption": "Figure 4: The Pareto fronts of AUTOPEFT on RTE, MRPC, STS-B, and CoLA compared\nto baselines on BERT_{base}, over varying parameter budgets. We report the\nsingle-seed task score but otherwise follow the settings in Table 1.",
            "box": [
                67.0,
                40,
                531.788818359375,
                200.1650390625
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Figure_4_block_38_83906404.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/fedeea5f3805e2b2bf97411a6627b4c8.png"
        },
        "Table_4_block_45_e1598c2a": {
            "source_page_num": 7,
            "caption": "Table 4: Experimental results on GLUE with RoBERTa_{large}. We report the full\nmodel fine-tuning^{\u2020} results from Liu et al. (2019b) with Pearson correlation\nfor STS-B. We include the LoRA^{\u2021} module performance from Hu et al. (2022a). We\nexclude QQP and MNLI tasks due to the large computation cost of RoBERTa_{large}.\nConsistent with Table 1, we again report AUTOPEFT results searched on RTE in\nfull-resource settings that are then transferred all included GLUE tasks\n(AUTOPEFT^{RTE}) and per-task AUTOPEFT (AUTOPEFT^{task} _{Avg.}) but on\nRoBERTa_{large}.",
            "box": [
                71.66200256347656,
                243.47320556640625,
                527.4473266601562,
                336.9195861816406
            ],
            "adjacent_neighbor": 38,
            "image_file": "images/Table_4_block_45_e1598c2a.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/901f2d19e640482c10331fe9a6575ec1.png"
        },
        "Figure_5_block_9_f53f5c56": {
            "source_page_num": 8,
            "caption": "Figure 5: Pairwise transferability study of AUTOPEFT-discovered configurations:\neach row (Ours^{[task]}) denotes the performances of the AUTOPEFT configuration\nsearched from [task] (e.g. RTE) to the task itself and 3 other GLUE tasks. The\nresults show that AUTOPEFT performance is largely robust to the choice of which\ntask to search on.",
            "box": [
                67.0,
                40,
                296.9258117675781,
                180.36642456054688
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Figure_5_block_9_f53f5c56.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/a12f1778d211066c4cc310df3862f86b.png"
        },
        "Figure_6_block_23_785d972e": {
            "source_page_num": 8,
            "caption": "Figure 6: The distribution of the discovered configurations via BO (orange,\ndescribed in \u00a72.2 and random search (grey) using the same total number of\nevaluations (200). Both searches use the same, 100 random initialising points\n(blue) on RTE. Note that for configurations with similar accuracy, the\nBO-generated configurations typically have much better parameter efficiency.",
            "box": [
                301.9169921875,
                40,
                532.198974609375,
                187.692626953125
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Figure_6_block_23_785d972e.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/9a0770e38543e71944cfed0c2044b51f.png"
        },
        "Figure_7_block_9_d37e222f": {
            "source_page_num": 9,
            "caption": "Figure 7: The performance of AUTOPEFT with ablation of search space on RTE on\nBERT_{base}. The SA results refer to the Pfeiffer adapter (Pfeiffer et al.,\n2020b) with an enumeration of its bottleneck size. For other search spaces, we\nreport the Pareto front of AUTOPEFT-found configurations, where SA-PA-PT-Layer\nforms the search space of AUTOPEFT.",
            "box": [
                66.64099884033203,
                40,
                296.9254150390625,
                196.08563232421875
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Figure_7_block_9_d37e222f.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/4bb7480546cc17c69ef4b549a2db85e2.png"
        },
        "Table_5_block_12_01cc2235": {
            "source_page_num": 9,
            "caption": "Table 5: Comparing AUTOPEFT to layer selection baselines with the same parameter\nbudget on BERT_{large}. We report the Pfeiffer adapter for all 24 layers\n(Serial), specialised AdapterDrop (R\u00fcckl\u00e9 et al., 2021) that inserts SA for the\nlast 13 layers, and AA^{uni} (Moosavi et al., 2022) without its rational\nactivation function with 13 selected layers (Adaptable Adapter). We run our\nAUTOPEFT under the comparable search space of 24 layers and approximately match\nthe size of Serial.",
            "box": [
                71.25299835205078,
                303.5675964355469,
                299.3522071838379,
                378.6244201660156
            ],
            "adjacent_neighbor": 9,
            "image_file": "images/Table_5_block_12_01cc2235.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/6d68bede33234f74a26c552e1a78f7fa.png"
        },
        "Table_6_block_5_0828804c": {
            "source_page_num": 15,
            "caption": "Table 6: The search space of the AUTOPEFT. Each insertion layer has a Boolean\ndecision for inserting the PEFT modules. The 0 size of submodules indicates that\nwe exclude the corresponding submodule from the configuration. The total number\nof configurations for BERT_{base}: 2^{12} \u00d7 11 \u00d7 11 \u00d7 11 \u2248 5 \u00d7 10^{6} and for\nBERT/RoBERTa_{large}: 2^{24} \u00d7 12 \u00d7 12 \u00d7 12 \u2248 3 \u00d7 10^{10}.",
            "box": [
                71.48699951171875,
                66.81876373291016,
                527.2857666015625,
                157.40731811523438
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Table_6_block_5_0828804c.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/945bdcc1557b1789ccf00139d7c35291.png"
        },
        "Table_7_block_11_9d6128d1": {
            "source_page_num": 16,
            "caption": "Table 7: The AUTOPEFT-found configurations reported in Table 1 using\nBERT_{base}. The average of fine-tuned parameters (%) of AUTOPEFT^{task} _{Avg.}\nis calculated by (1.42 + 3.86 + 1.06 + 0.29 + 1.42 + 0.30 + 1.42 + 1.42)/8 =\n1.40, where we transfer the best-found configurations to SST-2, QNLI, QQP, and\nMNLI as their best per-task configurations for achieving the best trade-off\nbetween task performance and efficiency.",
            "box": [
                71.50192260742188,
                82.02970123291016,
                525.553466796875,
                313.629638671875
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Table_7_block_11_9d6128d1.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/cb03ec60d276aee22f5b4afcdf3d6ea9.png"
        },
        "Table_8_block_14_cd0f69c4": {
            "source_page_num": 16,
            "caption": "Table 8: The AUTOPEFT-found configurations reported in Table 5 using\nBERT_{large}.",
            "box": [
                71.50192260742188,
                400.854736328125,
                525.553466796875,
                450.28564453125
            ],
            "adjacent_neighbor": 11,
            "image_file": "images/Table_8_block_14_cd0f69c4.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/1fe9288fd77971a03f9f3807f2a9cf2f.png"
        },
        "Table_9_block_24_beeeec0e": {
            "source_page_num": 16,
            "caption": "Table 9: The AUTOPEFT-found configurations reported in Table 4 using\nRoBERTa_{large}. The average of fine-tuned parameters (%) of AUTOPEFT^{task}\n_{Avg.} is calculated by (0.03 + 0.25 + 0.25 + 2.36 + 2.36 + 0.03)/6 = 0.88,\nwhere we transfer the best-found AUTOPEFT^{CoLA} to SST-2 and AUTOPEFT^{RTE} to\nQNLI as their best per-task configurations for achieving the best trade-off\nbetween performance and efficiency.",
            "box": [
                71.50192260742188,
                502.36376953125,
                525.553466796875,
                701.2740478515625
            ],
            "adjacent_neighbor": 14,
            "image_file": "images/Table_9_block_24_beeeec0e.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/097f2a88f1163ac69d12044b74ded09e.png"
        }
    }
}