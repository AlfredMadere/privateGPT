{
    "title": "Multitask Prompt Tuning Enables\nParameter-Efficient Transfer Learning",
    "authors": "Zhen Wang^{1*} Rameswar Panda^{2} Leonid Karlinsky^{2} Rogerio Feris^{2} Huan Sun^{1} Yoon Kim^{3}\n^{1}The Ohio State University,^{ 2}MIT-IBM Watson AI Lab,^{ 3}Massachusetts Institute of Technology\n{wang.9215,sun.397}@osu.edu, {rpanda, leonidka, rsferis}@ibm.com, yoonkim@mit.edu",
    "body": [
        {
            "section_title": "1 Introduction",
            "text": "Finetuning pretrained language models (PLMs) has led to significant improvements\nacross various downstream NLP tasks (Devlin et al., 2019; Howard & Ruder, 2018;\nRaffel et al., 2020). However, the conventional paradigm of full task-specific\nfinetuning (FT) is difficult to scale to multiple tasks, given that modern PLMs\ncan have hundreds of millions (or even billions) of parameters. There thus has\nbeen a growing interest in developing parameter-efficient methods for model\ntuning (Houlsby et al., 2019; Lester et al., 2021; Ding et al., 2022), where the\ngoal is to learn only a small number of additional parameters per task while\nachieving performance comparable to full finetuning.\n\nPrompt tuning (PT), which prepends tunable continuous prompt vectors to the\ninput, has emerged as a promising approach for parameter-efficient transfer\nlearning with PLMs (Liu et al., 2021a; Li & Liang, 2021; Lester et al., 2021;\nLiu et al., 2022b; 2021b). PT freezes the PLM parameters and only learns a small\nset of task-specific prompt vectors. However, despite their impressive\nperformance, there is still a large gap between prompt tuning and full\nfinetuning (Lester et al., 2021). Additionally, this approach is sensitive to\ninitialization and often requires more training time than finetuning (Su et al.,\n2022; Zhong et al., 2022).\n\nRecent work has proposed to address these issues by transferring prompt vectors\nfrom various tasks (Su et al., 2022; Zhong et al., 2022). These methods first\ntrain soft prompts on multiple source tasks and then use these pretrained\nprompts to initialize the prompt for further fine-tuning on a target task based\non a (potentially learned) similarity measure (Vu et al., 2022; Asai et al.,\n\n2022) (see Figure 1, top). In this paper, we extend this line of work and\nintroduce multitask prompt tuning (MPT), which uses multitask data to learn a\nsingle prompt that can be efficiently transferred to target tasks. While\nconceptually simple, learning a shared prompt space can be practically\nchallenging as it requires learning commonalities across different source tasks\nwhile minimizing interference. Therefore, we decompose the soft prompt of each\nsource task (which can be represented as a prompt matrix) into a multiplication\nof a shared matrix and a low-rank task-specific matrix, and find that this\ndecomposition is more effective than simply sharing the prompt matrix across all\ntasks. This decomposition is learned through knowledge distillation from soft\nprompts obtained from regular prompt tuning. To transfer to new tasks, we\nperform low-rank multiplicative updates to the shared prompt matrix. Figure 1\n(bottom) illustrates our approach.\n\nExtensive experiments on 23 NLP datasets across diverse tasks demonstrate the\neffectiveness of our proposed approach over state-of-the-art prompt transfer\nmethods. On the SuperGLUE benchmark (Wang et al., 2019), MPT with T5-Base\n(Raffel et al., 2020) yields a 16.3% improvement over the vanilla prompt tuning\nbaseline (PT, Lester et al., 2021), and also outperforms the most competitive\nmultitask prompt transfer baseline (ATTEMPT, Asai et al., 2022) despite tuning\nmuch fewer task-specific prompt parameters (77.6K vs 232K). On some benchmarks,\nMPT exceeds the performance of full finetuning while only requiring 0.035%\ntunable parameters per task (see Figure 2). We also find that MPT is very\neffective for few-shot learning with 4-32 labels for each target task."
        },
        {
            "section_title": "2 Related Work",
            "text": "Parameter-efficient transfer learning. Parameter-efficient transfer learning for\npretrained language models is an active research area (Ding et al., 2022).\nAdapters (Houlsby et al., 2019; Mahabadi et al., 2021) and its variants (Hu et\nal., 2021; Karimi Mahabadi et al., 2021) insert trainable layers, while BitFit\n(Zaken et al., 2022) only updates the bias parameters without changing any other\nmodel parameters. Diff pruning (Guo et al., 2021) and FISH (Sung et al., 2021)\nlearn sparse updates to the original PLM. Another popular choice is prompt\ntuning (Lester et al., 2021) which only updates soft prompt vectors prepended to\nthe input. Prefix-tuning of optimizing continuous prompts for natural language\ngeneration tasks is presented in Li & Liang (2021). UNIPELT learns to combine\ndifferent tuning methods via gating mechanism (Mao et al., 2022). HyperPrompt\n(He et al., 2022) introduces task-conditioned hyperprompts that condition the\nmodel on task-specific information for constructing prompts. LST (Sung et al.)\naims to reduce the training memory of parameter-efficient tuning by a ladder\nside network. Discrete (i.e., hard) prompts have also been shown to be effective\nin many cases (Schick & Sch\u00a8utze, 2021a;b; Gao et al., 2021; Malkin et al.,\n2022). However, our approach is most related to the transferability of prompts\n(Wang et al., 2021; Vu et al., 2022; Su et al., 2022), which focuses on boosting\nthe performance of prompt tuning across many tasks. SPoT (Vu et al., 2022)\nselects one prompt using a similarity measure, and ATTEMPT (Asai et al., 2022)\nadopts an attention mechanism over the source prompts to initialize the prompt\nfor a target task. Unlike existing works, our approach learns a single shared\nprompt by decomposing and distilling knowledge from source prompts for efficient\nadaptation to a diverse set of target tasks.\n\nMultitask learning. Multitask learning, which focuses on simultaneously solving\nmultiple related tasks with a single model, has been studied from multiple\nperspectives (Zhang & Yang, 2021; Ruder, 2017). A common approach is to transfer\na model that has been fine-tuned on multiple source tasks to another target task\n(Vu et al., 2020; Raffel et al., 2020; Aghajanyan et al., 2021a; Zhong et al.,\n2021; Clark et al., 2019b; Singh et al., 2022). A few recent works show\nzero-shot and few-shot transfer capabilities of language models through massive\nmultitask learning over a large number of tasks (Sanh et al., 2022; Wang et al.,\n2022; Liu et al., 2022a; Wei et al., 2021). Designing specific parameter-sharing\nstrategies is also another recent trend in multitask learning (Ruder et al.,\n2019; Sun et al., 2020; Misra et al., 2016). While our proposed approach is\ninspired by these methods, this paper focuses on multitask prompt transfer for\nparameter-efficient adaptation of language models, which still remains a\nchallenging and largely understudied problem.\n\nKnowledge distillation. Knowledge distillation has been used to improve\nperformance and efficiency across many tasks (Gou et al., 2021), including model\ncompression (Hinton et al., 2015; Jiao et al., 2020; Sanh et al., 2019),\ntransfer learning (Furlanello et al., 2018; Xu et al., 2020), machine\ntranslation (Zhou et al., 2019), question answering (Hu et al., 2018), and\ndocument retrieval (Shakeri et al., 2019). Concurrently with our work, PANDA\n(Zhong et al., 2022) uses knowledge distillation with a new metric to better\npredict prompt transferability across different combinations of source-target\ntasks. PANDA focuses on transferring from one source task to another target task\nusing a similarity measure (similar to SPoT (Vu et al., 2022)), while our MPT\napproach leverages multitask learning to better exploit cross-task knowledge for\nprompt transfer."
        },
        {
            "section_title": "3 Approach",
            "text": "Given a set of source tasks S = {S_{1}, S_{2}, ..., S_{\u03ba}} and target tasks T =\n{T_{1}, T_{2}, ..., T_{\u03c4}}, our goal is to learn a single soft prompt over S\nthat can be adapted to each task T_{i} in a parameter-efficient way. Simply\ntraining a single soft prompt on S and then finetuning on each T_{i} is\nsub-optimal as it can fail to leverage commonalities across source tasks while\nminimizing interference at the same time. To this end, multitask prompt tuning\n(MPT) aims to compress task-shared knowledge in S into a single prompt matrix\n\u03c6_{S} via knowledge distillation to improve performance on T while filtering out\ntask-specific information that is less useful for transfer learning.\n\nPrompt tuning. Given a pre-trained language model with parameters \u0398 and one\ntarget task T with training data (X, Y ) = {x_{i}, y_{i}}^{N} _{i=1}, the\nstandard approach is to directly finetune all the parameters by maximizing the\nconditional probability P(Y |X; \u0398), which can be parameter-inefficient when\nconsidering a group of target tasks T . An alternative that is more\nparameter-efficient is prompt tuning (PT), which randomly initializes a small\nnumber of learnable prompt vectors (i.e., soft prompts) to be prepended to the\ninput embeddings of the PLM while freezing model parameters \u0398 (Lester et al.,\n2021; Liu et al., 2022b). Formally, for a sequence of input tokens with token\nembeddings as T = [t_{1}, t_{2}, ..., t_{n}] \u2208 R^{n\u00d7d}, PT prepends a learnable\nprompt matrix P \u2208 R^{l\u00d7d} with the same dimension as the token embedding d,\nwhere l is a hyperparameter. PT then optimizes the following loss function with\nrespect to P ,\n\nL_{PLM} = \u2212 \ufffd i log P(y_{i} | x_{i} ; \u0398, P ), (1)\n\nwhere the input to the language model is given by the concatenated matrix [P ; T\n] \u2208 R^{(l+n)\u00d7d}. While this approach has been successful for some tasks and\nmodels, researchers have observed that vanilla PT can sometimes lead to lower\nperformance (especially on smaller PLMs), slow convergence, and high sensitivity\nto parameter initialization (Lester et al., 2021; Su et al., 2022; Zhong et al.,\n2022). Recent works address these issues by first training prompts on multiple\nsource tasks and then using these prompts to initialize the prompts for a target\ntask via some similarity measure (Asai et al., 2022; Vu et al., 2022). We extend\nthis line of work and propose a framework for transferring multitask knowledge\ninto a single soft prompt to enable more performant and parameter-efficient\ntransfer learning to downstream target tasks T ."
        },
        {
            "section_title": "3.1 MULTITASK PROMPT TUNING",
            "text": "Our proposed framework, dubbed MPT, consists of two stages: source training and\ntarget adaptation. MPT first focuses on source training to generate a single\nsoft prompt matrix to be reused in the second stage for target task adaptation.\nSpecifically, prompt matrices for the source tasks are decomposed into a\ntask-shared matrix and a low-rank task-specific matrix (prompt decomposition),\nwhere the former is shared across all tasks. This decomposition into shared and\ntask-specific components is learned through knowledge distillation. Once\nlearned, the shared prompt matrix is adapted to a downstream target task via\nlow-rank multiplicative updates.\n\nPrompt decomposition. The goal of prompt decomposition is to enable efficient\nknowledge sharing across source tasks S, while still allowing each task to\nmaintain its own parameters to encode task-specific knowledge. We decompose the\nsoft prompt P_{k} for the k-th task into two parts, as shown in Figure 3. Let\nP^{*} \u2208 R^{l\u00d7d} denote the shared prompt across all tasks, and further let u_{k}\n\u2208 R^{l}, v_{k} \u2208 R^{d} be the task-specific vectors for each task k. The\ntask-specific vectors form a rank-one matrix W_{k} = u_{k} \u2297 v^{T} _{k} , which\nhas the same dimensions as the shared prompt P^{*}. The task prompt \ufffd P for k-th\nsource task is then parameterized as:\n\n\ufffd P_{k} = P^{*} \u25e6 W_{k} = P^{*} \u25e6 (u_{k} \u2297 v^{T} _{k} ), (2)\n\nwhere \u25e6 denotes the Hadamard product between two matrices. Our parameterization\nof prompt decomposition is inspired by prior low-rank methods (Li et al., 2018;\nAghajanyan et al., 2021b; Wen et al., 2020), such that general information\nacross the set of source tasks S can be captured by \"slow\" weights P^{*} shared\nacross tasks, while the \"fast\" weights W_{k} could then encode task-specific\nknowledge for S_{k} in a low-rank subspace.\n\nPrompt distillation. Learning the prompt decomposition directly from the\nmultitask datasets S tended to make the shared component P^{*} overfit to larger\ntasks. We found knowledge distillation from separately-trained source prompts to\nbe an effective strategy for learning good decomposable prompts. Specifically,\nwe first obtain a teacher prompt P^{(teacher)} k for the k-th source task by\nconventional prompt tuning. We then randomly initialize a corresponding student\nprompt as \ufffd P_{k} = P^{*} \u25e6 (u_{k} \u2297 v^{T} _{k} ), where all student prompts\nshare P^{ *} and have their own task-specific vectors as described above. We\nthen use distillation to transfer cross-task knowledge into the shared prompt\nmatrix (Sanh et al., 2019). The first loss is to match the output probability\ndistributions of students and teachers by minimizing their KL-Divergence with\nrespect to the shared prompt matrix P^{*} and the task-specific parameters u_{k}\nand v_{k},\n\nL^{Logits} = \ufffd\n\nk\u2208|S|\n\n\ufffd (x_{i},y_{i})\u2208S_{k} KL \ufffd P \ufffd y_{i} | x_{i} ; \u0398, P^{(teacher)} k \ufffd \u2225 P \ufffd y_{i}\n| x_{i} ; \u0398, \ufffd P_{k} \ufffd\ufffd . (3)\n\nWe use a temperature T to control the smoothness of the output distribution for\nboth teacher and student models as p_{j} =^{1} _{Z} exp(z_{j}/T), where z_{i} is\nthe logit score for class j and Z is the normalization factor. We also have an\nadditional mean squared loss on teacher model hidden states,\n\nL^{Hidden} = \ufffd\n\nk\u2208|S|\n\n\ufffd (x_{i},y_{i})\u2208S_{k} (H_{k,i} \u2212 H^{(teacher)} k,i )^{2}, (4)\n\nwhere H^{(teacher)} k,i and H_{k,i} denote the hidden states of teacher and\nstudent networks respectively, which consist of a sequence of hidden vectors for\ni-th input. Such additional distillation loss from intermediate states has been\nshown to improve results in distilling PLMs (Jiao et al., 2020; Shleifer & Rush,\n2020). The total loss function for training student source prompts for obtaining\na single shared prompt to be transferred to the target side is then,\n\nL_{Total} = L_{PLM} + \u03bb(L_{Logits} + L_{Hidden}), (5)\n\nwhere L_{PLM} = \ufffd _{k\u2208|S|} L^{k} _{PLM} represents the aggregated task losses\nfor all source tasks, and \u03bb is a weight to balance the impact of distillation\nloss terms."
        },
        {
            "section_title": "3.2 SOURCE TRAINING AND TARGET ADAPTATION",
            "text": "Training the single source prompt to be transferred to target tasks requires two\nsteps. First, the teacher prompts for all source tasks are pretrained\nindividually through vanilla prompt tuning. Then, we perform multitask training\non S = {S_{1}, . . . , S_{\u03ba}} to jointly learn the single shared prompt via the\nknowledge distillation loss function in Equation 5. We also adopt a simple\nstochastic task sampling strategy, which dynamically changes the number of tasks\nper batch. For each batch of multitask samples, we randomly select a number K\nfrom [2, \u03ba] first, then randomly choose K tasks from S and their corresponding\nsamples to constitute mini-batches. Such dynamic task sampling strategies are\ncommon in the PLM multitask learning literature (Raffel et al., 2020).\n\nFor target adaptation, we initialize the target prompt for target task T_{t} to\nbe the Hadamard product of the shared prompt matrix and the task-specific\nlow-rank prompt matrix, i.e., \ufffd P_{t} = P^{*} \u25e6 (u_{t} \u2297 v^{\u22a4} _{t} ) and\noptimize with the regular task loss in Equation 1 with respect to P^{*}, u_{t},\nv_{t}, where we use separate learning rates for P^{*} vs. u_{t}, v_{t} (see\nAppedix A). We remark that MPT can also be used for multitask learning on a\ngroup of target tasks T = {T_{1}, T_{2}, ..., T_{\u03c4}}, where P^{*} is shared\nacross T .\n\nParameter-efficiency. Each task contains the shared prompt l \u00d7 d that has the\nsame dimensions as a vanilla soft prompt and a smaller number of task-specific\nvectors (l + d). Thus, the total number of tunable parameters for a single\ntarget task is (l \u00d7 d) + (l + d). After training, this can further be compressed\ninto a single matrix of size l \u00d7 d.(However for comparison against prior work we\nshow the number of tunable parameters, i.e., (l\u00d7d)+(l+d).) For a group of target\ntasks, the total number of tunable parameters is (l \u00d7 d) + (l + d)\u03c4, where \u03c4 is\nthe number of target tasks. We list and compare different methods in terms of\nthe number of trainable parameters in Table 1."
        },
        {
            "section_title": "4 Experiments",
            "text": "We conduct experiments across a comprehensive range of NLP datasets to show that\nMPT outperforms strong baselines in both full-dataset (Tables 1, 2) and few-shot\n(Tables 3, 4) adaptations, while being more parameter-efficient compared to\nexisting methods (Figure 2)."
        },
        {
            "section_title": "4.1 EXPERIMENTAL SETUP",
            "text": "Datasets and tasks. As in Asai et al. (2022) we evaluate MPT using 6 datasets\nwith more than 100k annotations as source tasks: MNLI (Williams et al., 2017),\nQNLI (Demszky et al., 2018), QQP (Wang et al., 2018), SST-2 (Socher et al.,\n2013), SQuAD (Rajpurkar et al., 2016), and ReCoRD (Zhang et al., 2018). We use\n23 datasets from four benchmarks as target tasks: MultiRC (Khashabi et al.,\n2018), BoolQ (Clark et al., 2019a), WiC (Pilehvar & Camacho-Collados, 2018), WSC\n(Levesque et al., 2012), and CB (De Marneffe et al., 2019) from SuperGLUE (Wang\net al., 2019); RTE (Giampiccolo et al., 2007), CoLA (Warstadt et al., 2019),\nSTS-B (Cer et al., 2017), MRPC (Dolan & Brockett, 2005), MNLI, QQP, QNLI and\nSST-2 from GLUE (Wang et al., 2018); Natural Questions (Kwiatkowski et al.,\n2019), HotpotQA (Yang et al., 2018), NewsQA (Trischler et al., 2017) and\nSearchQA (Dunn et al., 2017) from MRQA (Fisch et al., 2019); WinoGrande\n(Sakaguchi et al., 2021), Yelp-2 (Zhang et al., 2015), SciTail (Khot et al.,\n2018) and PAWS-Wiki (Zhang et al., 2019) from the \"Others\" benchmark in (Asai et\nal., 2022); and E2E (Novikova et al., 2017) and WebNLG (Gardent et al., 2017)\nfor experiments on adapting to natural language generation tasks.\n\nModels. Following the standard approach in prompt tuning (Lester et al., 2021;\nAsai et al., 2022), we mainly experiment using the publicly available pretrained\nT5-Base model with 220M parameters (Raffel et al., 2020). We use 100 prompt\nvectors for all benchmarks (hence \ufffd P_{k} \u2208 R^{100\u00d7d}). In our ablation study,\nwe also consider T5-Small (60M) and T5-Large (770M) models.\n\nBaselines. We compare MPT with the following baselines: (1) Full finetuning\n(FT), where all the model parameters are tuned during adaptation on each\ndownstream task. (2) Vanilla prompt tuning (PT) (Lester et al., 2021), where\ntarget prompt vectors are initialized by randomly sampled top vocabularies. (3)\nExisting prompt transfer methods, including SPoT (Vu et al., 2022) and ATTEMPT\n(Asai et al., 2022), which initialize target prompts by retrieving or\naggregating source prompts. (4) Popular parameter-efficient methods including\nAdapters (Houlsby et al., 2019) and BitFit (Zaken et al., 2022). On GLUE, we\nalso compare with several state-of-the-art methods that adapt a pretrained model\nto all the target tasks using multitask learning, such as HyperFomer (Mahabadi\net al., 2021), HyperDecoder (Ivison & Peters, 2022), multitask variants of FT\nand Adapters. We directly quote numbers reported in published papers when\npossible or use publicly available source code (Karimi Mahabadi et al., 2021;\nMahabadi et al., 2021; Asai et al., 2022) under the same backbone and\nexperimental settings for a fair comparison.\n\nImplementation details. For source training, we train MPT on the mixture of\nsource tasks for 5 epochs with the examples-proportional mixing strategy (Raffel\net al., 2020) and stochastic task sampling described in Section 3.2. For prompt\ndistillation, we calculate the hidden state loss for hidden states from both the\nencoder and decoder of T5. For target adaptation, we reuse the shared prompt\nfrom MPT and take averaged source task-specific vectors to initialize the target\ntask-specific vector. We run all the experiments three times with different\nrandom seeds and report the mean and standard deviations. In few-shot\nexperiments, for each number of shots k, we randomly sample 10 times from the\ntraining set with different random seeds and report the mean performances. Note\nthat for few-shot learning, the source prompt learning still uses the full set\nof the source tasks. See Appendix A for the full experimental setup including\nhyperparameters."
        },
        {
            "section_title": "4.2 RESULTS AND ANALYSIS",
            "text": "Full-dataset adaptation. Tables 1 and 2 show the per-task performance of\ndifferent methods on all four benchmarks. As seen from Table 1 (top), MPT\nestablishes new state-of-the-art results for parameter-efficient finetuning on\nboth GLUE and SuperGLUE. When compared to vanilla PT (Lester et al., 2021), MPT\nobtains a relative improvement of 13% on GLUE and 16% on SuperGLUE with the same\nnumber of task-specific parameters, highlighting the benefits of transferring\nknowledge from multiple source tasks. MPT also consistently outperforms other\nparameter-efficient methods such as SPoT (Vu et al., 2022), ATTEMPT (Asai et\nal., 2022), and BitFit (Zaken et al., 2022), despite updating far fewer\nparameters. Adapters is the most competitive in terms of average accuracy on\nboth benchmarks, but MPT is far more parameter efficient and requires 4\u00d7 fewer\ntask-specific parameters. More surprisingly, MPT outperforms the full finetuning\nbaseline on both benchmarks, despite tuning 0.035% as many task-specific\nparameters. See Figure 2 for the comparison against different methods in terms\nof accuracy and parameter-efficiency.\n\nTable 1 (bottom) shows the results when finetuning against a group of target\ntasks. ATTEMPT and MPT are particularly performant in this setting, even when\ncompared against state-of-the-art multitask baselines such as HyperFormer\n(Mahabadi et al., 2021) and HyperDecoder (Ivison & Peters, 2022), which train a\nsingle model on different target tasks. This reveals the potential of our MPT to\nfurther leverage multitask knowledge on the target side, enabling even more\nparameter-efficient adaptation of pretrained language models.\n\nTable 2 shows the performance of different methods on the MRQA and Others\nbenchmark. Our approach significantly improves the average performance of PT by\n+2.8% on MRQA and +13.5% on the Others benchmark, while adding only 0.01% more\ntask-specific parameters. Similarly, MPT obtains 85.5% average accuracy on\nWinoGrande, Yelp, SciTail, and PAWS, outperforming BitFit (84.7%), which updates\n10\u00d7 more task-specific parameters. When we increase the prompt length from 100\nto 300, we also found an average improvement of 0.8% on MRQA and 0.6% on Others,\nclosing the gap between MPT and Adapters. While our improvements being highly\nparameter-efficient are encouraging, the accuracy gap between MPT and the full\nfinetuning is still significant in MRQA, which indicates opportunities for\nfuture work in multitask prompt tuning.\n\nFew-shot adaptation. Following prior works (Mahabadi et al., 2021; Asai et al.,\n2022), we first conduct few-shot experiments on BoolQ, CB, and SciTail tasks to\nmeasure how the pretrained MPT prompts can be generalized to new tasks with only\na few training examples available (k = 4, 16, 32). Table 3 shows the results of\nour approach and other baselines, which includes full finetuning, Adapters,\nHyperFormer, PT, and SPoT. As can be seen from Table 3, vanilla PT struggles for\nfew-shot adaptation (esp., CB and SciTail), suggesting that randomly initialized\nprompts are hard to generalize to new tasks with only a few labeled examples.\nSPoT improves the performance of PT on CB and SciTail tasks, and MPT outperforms\nboth PT and SPoT. We also observe that other methods in Table 3 (Finetuning,\nAdapters, HyperFormer, and ATTEMPT) have trouble in the few-shot setting.\nMoreover, Table 4 shows the few-shot learning performance comparison between PT\nand MPT on all the GLUE and SuperGLUE tasks. As shown in Table 4, we can observe\nthat not only MPT outperforms the vanilla PT by a large margin in most of the\ndatasets, but also MPT can perform very well on many datasets to reach their\nfull-dataset performance with 16 or 32 shots, such as QQP, QNLI, STS-B, and WSC.\nThese results clearly indicate that MPT can effectively use cross-task knowledge\nin source tasks to target tasks where there are only a few labeled examples.\n\nNatural language generation tasks. We next conduct experiments to test whether\nprompt decomposition learned from source NLU tasks can generalize to target NLG\ntasks. We transfer the T5-Large prompt trained using 6 diverse source tasks to\ntwo NLG tasks: E2E (Novikova et al., 2017) and WebNLG (Gardent et al., 2017).\nTable 5 shows that our proposed MPT significantly outperforms standard PT\n(Lester et al., 2021) on both NLG tasks across all the metrics. Our BLEU\nimprovements over PT are 3.03% and 6.25% on E2E and WebNLG tasks respectively,\nshowing the effectiveness of our approach on both NLU (e.g., classification,\nNLI, QA tasks) and NLG tasks. This is an impressive result, particularly since\nthe source tasks are all NLU tasks, i.e., MPT can transfer knowledge from NLU\ntasks to NLG tasks.\n\nModel scaling. We conduct scaling experiments to analyze how MPT performs with\nincreasing pretrained model sizes on three SuperGLUE tasks as in Asai et al.\n(2022). Figure 4 (left) shows the performance of MPT as well as full finetuning\n(FT), Adapter, PT, and ATTEMPT with three different T5 models (T5-Small,\nT5-Base, T5-Large). These results show that MPT is not only able to achieve the\nbest parameter efficiency, but also is effective across different model scales\nranging from 60M to 770M parameters.\n\nAnalyzing prompt matrices. We conduct qualitative analyses on prompts learned\nusing MPT to investigate whether cross-task knowledge is indeed encoded in the\ntask-shared prompt, making it easier for target tasks to effectively adapt and\nencode their own knowledge. Following Vu et al. (2022), we use the prompt\nmatrices to compute cosine similarities between all pairs of target tasks after\nadaptation, where each task is represented by the composition of task-shared and\ntask-specific prompts (averaged to obtain a single vector). Figure 4 (right)\nshows the visualization of cosine similarity matrices for SPoT and MPT on\nSuperGLUE tasks. We find that task embeddings can effectively cluster similar\ntasks together (e.g., MultiRC is similar to BoolQ)."
        },
        {
            "section_title": "4.3 ABLATION STUDIES",
            "text": "Prompt decomposition and distillation. Table 6 presents the results on SuperGLUE\nwhere we fix all the hyper-parameters across all settings and rerun MPT source\ntraining to get various ablated versions of the transferred prompt.\n\nTo measure the effect of prompt decomposition, we replace the vanilla source\nprompt with our decomposable prompt of task-shared and task-specific components\nand train it without prompt distillation (third row in Table 6), which gives us\n3.5% average performance improvement on SuperGLUE over the baseline (first row\nin Table 6). This ablation clearly demonstrates the importance of the prompt\ndecomposition strategy in MPT and shows that the shared component can\neffectively capture the rich cross-task knowledge that is beneficial for target\ndownstream tasks.\n\nTo test the effect of prompt distillation, we train a vanilla prompt shared by\nall the source tasks with the same training loss of MPT in Equation 5. The\nteacher models are kept the same for this ablation and MPT. Compared with the\nsimple baseline (first row in Table 6), adding prompt distillation (second row)\nproduces a 1.1% average performance improvement. Furthermore, we observe that\nprompt distillation combined with prompt decomposition yields the best average\nperformance of 74.1% on the SuperGLUE benchmark. This confirms that distilling\nknowledge from separately-trained source prompts is an effective strategy for\nlearning good decomposable prompts.\n\nDistillation objective. We further investigate the individual components of\nprompt distillation to measure their influences on the final performance. We\nremove the loss of hidden states from Equation 5 and find that it produces an\naverage performance of 73.7% on SuperGLUE, verifying the effectiveness of\nregularizing hidden states in conjunction with logits to reach its full\nperformance, which is consistent with findings in Sanh et al. (2019). Finally,\nwe consider a variant of distillation loss to match the teacher and student\nprompts directly by adding an MSE loss to minimize the distance between the two\nprompts. Replacing our proposed distillation losses with this prompt distance\nloss and jointly training it with prompt decomposition yield an average\nSuperGLUE performance of 73.6%, which performs worse than the distillation\nlosses based on logits and hidden states.\n\nPrompt length. While our experiments use l = 100 prompt vectors, we show in\nFigure 5 that using longer prompts obtains improvements up to l = 300, reaching\n76.8% on SuperGLUE. However, further increasing the prompt length from 300 to\n400 leads to an absolute 1.8% drop in accuracy, possibly due to overfitting.\n\nTarget adaptation strategy. When transferring the shared prompt from source to\ntarget tasks, we find that only updating task-shared component (i.e., removing\ntask-specific vectors) or only updating task-specific vectors (i.e., freezing\ntask-shared component) produces suboptimal results (62.5% and 71.3% on\nSuperGLUE). This shows the importance of updating both components (which have\ndifferent learning rates) for target adaptation.\n\nStochastic task sampling. MPT uses a multitask training strategy in Section 3.2,\nwhich stochastically samples a number of tasks within each mini-batch. Ablating\nthe stochastic task sampling results in 73.7% on SuperGLUE (lower than the full\nperformance of 74.1%), which demonstrates the slight benefit of this simple\nmultitask training strategy.\n\nNumber of source tasks for pretraining. For our main experiments, we selected 6\nNLP tasks following Asai et al. (2022). To investigate the effect of more source\ntasks, we incorporated 6 additional diverse source tasks on top of the original\n6 tasks, including topic classification (AGNews (Zhang et al., 2015)),\nmulti-choice QA (CommmonsenseQA (Talmor et al., 2019), OpenBookQA (Mihaylov et\nal., 2018), ARC (Clark et al., 2018)), adversarial NLI (ANLI (Nie et al., 2020))\nand commonsense reasoning (Winogrande (Sakaguchi et al., 2021)). Table 7 shows\nthe results on MRQA and Others benchmarks. MPT with 12 tasks is still quite\neffective for target adaptation on both benchmarks, slightly outperforming MPT\ntrained using 6 tasks. While it is unclear how much MPT would benefit from even\nmore source tasks, it would be interesting to see whether MPT trained on\nlarge-scale benchmarks such as CrossFit (Ye et al., 2021)-which consist of 160\nNLP tasks-can enable even more parameter-efficient (and accurate) transfer\nlearning."
        },
        {
            "section_title": "5 Conclusion",
            "text": "We introduced and studied multitask prompt tuning (MPT), which learns a single\ntransferable prompt by decomposing and distilling knowledge from multiple source\ntasks and their task-specific source prompts. MPT decomposes the task prompt as\nthe Hadamard product of a shared prompt matrix and a rank-one task-specific\nmatrix. The shared component is then transferred and adapted to target tasks for\nfurther tuning. Empirically we found this approach enables parameter-efficient\ntransfer learning to target downstream tasks across diverse NLP benchmarks, even\noutperforming the full finetuning baseline in some cases, despite tuning much\nfewer task-specific parameters."
        }
    ],
    "addenda": [
        {
            "section_title": "A Experimental Setup",
            "text": "For initial training of source prompts, we train MPT on the mixture of source\ntasks for 5 epochs with the examples-proportional mixing strategy (Raffel et\nal., 2020) and stochastic task sampling. For prompt distillation, we calculate\nthe hidden state loss for hidden states from both the encoder and decoder of T5.\nFor target adaptation, we reuse the shared prompt from MPT and take averaged\nsource task-specific vectors to initialize the target task-specific vector. We\ntrain 20 epochs on small datasets, 10 epochs on large (more than 10k examples)\ndatasets, and 5 epochs on the MRQA datasets. We run all the experiments three\ntimes with different random seeds and report the mean and standard deviations.\nIn few-shot experiments, for each number of shots k, we randomly sample 10 times\nfrom the training set with different random seeds and report the mean\nperformances. For the few-shot setting, the source prompt learning still uses\nthe full set of the source tasks.\n\nDuring source training, we set the default learning rate as 0.3 for both\ntask-shared and task-specific components. However, during target adaptation, we\nuse a strategy of two-speed learning rates for those two components, as in Ponti\net al. (2022). Specifically, we set the learning rate to 0.3 and 0.4 for the\ntask-shared and task-specific components, respectively, during target task\nadaptation. Following Lester et al. (2021), we set the default number of tunable\ntokens per each prompt to 100 and initialize the teacher and student prompts by\nrandomly sampling tokens from T5's vocabulary (Raffel et al., 2020). We set the\ndefault batch size for T5-Base as 32 and for model scaling experiments, the\nbatch sizes for T5-Small and T5-Large are 100, and 12 respectively. The default\ninput length for most tasks are set to 256, except MultiRC and MRQA benchmarks\nhave input length of 348 and 512. We set the distillation loss coefficient \u03bb in\nEquation 5 to 0.9 and keep it fixed for all our experiments.\n\nFor all datasets, we use the development set as the testing set if the original\ntesting set is not publicly available. If the training set is small, we split\nthe original development set into the development and testing set; otherwise, we\nseparate a development set from the training set and use the original\ndevelopment set for testing. We limit the number of training data for Yelp to\n100k."
        }
    ],
    "abstract": "Prompt tuning, in which a base pretrained model is adapted to each task via\nconditioning on learned prompt vectors, has emerged as a promising approach for\nefficiently adapting large language models to multiple downstream tasks.\nHowever, existing methods typically learn soft prompt vectors from scratch, and\nit has not been clear how to exploit the rich cross-task knowledge with prompt\nvectors in a multitask learning setting. We propose multitask prompt tuning\n(MPT), which first learns a single transferable prompt by distilling knowledge\nfrom multiple task-specific source prompts. We then learn multiplicative low\nrank updates to this shared prompt to efficiently adapt it to each downstream\ntarget task. Extensive experiments on 23 NLP datasets demonstrate that our\nproposed approach outperforms the state-of-the-art methods, including the full\nfinetuning baseline in some cases, despite only tuning 0.035% as many\ntask-specific parameters.(Project page: https://zhenwang9102.github.io/mpt.html)",
    "acknowledgements": "We are grateful to the anonymous reviewers for their constructive comments and\nsuggestions. ZW sincerely thanks Peihao Wang for the insightful discussion\nduring the internship. YK was partially supported by an MIT-IBM Watson AI grant.\nWe also acknowledge support from the IBM Research AI Hardware Center and the\nCenter for Computational Innovation at Rensselaer Polytechnic Institute for the\ncomputational resources on the AiMOS Supercomputer.",
    "references": "Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke\nZettlemoyer, and Sonal Gupta. Muppet: Massive multi-task representations with\npre-finetuning. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pp. 5799-5811, 2021a.\n\nArmen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality\nexplains the effectiveness of language model fine-tuning. In Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing (Volume 1:\nLong Papers), pp. 7319-7328, 2021b.\n\nAkari Asai, Mohammadreza Salehi, Matthew E Peters, and Hannaneh Hajishirzi.\nAttentional mixtures of soft prompt tuning for parameter-efficient multi-task\nknowledge sharing. arXiv preprint arXiv:2205.11961, 2022.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia.\nSemeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual\nfocused evaluation. arXiv preprint arXiv:1708.00055, 2017.\n\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins,\nand Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural\nyes/no questions. arXiv preprint arXiv:1905.10044, 2019a.\n\nKevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D Manning, and\nQuoc Le. Bam! born-again multi-task networks for natural language understanding.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pp. 5931-5937, 2019b.\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa\nSchoenick, and Oyvind Tafjord. Think you have solved question answering? try\narc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.\n\nMarie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The\ncommitmentbank: Investigating projection in naturally occurring discourse. In\nproceedings of Sinn und Bedeutung, volume 23, pp. 107-124, 2019.\n\nDorottya Demszky, Kelvin Guu, and Percy Liang. Transforming question answering\ndatasets into natural language inference datasets. arXiv preprint\narXiv:1809.02922, 2018.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:\nPre-training of deep bidirectional transformers for language understanding. In\nProceedings of NAACL, 2019.\n\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su,\nShengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Delta tuning: A\ncomprehensive study of parameter efficient methods for pre-trained language\nmodels. arXiv preprint arXiv:2203.06904, 2022.\n\nBill Dolan and Chris Brockett. Automatically constructing a corpus of sentential\nparaphrases. In Third International Workshop on Paraphrasing (IWP2005), 2005.\n\nMatthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and\nKyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search\nengine. arXiv preprint arXiv:1704.05179, 2017.\n\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen.\nMRQA 2019 shared task: Evaluating generalization in reading comprehension. In\nProceedings of 2nd Machine Reading for Reading Comprehension (MRQA) Workshop at\nEMNLP, 2019.\n\nTommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima\nAnandkumar. Born again neural networks. In International Conference on Machine\nLearning, pp. 1607-1616, 2018.\n\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models\nbetter few-shot learners. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers), pp.\n3816-3830, 2021.\n\nClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura\nPerez-Beltrachini. The webnlg challenge: Generating text from rdf data. In\nProceedings of the 10th International Conference on Natural Language Generation,\n2017.\n\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and William B Dolan. The third\npascal recognizing textual entailment challenge. In Proceedings of the\nACL-PASCAL workshop on textual entailment and paraphrasing, pp. 1-9, 2007.\n\nJianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge\ndistillation: A survey. International Journal of Computer Vision,\n129(6):1789-1819, 2021.\n\nDemi Guo, Alexander M. Rush, and Yoon Kim. Parameter-efficient transfer learning\nwith diff pruning. In Proceedings of ACL, 2021.\n\nYun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao,\nYaGuang Li, Zhao Chen, Donald Metzler, et al. Hyperprompt: Prompt-based\ntask-conditioning of transformers. In International Conference on Machine\nLearning, pp. 8678-8690, 2022.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a\nneural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.\n\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De\nLaroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.\nParameter-efficient transfer learning for nlp. In International Conference on\nMachine Learning, pp. 2790-2799, 2019.\n\nJeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text\nclassification. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 328-339, 2018.\n\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nWeizhu Chen, et al. Lora: Low-rank adaptation of large language models. In\nInternational Conference on Learning Representations, 2021.\n\nMinghao Hu, Yuxing Peng, Furu Wei, Zhen Huang, Dongsheng Li, Nan Yang, and Ming\nZhou. Attention-guided answer distillation for machine reading comprehension. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, pp. 2077-2086, 2018.\n\nHamish Ivison and Matthew E Peters. Hyperdecoders: Instance-specific decoders\nfor multi-task nlp. arXiv preprint arXiv:2203.08304, 2022.\n\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang\nWang, and Qun Liu. Tinybert: Distilling bert for natural language understanding.\nIn Findings of the Association for Computational Linguistics: EMNLP 2020, pp.\n4163-4174, 2020.\n\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter:\nEfficient low-rank hypercomplex adapter layers. Advances in Neural Information\nProcessing Systems, 34:1022-1035, 2021.\n\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth.\nLooking beyond the surface: A challenge set for reading comprehension over\nmultiple sentences. In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pp. 252-262, 2018.\n\nTushar Khot, Ashish Sabharwal, and Peter Clark. Scitail: A textual entailment\ndataset from science question answering. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 32, 2018.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton\nLee, et al. Natural questions: a benchmark for question answering research.\nTransactions of the Association for Computational Linguistics, 7:453-466, 2019.\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for\nparameter-efficient prompt tuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, pp. 3045-3059, 2021.\n\nHector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema\nchallenge. In Thirteenth international conference on the principles of knowledge\nrepresentation and reasoning, 2012.\n\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the\nintrinsic dimension of objective landscapes. In International Conference on\nLearning Representations, 2018.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for\ngeneration. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pp. 4582-4597, 2021.\n\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal,\nand Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper\nthan in-context learning. arXiv preprint arXiv:2205.05638, 2022a.\n\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham\nNeubig. Pretrain, prompt, and predict: A systematic survey of prompting methods\nin natural language processing. arXiv preprint arXiv:2107.13586, 2021a.\n\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang.\nP-tuning v2: Prompt tuning can be comparable to fine-tuning universally across\nscales and tasks. arXiv preprint arXiv:2110.07602, 2021b.\n\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie\nTang. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and\ntasks. In Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers), pp. 61-68, 2022b.\n\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson.\nParameter-efficient multi-task fine-tuning for transformers via shared\nhypernetworks. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pp. 565-576, 2021.\n\nNikolay Malkin, Zhen Wang, and Nebojsa Jojic. Coherence boosting: When your\npretrained language model is not paying enough attention. In Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pp. 8214-8236, 2022.\n\nYuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott\nYih, and Madian Khabsa. Unipelt: A unified framework for parameter-efficient\nlanguage model tuning. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp.\n6253-6264, 2022.\n\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of\narmor conduct electricity? a new dataset for open book question answering. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, pp. 2381-2391, 2018.\n\nIshan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert.\nCross-stitch networks for multi-task learning. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 3994-4003, 2016.\n\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe\nKiela. Adversarial nli: A new benchmark for natural language understanding. In\nProceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 4885-4901, 2020.\n\nJekaterina Novikova, Ond\u02c7rej Du\u02c7sek, and Verena Rieser. The e2e dataset: New\nchallenges for end-to-end generation. arXiv preprint arXiv:1706.09254, 2017.\n\nMohammad Taher Pilehvar and Jose Camacho-Collados. Wic: the word-in-context\ndataset for evaluating context-sensitive meaning representations. arXiv preprint\narXiv:1808.09121, 2018.\n\nEdoardo M Ponti, Alessandro Sordoni, and Siva Reddy. Combining modular skills in\nmultitask learning. arXiv preprint arXiv:2202.13914, 2022.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer\nlearning with a unified text-to-text transformer. J. Mach. Learn. Res.,\n21(140):1-67, 2020.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad:\n100,000+ questions for machine comprehension of text. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing, pp. 2383-2392,\n2016.\n\nSebastian Ruder. An overview of multi-task learning in deep neural networks.\narXiv preprint arXiv:1706.05098, 2017.\n\nSebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders S\u00f8gaard. Latent\nmulti-task architecture learning. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 33, pp. 4822-4829, 2019.\n\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.\nWinogrande: An adversarial winograd schema challenge at scale. Communications of\nthe ACM, 64(9):99-106, 2021.\n\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a\ndistilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint\narXiv:1910.01108, 2019.\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid\nAlyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al.\nMultitask prompted training enables zero-shot task generalization. In The Tenth\nInternational Conference on Learning Representations, 2022.\n\nTimo Schick and Hinrich Sch\u00a8utze. Exploiting cloze-questions for few-shot text\nclassification and natural language inference. In Proceedings of the 16th\nConference of the European Chapter of the Association for Computational\nLinguistics: Main Volume, pp. 255-269, 2021a.\n\nTimo Schick and Hinrich Sch\u00a8utze. It's not just size that matters: Small\nlanguage models are also few-shot learners. In Proceedings of the 2021\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pp. 2339-2352, 2021b.\n\nSiamak Shakeri, Abhinav Sethy, and Cheng Cheng. Knowledge distillation in\ndocument retrieval. arXiv preprint arXiv:1911.11065, 2019.\n\nSam Shleifer and Alexander M Rush. Pre-trained summarization distillation. arXiv\npreprint arXiv:2010.13002, 2020.\n\nJanvijay Singh, Fan Bai, and Zhen Wang. Frustratingly simple entity tracking\nwith effective use of multi-task learning models. arXiv preprint\narXiv:2210.06444, 2022.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,\nAndrew Y Ng, and Christopher Potts. Recursive deep models for semantic\ncompositionality over a sentiment treebank. In Proceedings of the 2013\nconference on empirical methods in natural language processing, pp. 1631-1642,\n2013.\n\nYusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang,\nKaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al. On transferability of prompt\ntuning for natural language processing. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pp. 3949-3969, 2022.\n\nXimeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko. Adashare: Learning\nwhat to share for efficient deep multi-task learning. Advances in Neural\nInformation Processing Systems, 33: 8728-8740, 2020.\n\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder side-tuning for parameter\nand memory efficient transfer learning. In Advances in Neural Information\nProcessing Systems.\n\nYi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with fixed\nsparse masks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J.\nWortman Vaughan (eds.), Advances in Neural Information Processing Systems,\nvolume 34, pp. 24193-24205. Curran Associates, Inc., 2021. URL\nhttps://proceedings.neurips.cc/paper/2021/file/\ncb2653f548f8709598e8b5156738cc51-Paper.pdf.\n\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.\nCommonsenseqa: A question answering challenge targeting commonsense knowledge.\nIn Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pp. 4149-4158, 2019.\n\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni,\nPhilip Bachman, and Kaheer Suleman. Newsqa: A machine comprehension dataset. In\nProceedings of the 2nd Workshop on Representation Learning for NLP, pp. 191-200,\n2017.\n\nTu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler,\nAndrew Mattarella-Micke, Subhransu Maji, and Mohit Iyyer. Exploring and\npredicting transferability across nlp tasks. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP), pp.\n7882-7926, 2020.\n\nTu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. Spot: Better\nfrozen model adaptation through soft prompt transfer. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 5039-5059, 2022.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel\nBowman. Glue: A multi-task benchmark and analysis platform for natural language\nunderstanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pp. 353-355, 2018.\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for\ngeneral-purpose language understanding systems. Advances in neural information\nprocessing systems, 32, 2019.\n\nChengyu Wang, Jianing Wang, Minghui Qiu, Jun Huang, and Ming Gao. Transprompt:\nTowards an automatic transferable prompting framework for few-shot text\nclassification. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pp. 2792-2802, 2021.\n\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza\nMirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik,\nDavid Stap, et al. Benchmarking generalization via in-context instructions on\n1,600+ language tasks. arXiv preprint arXiv:2204.07705, 2022.\n\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network\nacceptability judgments. Transactions of the Association for Computational\nLinguistics, 7:625-641, 2019.\n\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester,\nNan Du, An-drew M Dai, and Quoc V Le. Finetuned language models are zero-shot\nlearners. In International Conference on Learning Representations, 2021.\n\nYeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to\nefficient ensemble and lifelong learning. In International Conference on\nLearning Representations, 2020.\n\nAdina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge\ncorpus for sentence understanding through inference. arXiv preprint\narXiv:1704.05426, 2017.\n\nGuodong Xu, Ziwei Liu, Xiaoxiao Li, and Chen Change Loy. Knowledge distillation\nmeets self-supervision. In European Conference on Computer Vision, pp. 588-604,\n2020.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\nSalakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse,\nexplainable multi-hop question answering. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing, pp. 2369-2380, 2018.\n\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. Crossfit: A few-shot learning\nchallenge for cross-task generalization in nlp. arXiv preprint arXiv:2104.08835,\n2021.\n\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple\nparameter-efficient fine-tuning for transformer-based masked language-models. In\nProceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pp. 1-9, 2022.\n\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin\nVan Durme. Record: Bridging the gap between human and machine commonsense\nreading comprehension. arXiv preprint arXiv:1810.12885, 2018.\n\nXiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks\nfor text classification. Advances in neural information processing systems, 28,\n2015.\n\nYu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on\nKnowledge and Data Engineering, 2021.\n\nYuan Zhang, Jason Baldridge, and Luheng He. Paws: Paraphrase adversaries from\nword scrambling. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 1298-1308, 2019.\n\nQihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. Panda: Prompt\ntransfer meets knowledge distillation for efficient model adaptation. arXiv\npreprint arXiv:2208.10160, 2022.\n\nRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models\nfor zero-shot learning by meta-tuning on dataset and prompt collections. In\nFindings of the Association for Computational Linguistics: EMNLP 2021, pp.\n2856-2878, 2021.\n\nChunting Zhou, Jiatao Gu, and Graham Neubig. Understanding knowledge\ndistillation in non-autoregressive machine translation. In International\nConference on Learning Representations, 2019.",
    "images": {
        "Figure_1_block_35_354d6922": {
            "source_page_num": 0,
            "caption": "Figure 1: A conceptual overview of our approach. Instead of retrieving or\naggregating source prompts (top), multitask prompt tuning (MPT, bottom) learns a\nsingle transferable prompt. The transferable prompt is learned via prompt\ndecomposition and distillation.",
            "box": [
                301.0,
                466.47894287109375,
                509.00494384765625,
                630.511474609375
            ],
            "adjacent_neighbor": 7,
            "image_file": "images/Figure_1_block_35_354d6922.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/ebe4fd49a9aa82dc6f1377b6a3c6e043.png"
        },
        "Figure_2_block_18_e98ec95e": {
            "source_page_num": 1,
            "caption": "Figure 2: Parameter efficiency on GLUE (left) and SuperGLUE (right). Our\nmultitask prompt tuning (MPT) approach, which transfers a single shared prompt\nlearned from multiple source tasks using prompt decomposition and distillation,\nmaintains high accuracy (y-axis) while finetuning only a small number of\nparameters per task (x-axis). All results are based on T5-Base (Raffel et al.,\n2020). Baselines include: Adapters (Houlsby et al., 2019), BitFit (Zaken et al.,\n2022), PT (Lester et al., 2021), SPoT (Vu et al., 2022), and ATTEMPT (Asai et\nal., 2022).^{*}Indicates multitask training on target tasks. Best viewed in\ncolor.",
            "box": [
                103.0,
                40,
                509.00494384765625,
                203.46949768066406
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Figure_2_block_18_e98ec95e.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/6f228b031147750a93bbbb90c5223ed6.png"
        },
        "Figure_3_block_11_7d69ce07": {
            "source_page_num": 3,
            "caption": "Figure 3: An illustration on prompt decomposition for two tasks. The shared\nmatrix P^{\u22c6} is combined with task-specific vectors u_{k}, v_{k} to obtain the\ntask-specific prompt matrices \ufffd P_{k} for k \u2208 {1, 2}.",
            "box": [
                340.60198974609375,
                142.98313903808594,
                509.0044250488281,
                337.1255187988281
            ],
            "adjacent_neighbor": 1,
            "image_file": "images/Figure_3_block_11_7d69ce07.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/2b2f87c4ca7447d409a5f54f175c4d18.png"
        },
        "Table_1_block_1_de535d75": {
            "source_page_num": 5,
            "caption": "Table 1: Results on GLUE and SuperGLUE. The metrics are Pearson correlation for\nSTS-B, F1 for MultiRC (Multi), and accuracy for other tasks as evaluation\nmetrics. MPT results are averaged over three runs, and subscripts denote\nstandard deviation. The column \"param/task\" represents the number of trainable\nparameters for each task in GLUE. (Top) Model adaptation to each target task\nwith no parameter sharing on the target side (so params/task for MPT is just (l\n\u00d7 d) + (l + d)). (Bottom) Model adaptation to a group of tasks (marked by ^{*}),\nwhere param/task for MPT * is (l \u00d7 d)/\u03c4 + (l + d). See Section 3.2 for more\ndetails.",
            "box": [
                107.99996948242188,
                137.93093872070312,
                504.00360107421875,
                231.24046325683594
            ],
            "adjacent_neighbor": 7,
            "image_file": "images/Table_1_block_1_de535d75.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/4bda1c398afa78ab095902e91771157a.png"
        },
        "Table_2_block_7_35d3c2d3": {
            "source_page_num": 5,
            "caption": "Table 2: Results on MRQA and Others. We use F1 for MRQA tasks and accuracy for\nothers as the evaluation metrics. MPT results are averaged over three runs and\nsubscripts indicate standard deviation.",
            "box": [
                107.99996948242188,
                267.9699401855469,
                504.00360107421875,
                350.0169677734375
            ],
            "adjacent_neighbor": 11,
            "image_file": "images/Table_2_block_7_35d3c2d3.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/ec5da92922b2458ea618aedf545ae938.png"
        },
        "Table_3_block_1_8be67ad6": {
            "source_page_num": 6,
            "caption": "Table 3: Few-shot learning results with k = {4, 16, 32} on BoolQ, CB, and\nSciTail. FT: Finetuning, AD: Adapters, PT: Prompt tuning, ST: SPoT, HF:\nHyperFormer, ATP: ATTEMPT. Numbers in brackets denote the number of parameters\ntuned for each task. MPT is very competitive or even better than existing\nmethods in the majority of the cases while tuning much fewer task-specific\nparameters.",
            "box": [
                108.0,
                112.3108901977539,
                504.0048828125,
                209.56626892089844
            ],
            "adjacent_neighbor": 15,
            "image_file": "images/Table_3_block_1_8be67ad6.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/31d7e9fca35b0389c24ca205e773119a.png"
        },
        "Table_4_block_15_41093e4d": {
            "source_page_num": 6,
            "caption": "Table 4: Few-shot learning results on GLUE and SuperGLUE for vanilla prompt\ntuning (PT) and MPT with 4, 16, and 32 training examples. MPT consistently\noutperforms PT, demonstrating the generalizability of MPT prompts to new tasks\nwith only a few training examples.",
            "box": [
                108.0,
                261.4848937988281,
                504.0048828125,
                322.0492248535156
            ],
            "adjacent_neighbor": 21,
            "image_file": "images/Table_4_block_15_41093e4d.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/78ce99c8b5c5cf5d33c6fdabcb633df3.png"
        },
        "Table_5_block_1_7d7b160d": {
            "source_page_num": 7,
            "caption": "Table 5: Results on NLG tasks. The source prompt decomposition is learned\nagainst NLU tasks and adapted to target NLG tasks. MPT consistently outperforms\nPT on both tasks.",
            "box": [
                108.0,
                95.2199478149414,
                504.0048522949219,
                136.945556640625
            ],
            "adjacent_neighbor": 5,
            "image_file": "images/Table_5_block_1_7d7b160d.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/7a299b90833425af5291a7cbb840f20d.png"
        },
        "Figure_4_block_63_cb889f6f": {
            "source_page_num": 7,
            "caption": "Figure 4: (Left) Performance of various baselines as a function of model size\n(from T5-Small to T5-Large). (Right) Correlation of prompt matrices on SuperGLUE\ntasks. Best viewed in color.",
            "box": [
                103.00003051757812,
                100.2199478149414,
                509.0048522949219,
                276.0985107421875
            ],
            "adjacent_neighbor": 1,
            "image_file": "images/Figure_4_block_63_cb889f6f.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/1939625e55c12baab0f7768f3ff7fbec.png"
        },
        "Table_6_block_69_06badda3": {
            "source_page_num": 7,
            "caption": "Table 6: Ablation results on prompt decomposition and distillation.",
            "box": [
                306.00242614746094,
                605.0568237304688,
                504.0048522949219,
                650.4877319335938
            ],
            "adjacent_neighbor": 72,
            "image_file": "images/Table_6_block_69_06badda3.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/a10ad522a5e4ea9d987e33b89517196e.png"
        },
        "Table_7_block_1_7882a1ac": {
            "source_page_num": 8,
            "caption": "Table 7: MPT performance on MRQA and Others with more source tasks.",
            "box": [
                107.99999237060547,
                91.18614959716797,
                504.0033874511719,
                135.40969848632812
            ],
            "adjacent_neighbor": 5,
            "image_file": "images/Table_7_block_1_7882a1ac.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/6321e0c404a59bdf6061eead64efbffe.png"
        },
        "Figure_5_block_20_2655fd32": {
            "source_page_num": 8,
            "caption": "Figure 5: Performance on SuperGLUE as a function of prompt length for PT and\nMPT.",
            "box": [
                372.2770080566406,
                294.824462890625,
                509.00006103515625,
                440.2855224609375
            ],
            "adjacent_neighbor": 6,
            "image_file": "images/Figure_5_block_20_2655fd32.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/eb93e8c89fcbab6797d4187fea0dafc2.png"
        }
    }
}