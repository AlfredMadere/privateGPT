{
    "title": "MemoryBank: Enhancing Large Language Models\nwith Long-Term Memory",
    "authors": "Wanjun Zhong^{1}, Lianghong Guo^{1}, Qiqi Gao^{2}, He Ye^{3}, Yanlin Wang^{1}\n^{1} Sun Yat-Sen University_{ 2} Harbin Institute of Technology\n^{3} KTH Royal Institute of Technology\n{zhongwj25@mail2, wangylin36@mail}.sysu.edu.com\n2231612405@qq.com, 18b903026@stu.hit.edu.cn\nheye@kth.se",
    "body": [
        {
            "section_title": "1 Introduction",
            "text": "The advent of Large Language Models (LLMs) such as ChatGPT (OpenAI, 2022) and\nGPT-4 (OpenAI, 2023) has led to increasing influence across various sectors,\nfrom education and healthcare to customer service and entertainment. These\npowerful AI systems have demonstrated a remarkable ability to understand and\ngenerate human-like responses. Despite the remarkable capabilities of LLMs, a\nkey limitation is their lack of long-term memory, an essential aspect of\nhuman-like communication, particularly noticeable in scenarios requiring\nsustained interactions like personal companionship, psychological counseling,\nand secretarial tasks. Long-term memory in AI is vital to maintain contextual\nunderstanding, ensure meaningful interactions and understand user behaviors over\ntime. For instance, personal AI companions need to recall past conversations for\nrapport building. In psychological counseling, an AI can provide more effective\nsupport with knowledge of the user's history and past emotional states.\nSimilarly, secretarial AI requires memory for task management and preference\nrecognition. The absence of long-term memory in LLMs hinders their performance\nand user experience. Therefore, it is essential to develop AI systems with\nimproved memory capabilities for a more seamless and personalized interaction.\n\nTherefore, we introduce MemoryBank, a novel mechanism designed to provide LLMs\nwith the ability to retain long-term memory and draw user portraits. MemoryBank\nenables LLMs to recall historical interactions, continually evolve their\nunderstanding of context, and adapt to a user's personality based on past\ninteractions, thereby enhancing their performance in long-term interaction\nscenarios. Inspired by the Ebbinghaus Forgetting Curve theory, a\nwell-established psychological principle that describes how the strength of\nmemory decreases over time, MemoryBank further incorporates a dynamic memory\nmechanism closely mirroring human cognitive process. This mechanism empowers the\nAI to remember, selectively forget, and strengthen memories based on time\nelapsed, offering more natural and engaging user experience. Specifically,\nMemoryBank is built on a memory storage with memory retrieval and updating\nmechanism, and ability to summarize past events and users' personality.\n\nMemoryBank is versatile as it can accommodate both closed-source LLMs like\nChatGPT and open-source LLMs like ChatGLM (Zeng et al., 2022) or BELLE (Yunjie\nJi & Li, 2023).\n\nTo exemplify the practical implications of MemoryBank, we develop SiliconFriend,\nan LLM-based AI Companion chatbot integrated with this innovative memory\nmechanism. SiliconFriend is designed to retain and reference past interactions,\nreinforcing the transformative influence of MemoryBank in crafting a more\npersonable AI companion. A distinctive features of SiliconFriend is its tuning\nwith 38k psychological conversations, collected from various online sources,\nwhich enables it to exhibit empathy, carefulness, and provide useful guidance,\nmaking it adept at handling emotionally charged dialogues. Moreover, one of the\nstandout capabilities of SiliconFriend is to understand a user's personality by\nsummarizing from past interactions, which empowers it to tailor responses to the\nuser's individual traits, thereby enhancing user experience. Additionally,\nSiliconFriend supports bilingual functionality, catering to users who\ncommunicate in English and Chinese. This multi-language support broadens its\naccessibility and usability across different user groups. SiliconFriend is\nimplemented with two open-source models, ChatGLM and BELLE, along with one\nclosed-source model, ChatGPT, showcasing the versatility of MemoryBank in\naccommodating different LLMs.\n\nTo evaluate the effectiveness of MemoryBank, we conduct evaluations covering\nboth qualitative and quantitative analyses, where the former involves real-world\nuser dialogs and the latter employs simulated dialogs. For the quantitative\nanalysis, we create a memory storage consisting of 10 days of conversations\nencompassing a diverse range of topics. These conversations involve 15 virtual\nusers with diverse personalities, for which ChatGPT plays the role of users and\ngenerates dialog contexts according to their personalities. Based on this memory\nstorage, we design 194 probing questions to assess whether the model could\nsuccessfully recall pertinent memories and provide appropriate responses.\nExperiment results showcase the capabilities of SiliconFriend in memory recall,\nprovision of empathetic companionship, and understanding of user portraits.\nThese findings corroborate the potential of MemoryBank to significantly improve\nthe performance of LLMs in long-term interaction scenarios. In this paper, we\nsummarize the key contributions as follows:\n\n\u2022 We introduce MemoryBank, a novel human-like long-term memory mechanism, which\nenables LLMs to store, recall, update memory, and draw user portrait.\n\n\u2022 We demonstrate the practical applicability of MemoryBank through\nSiliconFriend, an LLM-based AI companion equipped with MemoryBank and tuned with\npsychological dialogs. It can recall past memories, provide empathetic\ncompanionship, and understand user behaviors.\n\n\u2022 We show the generalizability of MemoryBank in three key aspects: (1)\nAccommodation of both open-source and closed-source LLMs; (2) Bilingual ability\nin both Chinese and English; (3) Applicability with and without memory\nforgetting mechanism."
        },
        {
            "section_title": "2 MemoryBank: A Novel Memory Mechanism Tailored for LLMs",
            "text": "In this section, we provide a detailed description of MemoryBank, our novel\nmemory mechanism designed for LLMs. As shown in Fig. 1, MemoryBank is a unified\nmechanism structured around three central pillars: (1) a memory storage (\u00a7 2.1)\nserving as the primary data repository, (2) a memory retriever (\u00a7 2.2) for\ncontext-specific memory recollection, and (3) a memory updater (\u00a7 2.3) drawing\ninspiration from the Ebbinghaus Forgetting Curve theory, a time-tested\npsychological principle pertaining to memory retention and forgetting."
        },
        {
            "section_title": "2.1 Memory Storage: The Warehouse of MemoryBank",
            "text": "Memory storage, the warehouse of MemoryBank, is a robust data repository holding\na meticulous array of information. As shown in Fig. 1, it stores daily\nconversations records, summaries of past events, and evolving assessments of\nuser personalities, thereby constructing a dynamic and multi-layered memory\nlandscape.\n\nIn-Depth Memory Storage: MemoryBank's storage system captures the richness of\nAI-user interactions by recording multi-turn conversations in a detailed,\nchronological fashion. Each piece of dialogue is stored with timestamps,\ncreating an ordered narrative of past interactions. This detailed record not\nonly aids in precise memory retrieval but also facilitates the memory updating\nprocess afterwards, offering a detailed index of conversational history.\n\nHierarchical Event Summary: Reflecting the intricacies of human memory,\nMemoryBank goes beyond mere detailed storage. It processes and distills\nconversations into a high-level summary of daily events, much like how humans\nremember key aspects of their experiences. We condense verbose dialogues into a\nconcise daily event summary, which is further synthesized into a global summary.\nThis process results in a hierarchical memory structure, providing a bird's eye\nview of past interactions and significant events. Specifically, taken previous\ndaily conversations or daily events as input, we ask the LLMs to summarize daily\nevents or global events with the prompt \"Summarize the events and key\ninformation in the content [dialog/events]\".\n\nDynamic Personality Understanding: MemoryBank focuses on user personality\nunderstanding. It continuously assesses and updates these understandings with\nthe long-term interactions and creates daily personality insights. These\ninsights are further aggregated to form a global understanding of the user's\npersonality. This multi-tiered approach results in an AI companion that learns,\nadapts, and tailors its responses to the unique traits of each user, enhancing\nuser experience. Specially, taken the daily conversations or personality\nanalysis, we ask the LLM to deduce with prompts: \"Based on the following\ndialogue, please summarize the user's personality traits and emotions.[dialog]\"\nor \"The following are the user's exhibited personality traits and emotions\nthroughout multiple days. Please provide a highly concise and general summary of\nthe user's personality[daily Personalities]\"."
        },
        {
            "section_title": "2.2 Memory Retrieval",
            "text": "Built on the robust infrastructure of memory storage, our memory retrieval\nmechanism operates akin to a knowledge retrieval task. In this context, we adopt\na dual-tower dense retrieval model similar to Dense Passage Retrieval (Karpukhin\net al., 2020). In this paradigm, every turn of conversations and event summaries\nis considered as a memory piece m, which is pre-encoded into a contextual\nrepresentation h_{m} using the encoder model E(\u00b7). Consequently, the entire\nmemory storage M is pre-encoded into M = {h^{0} _{m}, h^{1} _{m}, ...h^{|M|}\n^{m} }, where each h_{m} is a vector representation of a memory piece. These\nvector representations are then indexed using FAISS (Johnson et al., 2019) for\nefficient retrieval. Parallel to this, the current context of conversation c is\nencoded by E(\u00b7) into h_{c}, which serves as the query to search M for the most\nrelevant memory. In practice, the encoder E(\u00b7) can be interchanged to any\nsuitable model."
        },
        {
            "section_title": "2.3 Memory Updating Mechanism",
            "text": "With the persistent memory storage and the memory retrieval mechanism discussed\nin \u00a7 2.1 and \u00a7 2.2, the memorization capability of LLMs can be greatly enhanced.\nHowever, for scenarios that expect more anthropopathic memory behavior, memory\nupdating is needed. These scenarios include AI companion, virtual IP, etc.\nForgetting less important memory pieces that are long time ago and have not been\nrecalled much can make the AI companion more natural.\n\nOur memory forgetting mechanism is inspired from Ebbinghaus Forgetting Curve\ntheory and follow the following principle rules^{2}:\n\n\u2022 Rate of Forgetting. Ebbinghaus found that memory retention decreases over\ntime. He quantified this in his forgetting curve, showing that information is\nlost rapidly after learning unless it is consciously reviewed.\n\n\u2022 Time and Memory Decay. The curve is steep at the beginning, indicating that a\nsignificant amount of learned information is forgotten within the first few\nhours or days after learning. After this initial period, the rate of memory loss\nslows down.\n\n\u2022 Spacing Effect. Ebbinghaus discovered that relearning information is easier\nthan learning it for the first time. Regularly revisiting and repeating the\nlearned material can reset the forgetting curve, making it less steep and\nthereby improving memory retention.\n\nThe Ebbinghaus forgetting curve is expressed using an exponential decay model: R\n= e^{\u2212 t} ^{S} , where R is the memory retention, or what fraction of the\ninformation can be retained. t is the time elapsed since learning the\ninformation. e is approximately equal to 2.71828. S is the memory strength,\nwhich changes based on factors such as the depth of learning and the amount of\nrepetition. To simply memory updating process, we model S as a discrete value\nand initialize it with 1 upon its first mention in a conversation. When a memory\nitem is recalled during conversations, it will persist longer in memory. We\nincrease S by 1 and reset t to 0, hence forget it with a lower probability.\n\nIt is important to note that this is an exploratory and highly simplified memory\nupdating model. Real-life memory processes are more complex and can be\ninfluenced by a variety of factors. The forgetting curve will look different for\ndifferent people and different types of information. In summary, MemoryBank\nweaves together these critical components to form a more comprehensive memory\nmanagement system for LLMs. It enhances their ability to provide meaningful and\npersonalized interactions over extended periods, opening up new possibilities\nfor AI applications."
        },
        {
            "section_title": "3\nSiliconFriend: An AI Chatbot Companion Powered by MemoryBank\n",
            "text": "To demonstrate the practicality of MemoryBank in the field of long-term personal\nAI companionship, we create an AI chatbot named SiliconFriend. It is designed to\nserve as an emotional companion for users, recalling pertinent user memories,\nand understanding users' personalities and emotional states.\n\nOur implementation demonstrates adaptability by integrating three powerful LLMs\nthat originally lack long-term memory and specific adaptation to the psychology\ndomain. 1) ChatGPT (OpenAI, 2022), a closed-source conversation model built by\nOpenAI, is a proprietary conversational AI model known for its ability to\nfacilitate dynamic and interactive conversations. This model is trained on vast\namount of data and further fine-tuned with reinforcement learning from human\nfeedback. This approach enables ChatGPT to generate responses that are not only\ncontextually appropriate but also closely align with human conversational\nexpectations. 2) ChatGLM (Zeng et al., 2022): ChatGLM is an open-source\nbilingual language model founded on the General Language Model (GLM) framework.\nThis model is characterized by its 6.2 billion parameters and its specific\noptimization for Chinese dialogue data. The model's training involves processing\napproximately one trillion tokens of Chinese and English text, supplemented by\nsupervised fine-tuning, feedback bootstrap, and reinforcement learning with\nhuman feedback. 3) BELLE (Yunjie Ji & Li, 2023): BELLE is an open-source\nbilingual language model that is continuously fine-tuned from 7B LLaMA (Touvron\net al., 2023). BELLE's feature is its automated instruction data synthesis using\nChatGPT, which enhances its Chinese conversation ability.\n\nThe development of SiliconFriend is divided into two stages. The first stage\n(only for open-source LLMs) involves parameter-efficient tuning of the LLM with\npsychological dialogue data. This step is crucial as it allows SiliconFriend to\noffer useful and empathetic emotional support to users, mirroring the\nunderstanding and compassionate responses one would expect from a human\ncompanion. The second stage is to integrate MemoryBank into SiliconFriend,\nthereby instilling it with a robust memory system. MemoryBank allows the chatbot\nto retain, recall, and leverage past interactions and user portrait, providing a\nricher, more personalized user experience.\n\nParameter-efficient Tuning with Psychological Dialogue Data: The initial stage\nof Silicon-Friend's development involves tuning the LLMs using a dataset of 38k\npsychological dialogues. This data, parsed from online sources, comprises a\nrange of conversations that cover an array of emotional states and responses.\nThis tuning process enables SiliconFriend to understand and respond to emotional\ncues effectively, mimicking the empathy, understanding, and support of a human\ncompanion. It equips the AI with the ability to handle emotionally guided\nconversations with psychological knowledge, provide meaningful emotional support\nto users based on their emotional state.\n\nTo adapt LLMs to scenarios with limited computational resources, we utilize a\ncomputation-efficient tuning approach, known as the Low-Rank Adaptation (LoRA)\nmethod (Hu et al., 2021). LoRA significantly reduces the quantity of trainable\nparameters by learning pairs of rank-decomposition matrices, while keeping the\noriginal weights frozen. Formally, consider a linear layer defined as y = Wx\nwith weight W. LoRA modifies this into y = Wx + BAx, where W \u2208 R^{d\u00d7k}, B \u2208\nR^{d\u00d7r}, A \u2208 R^{r\u00d7k}, and r \u226a min(d, k). This method greatly reduce amount of\nparameters need to be learned, which is crucial for efficiency in\nresource-limited scenarios. We set LoRA rank r as 16 and train the model for 3\nepochs with an A100 GPU.\n\nNoting that this stage is only conducted for open-source LLMs like ChatGLM and\nBELLE. In essence, this stage lays the foundation for SiliconFriend's role as an\nempathetic AI companion, ensuring it can respond appropriately and helpfully to\nusers' emotional needs.\n\nIntegration with MemoryBank: The second stage in SiliconFriend's development\ninvolves the integration of MemoryBank. This stage is vital as it equips\nSiliconFriend with the ability to store, retrieve past interactions and\nunderstand user portraits, thereby offering a more personalized and engaging\nuser experience.\n\nWhen it comes to memory storage, the dialogues between SiliconFriend and users\nare logged and updated in the memory storage, a process that is adaptable across\nvarious model backbones. The memory updating mechanism operates using principles\ninspired by the Ebbinghaus Forgetting Curve theory, allowing for a realistic and\nhuman-like memory recall process.\n\nDuring real-time conversation, the user's conversation serves as the query for\nmemory retrieval. In practice, we use LangChain (LangChain Inc., 2022) for\nmemory retrieval. LangChain supports open-source embedding models and FAISS\nindexing, making it a versatile choice. In language-specific implementations of\nthe open-source version of SiliconFriend, we use MiniLM (Wang et al., 2020) as\nthe embedding model for English and Text2vec (Ming, 2022) for Chinese. It is\nworth noting that the embedding models can be flexibly interchanged to suit\nvarying needs, even accommodating multilingual models. Upon memory retrieval, a\nseries of information is organized into the conversation prompt, including\nrelevant memory, global user portrait, and global event summary. Consequently,\nSiliconFriend can generate responses that refer past memories and deliver\ninteractions tailored to the user's portrait.\n\nIn conclusion, these stages transform SiliconFriend from a standard AI chatbot\ninto a long-term AI companion, capable of remembering and learning from past\ninteractions to provide personalized and empathetic user experience."
        },
        {
            "section_title": "4 Experiments",
            "text": "The primary objective of our experiments is to evaluate the efficacy of\nMemoryBank within the framework of an LLM, specifically in its ability as an AI\ncompanion. We are particularly interested in determining whether embedding a\nlong-term memory module could augment the AI's proficiency in recalling\nhistorical interactions and deepening its understanding of user personalities.\nAdditionally, we aim to testify whether the tuning based on psychological data\ncan bolster the AI's capability to provide more effective emotional support.\n\nThe qualitative analysis focuses on three aspects: (1) a comparative study\nbetween SiliconFriend and baseline LLMs to evaluate their capabilities in\nproviding empathetic and beneficial psychological companionship; (2) an\ninvestigation into SiliconFriend's memory recall ability; (3) an analysis of how\nthe model's understanding of user profiles influences the responses. Moreover,\nto demonstrate the model's proficiency in memory recall on a broader scale, we\ndesign a qualitative analysis that uses simulated long-term dialog history and\n194 memory probing questions. This simulated dialog history, spanning a period\nof 10 days and encompassing a wide array of topics, is produced by ChatGPT\nthrough the role-play of 15 distinct virtual users, each embodying the users'\npersonality."
        },
        {
            "section_title": "4.1 Qualitative Analysis",
            "text": "The qualitative analysis is conducted by showcasing practical examples of\nSiliconFriend's capabilities. To gather these examples, we have developed an\nonline platform for SiliconFriend and collected real-time conversations from\nactual users.\n\nPsychological Companionship The ability to exhibit empathy in a conversation is\na key attribute of an effective AI companion. To evaluate models' ability to\nprovide psychological comfort to users, we compared the responses shown by\nSiliconFriend with that of the baseline LLMs in real-world conversations. As\ndemonstrated in Fig. 2, when a user expresses emotional difficulties and seeks\nassistance from SiliconFriend, the model is capable of delivering empathetic\nresponses along with constructive suggestions. SiliconFriend's responses stand\nout due to their emotional support, showcasing a stark contrast to its baseline\nChatGLM.\n\nMemory Recall Analysis To evaluate SiliconFriend's ability in memory recall, we\nintegrate memory probing questions into the dialogues. These questions are\ndesigned to prompt SiliconFriend to retrieve specific details from the chat\nhistory. As shown in Fig. 3, the user and SiliconFriend engaged in a discussion\nabout programming learning suggestions. Several days later, the user posed\nseveral memory probing questions. SiliconFriend successfully recalled previously\nsuggested book and algorithm. Furthermore, it correctly identified an event\n(i.e., the heap sort algorithm) that had not been discussed before. These\ninstances underscore SiliconFriend's successful memory recall and recognition\ncapabilities.\n\nPersonality Interaction Analysis As shown in Fig. 4, we examine the capability\nof SiliconFriend with users of diverse personalities. We observe that it\neffectively recommend activities tailored to users' interests based on their\ncharacter traits. This analysis demonstrates SiliconFriend's ability to draw\ninteract effectively with various user personalities."
        },
        {
            "section_title": "4.2 Quantitative Analysis",
            "text": "Quantitative analysis is conducted to exemplify the memory recall ability of\nSiliconFriend in a larger scale. We ask the human annotators to score the\nretrieved memories and responses from the models: (1) SiliconFriend_{ ChatGPT};\n(2) SiliconFriend_{ ChatGLM}; (3) SiliconFriend_{ BELLE}.\n\nMemory Storage Construction: We initially establish an evaluation foundation\nwith a memory storage of 10 days of conversations involving 15 virtual users.\nThese users have diverse personalities and dialogue on each day covers at least\ntwo topics. User meta-information, including names, personalities, and\ninterested topics is generated using ChatGPT. Conversations are synthesized by\nusers acted by ChatGPT based on predefined topics and user personalities. We\ncreate memory storages in both English and Chinese. After memory storage\nconstruction, we manually write 194 probing questions (97 in English and 97 in\nChinese) to assess whether the model could accurately recall pertinent memory\nand appropriately formulate answers. Table 1 presents an example of user\nmeta-information, generated conversations, and probing questions.\n\nEvaluation Metrics The performance of models is assessed based on the following\nmetrics. (1) Memory Retrieval Accuracy: Determines if related memory can be\nsuccessfully retrieved (labels: {0 : no, 1 : yes}). (2) Response Correctness:\nEvaluates if the response contains the correct answer to the probing question\n(labels: {0 : wrong, 0.5 : partial, 1 : correct}). (3) Contextual Coherence:\nAssesses whether the response is naturally and coherently structured, connecting\nthe dialogue context and retrieved memory (labels: 0 : not coherent, 0.5 :\npartially coherent, 1 : coherent). (4) Model Ranking Score: Ranks outputs from\nthe three SiliconFriend variants (SiliconFriend_{ ChatGLM}, SiliconFriend_{\nChatGPT}, and SiliconFriend_{ BELLE}) for the same question and context. Models'\nscores are calculated using s = 1/r, where r = 1, 2, 3 indicates its relative\nranking.\n\nResult Analysis. We evaluate 3 SiliconFriend variants using both English and\nChinese test set. Table 2 yields the following insights: (1) Our overall best\nvariant SiliconFriend_{ ChatGPT} has high performance across all metrics,\nshowing the effectiveness of our overall framework. (2) SiliconFriend _{BELLE}\nand SiliconFriend_{ChatGLM} also have high performance in retrieval accuracy,\nshowing the generality and effectiveness of our MemoryBank mechanism for both\nopen-source and closed-source LLMs. Nonetheless, their performance on other\nmetrics is not as good as SiliconFriend_{ ChatGPT}. This might be attributed to\nthe inferior overall abilities of the base models (BELLE and ChatGLM) compared\nto ChatGPT. (3) Models' performance varies on different languages.\nSiliconFriend_{ ChatGLM} and SiliconFriend_{ ChatGPT} deliver better results in\nEnglish, while SiliconFriend_{ BELLE} excells in Chinese."
        },
        {
            "section_title": "5 Related Works",
            "text": "Large Language Models: LLMs such as GPT-3 (Brown et al., 2020), OPT (Zhang et\nal., 2022), and FLAN-T5 (Chung et al., 2022) have made remarkable strides in a\nbroad spectrum of natural language processing tasks in recent years. Recently,\ncutting-edge closed-source language models, like PaLM (Chowdhery et al., 2022),\nGPT-4 (OpenAI, 2023) and ChatGPT (OpenAI, 2022), continue to display substantial\nflexibility, adapting to a wide variety of domains. They have increasingly\nbecome daily decision-making aids for many people. However, the close-source\nnature of these models prohibit the researchers and companies to study the inner\nmechanism of LLMs and built domain-adapted applications. Therefore, many\nopen-source LLMs emerged in the community, like LLaMa (Touvron et al., 2023),\nChatGLM (Zeng et al., 2022) and Alpaca (Taori et al., 2023). For more details,\nwe refer readers to this comprehensive review: Zhao et al. (2023). Nevertheless,\nthese models still have shortcomings. A noticeable gap lies in their deficiency\nin a robust long-term memory function. This limitation hinders their ability to\nmaintain context over a long period and retrieve pertinent information from past\ninteractions. Our research steps in here, with the primary objective of\ndeveloping long-term memory mechanism for LLMs.\n\nLong-term Memory Mechanisms: Numerous attempts have been made to enhance the\nmemory capabilities of neural models. Memory-augmented networks (MANNs) (Meng &\nHuang, 2018; Graves et al., 2014) like Neural Turing Machines (NTMs)(Graves et\nal., 2014) is an example of this, designed to increase the memory capacity of\nneural networks. These models are structured to interact with an external memory\nmatrix, enabling them to handle tasks that necessitate the maintenance and\nmanipulation of stored information over extended periods. Despite showing\npotential, these methods have not fully addressed the need for a reliable and\nadaptable long-term memory function in LLMs. There have also been studies\nfocusing on long-range conversations (Xu et al., 2021, 2022). For instance, Xu\net al. (2021) introduced a new English dataset comprised of multi-session\nhuman-human crowdworker chats for long-term conversations. However, these\nconversations are generally restricted to a few rounds of conversation, which\ncan not align with the application scenarios of long-term AI companions.\nMoreover, these models often fail to create a detailed user portrait and lack a\nhuman-like memory updating mechanism, both crucial for facilitating more natural\ninteractions. The concept of memory updating has been extensively researched in\npsychology. The Forgetting Curve theory by Ebbinghaus (1964) offers valuable\ninsights into the pattern of memory retention and forgetting over time. Taking\ninspiration from this theory, we integrate a memory updating mechanism into\nMemoryBank to bolster its long-term memory function.\n\nIn summary, while significant progress has been made in the field of LLMs, there\nis still a need for long-term memory mechanism to empower LLMs in the scenarios\nrequiring personalized and persistent interactions. Our work presents MemoryBank\nas a novel approach to address this challenge."
        },
        {
            "section_title": "6 Conclusion",
            "text": "We present MemoryBank, a novel long-term memory mechanism designed to address\nthe memory limitation of LLMs. MemoryBank enhances the ability to maintain\ncontext over time, recall relevant information, and understand user personality.\nBesides, the memory updating mechanism of MemoryBank draws inspiration from the\nEbbinghaus Forgetting Curve theory, a psychological principle that describes the\nnature of memory retention and forgetting over time. This design improves the\nanthropomorphism of AI in long-term interactions scenarios. The versatility of\nMemoryBank is demonstrated through its accommodation of both open-source models\nsuch as ChatGLM and BELLE, and close-source models like ChatGPT.\n\nWe further illustrate the practical application of MemoryBank through the\ndevelopment of Silicon-Friend, an LLM-based chatbot designed to serve as a\nlong-term AI companion. Equipped with MemoryBank, SiliconFriend can establish a\ndeeper understanding of users, offering more personalized and meaningful\ninteractions, emphasizing the potential for MemoryBank to humanize AI\ninteractions. The tuning of SiliconFriend with psychological dialogue data\nenables it to provide empathetic emotional support. Extensive experiments\nincluding both qualitative and quantitative methods validate the effectiveness\nof MemoryBank. The findings demonstrate that MemoryBank empowers SiliconFriend\nwith memory recall capabilities and deepens the understanding of user behaviors.\nBesides, SiliconFriend can provide empathetic companionship of higher quality."
        }
    ],
    "addenda": [],
    "abstract": "Revolutionary advancements in Large Language Models (LLMs) have drastically\nreshaped our interactions with artificial intelligence (AI) systems, showcasing\nimpressive performance across an extensive array of tasks. Despite this, a\nnotable hindrance remains-the deficiency of a long-term memory mechanism within\nthese models. This shortfall becomes increasingly evident in situations\ndemanding sustained interaction, such as personal companion systems,\npsychological counseling, and secretarial assistance. Recognizing the necessity\nfor long-term memory, we propose MemoryBank, a novel memory mechanism tailored\nfor LLMs. Memory-Bank enables the models to summon relevant memories,\ncontinually evolve through continuous memory updates, comprehend, and adapt to a\nuser's personality over time by synthesizing information from previous\ninteractions. To mimic anthropomorphic behaviors and selectively preserve\nmemory, MemoryBank incorporates a memory updating mechanism, inspired by the\nEbbinghaus Forgetting Curve theory. This mechanism permits the AI to forget and\nreinforce memory based on time elapsed and the relative significance of the\nmemory, thereby offering a more human-like memory mechanism and enriched user\nexperience. MemoryBank is versatile in accommodating both closed-source models\nlike ChatGPT and open-source models such as ChatGLM. To validate MemoryBank's\neffectiveness, we exemplify its application through the creation of an LLM-based\nchatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned\nwith psychological dialog data, SiliconFriend displays heightened empathy and\ndiscernment in its interactions. Experiment involves both qualitative analysis\nwith real-world user dialogs and quantitative analysis with simulated dialogs.\nIn the latter, ChatGPT acts as multiple users with diverse characteristics and\ngenerates long-term dialog contexts covering a wide array of topics. The results\nof our analysis reveal that SiliconFriend, equipped with MemoryBank, exhibits a\nstrong capability for long-term companionship as it can provide emphatic\nresponse, recall relevant memories and understand user personality. This\nunderscores the effectiveness of MemoryBank(The materials are released in\nhttps://github.com/zhongwanjun/MemoryBank-SiliconFriend. The corresponding\nauthor is Yanlin Wang.).",
    "references": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing\nsystems, 33:1877-1901, 2020.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\net al. Palm: Scaling language modeling with pathways. arXiv preprint\narXiv:2204.02311, 2022.\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,\nEric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling\ninstruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\n\nH Ebbinghaus. Memory: A contribution to experimental, 1964.\n\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv\npreprint arXiv:1410.5401, 2014.\n\nEdward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\n\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search\nwith GPUs. IEEE Transactions on Big Data, 7(3):535-547, 2019.\n\nVladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain\nquestion answering. arXiv preprint arXiv:2004.04906, 2020.\n\nLangChain Inc. Langchain, 2022. https://docs.langchain.com/docs/.\n\nLian Meng and Minlie Huang. Dialogue intent classification with long short-term\nmemory networks. In Natural Language Processing and Chinese Computing: 6th CCF\nInternational Conference, NLPCC 2017, Dalian, China, November 8-12, 2017,\nProceedings 6, pp. 42-50. Springer, 2018.\n\nXu Ming. text2vec: A tool for text to vector, 2022. URL\nhttps://github.com/shibing624/ text2vec.\n\nOpenAI. Chatgpt, 2022. https://chat.openai.com/chat.\n\nOpenAI. Gpt-4 technical report, 2023.\n\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An\ninstruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca,\n2023.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023.\n\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm:\nDeep self-attention distillation for task-agnostic compression of pre-trained\ntransformers. Advances in Neural Information Processing Systems, 33:5776-5788,\n2020.\n\nJing Xu, Arthur Szlam, and Jason Weston. Beyond goldfish memory: Long-term\nopen-domain conversation. arXiv preprint arXiv:2107.07567, 2021.\n\nXinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and\nShihang Wang. Long time no see! open-domain conversation with long-term persona\nmemory. arXiv preprint arXiv:2203.05797, 2022.\n\nYan Gong Yiping Peng Qiang Niu Baochang Ma Yunjie Ji, Yong Deng and Xiangang Li.\nBelle: Be everyone's large language model engine.\nhttps://github.com/LianjiaTech/BELLE, 2023.\n\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi\nYang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual\npre-trained model. arXiv preprint arXiv:2210.02414, 2022.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui\nChen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open\npre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large\nlanguage models. arXiv preprint arXiv:2303.18223, 2023.",
    "images": {
        "Figure_1_block_34_484fef62": {
            "source_page_num": 2,
            "caption": "Figure 1: Overview of MemoryBank. The memory storage (\u00a7 2.1) stores past\nconversations, summarized events and user portraits, while the memory updating\nmechanism (\u00a7 2.3) updates the memory storage. Memory retrieval (\u00a7 2.2) recall\nrelevant memory. SiliconFriend (\u00a7 3) serves as an LLM-based AI companion\naugmented with MemoryBank.",
            "box": [
                102.64099884033203,
                40,
                510.6490783691406,
                231.3776092529297
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Figure_1_block_34_484fef62.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/f99c6b6414514bd0f3be988670a5816b.png"
        },
        "Figure_2_block_32_9647d88d": {
            "source_page_num": 5,
            "caption": "Figure 2: Example of consulting SiliconFriend_{ ChatGLM} for psychological\ncompanionship. Overall, SiliconFriend can provide more empathic response, offer\nconstructive emotional support to user and help him to face sorrow with positive\naltitude.",
            "box": [
                102.94902038574219,
                40,
                510.2433166503906,
                279.7121887207031
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Figure_2_block_32_9647d88d.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/29f3ee4196112190934c09ed0ecda3d2.png"
        },
        "Figure_3_block_40_d5eaf174": {
            "source_page_num": 6,
            "caption": "Figure 3: Example responses from SiliconFriend_{ BELLE} in memory recall.",
            "box": [
                102.66551971435547,
                40,
                507.4825439453125,
                203.58663940429688
            ],
            "adjacent_neighbor": null,
            "image_file": "images/Figure_3_block_40_d5eaf174.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/be62953c711d8390b4f1c16c7f60c49d.png"
        },
        "Figure_4_block_67_f6a1af07": {
            "source_page_num": 6,
            "caption": "Figure 4: Example responses from SiliconFriend_{ ChatGPT} to users with\ndifferent personalities.",
            "box": [
                109.5980453491211,
                225.0432586669922,
                503.1136474609375,
                380.8126220703125
            ],
            "adjacent_neighbor": 40,
            "image_file": "images/Figure_4_block_67_f6a1af07.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/b493f9c2168956e2dccb26346ef26d45.png"
        },
        "Table_1_block_0_4db4fb53": {
            "source_page_num": 7,
            "caption": "Table 1: An example of one memory piece (on May 3th) in the memory bank, and\ncorresponding probing question asked on May 10th. The outputs from three\nvariances of SiliconFriend are shown for comparison. All these models answer\ncorrectly and coherently.",
            "box": [
                107.64099884033203,
                103.77824401855469,
                505.74609375,
                384.3379211425781
            ],
            "adjacent_neighbor": 10,
            "image_file": "images/Table_1_block_0_4db4fb53.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/93e2e3b37d32cf13054640ee9c345f62.png"
        },
        "Table_2_block_0_5d1b423e": {
            "source_page_num": 8,
            "caption": "Table 2: Results of quantitative analysis.",
            "box": [
                107.53199768066406,
                88.93321228027344,
                505.744140625,
                173.14498901367188
            ],
            "adjacent_neighbor": 5,
            "image_file": "images/Table_2_block_0_5d1b423e.png",
            "image_url": "https://d3f0003gnwhznr.cloudfront.net/80492850b9d421249ceb3dc5a278e738.png"
        }
    }
}