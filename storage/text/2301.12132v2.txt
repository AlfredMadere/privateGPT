# AUTOPEFT: Automatic Configuration Search for
# Parameter-Efficient Fine-Tuning

_ Han Zhou^{1,*}
Xingchen Wan^{2,*}
Ivan Vuli´c^{1}
Anna Korhonen^{1}
^{1}Language Technology Lab, University of Cambridge
^{2}Machine Learning Research Group, University of Oxford
{hz416, iv250, alk23}@cam.ac.uk
xwan@robots.ox.ac.uk _

## Abstract

Large pretrained language models are widely used in downstream NLP tasks via
task-specific fine-tuning, but such procedures can be costly. Recently,
Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task
performance while updating a much smaller number of parameters compared to full
model fine-tuning (FFT). However, it is non-trivial to make informed design
choices on the PEFT configurations, such as their architecture, the number of
tunable parameters, and even the layers in which the PEFT modules are inserted.
Consequently, it is highly likely that the current, manually designed
configurations are suboptimal in terms of their performance-efficiency
trade-off. Inspired by advances in neural architecture search, we propose
AUTOPEFT for automatic PEFT configuration selection: we first design an
expressive configuration search space with multiple representative PEFT modules
as building blocks. Using multi-objective Bayesian optimisation in a low-cost
setup, we then discover a Pareto-optimal set of configurations with strong
performance-cost trade-offs across different numbers of parameters that are also
highly transferable across different tasks. Empirically, on GLUE and SuperGLUE
tasks, we show that AUTOPEFT-discovered configurations significantly outperform
existing PEFT methods and are on par or better than FFT, without incurring
substantial training efficiency costs.

### Figure and Table Captions:

* Figure 1: Performance of AUTOPEFT-discovered configurations (AutoPEFT &
AutoPEFT(per-task); see details in Table 1) compared to other baseline PEFT
methods (markers) and full model FT that updates 100% of parameters (dashed
horizontal bar), averaged across 8 GLUE tasks. Our approach achieves the best
trade-off between task performance and parameter efficiency.

* Figure 2: Illustration of the AUTOPEFT search space which combines both
layer-level (Layers) and within-layer (Serial, Parallel, Prefix) search, and the
connections within a layer (Left). We further show two possible configurations
in the search space (Right): note that some PEFT layers can be inactive
altogether and the searchable module sizes (shaded in green), i.e. the
bottleneck sizes in Serial and Parallel (D_{SA} and D_{PA} respectively) and
sizes of P_{K}, P_{V} in Prefix (L_{PT}), are dynamic.

* Figure 3: Illustration of the Pareto-optimal search with multi-objective
Bayesian optimisation (BO; §2.2): The BO agent trains on the vector
representations of the evaluated configurations as inputs and their performance
under a low-fidelity setup (e.g. accuracy - obtained by fine-tuning the language
model with the PEFT configuration for a small number of iterations) and cost
(e.g. number of parameters) as targets. The BO agent then iteratively suggests
new configurations until convergence.

* Table 1: Results on the GLUE benchmark with BERT_{base} (tasks are ranked in an
ascending order of training resources required from left to right). For^{
AUTO}PEFT^{RTE}, we search on RTE with a low-fidelity proxy, training for 1
epoch per iteration, only at a search cost of 1.9% (in terms of additional
fine-tuning steps required) over the full GLUE experiment. We report the average
fine-tuned parameters of per-task AUTOPEFT, where we conduct additional per-task
searches on RTE, MRPC, STS-B, and CoLA, and take best-found configurations for
the remaining tasks. We report Spearman's Correlation for STS-B, Matthew's
Correlation for CoLA, and accuracy for all other tasks (matched accuracy for
MNLI). The percentage of parameters is the ratio of the number of additional
parameters to the pretrained parameters. We reproduce all baselines and report
the mean and standard deviation of all results for 5 random seeds. The best,
second-best, and third-best results are marked in bold fonts and ranked by
colour.

* Table 2: Specification of the discovered AUTOPEFT configuration reported in
Table 1 (^{AUTO}PEFT^{RTE}) using BERT_{base}.

* Table 3: Results on SuperGLUE tasks with AUTOPEFT-discovered configurations
searched on RTE with BERT_{base} as the underlying PLM. We split 10% of the
training set as the new validation set, and report the ^{AUTO}PEFT^{RTE}-found
configuration transfer results on the evaluation set over five random seeds.

* Figure 4: The Pareto fronts of AUTOPEFT on RTE, MRPC, STS-B, and CoLA compared
to baselines on BERT_{base}, over varying parameter budgets. We report the
single-seed task score but otherwise follow the settings in Table 1.

* Table 4: Experimental results on GLUE with RoBERTa_{large}. We report the full
model fine-tuning^{†} results from Liu et al. (2019b) with Pearson correlation
for STS-B. We include the LoRA^{‡} module performance from Hu et al. (2022a). We
exclude QQP and MNLI tasks due to the large computation cost of RoBERTa_{large}.
Consistent with Table 1, we again report AUTOPEFT results searched on RTE in
full-resource settings that are then transferred all included GLUE tasks
(AUTOPEFT^{RTE}) and per-task AUTOPEFT (AUTOPEFT^{task} _{Avg.}) but on
RoBERTa_{large}.

* Figure 5: Pairwise transferability study of AUTOPEFT-discovered configurations:
each row (Ours^{[task]}) denotes the performances of the AUTOPEFT configuration
searched from [task] (e.g. RTE) to the task itself and 3 other GLUE tasks. The
results show that AUTOPEFT performance is largely robust to the choice of which
task to search on.

* Figure 6: The distribution of the discovered configurations via BO (orange,
described in §2.2 and random search (grey) using the same total number of
evaluations (200). Both searches use the same, 100 random initialising points
(blue) on RTE. Note that for configurations with similar accuracy, the
BO-generated configurations typically have much better parameter efficiency.

* Figure 7: The performance of AUTOPEFT with ablation of search space on RTE on
BERT_{base}. The SA results refer to the Pfeiffer adapter (Pfeiffer et al.,
2020b) with an enumeration of its bottleneck size. For other search spaces, we
report the Pareto front of AUTOPEFT-found configurations, where SA-PA-PT-Layer
forms the search space of AUTOPEFT.

* Table 5: Comparing AUTOPEFT to layer selection baselines with the same parameter
budget on BERT_{large}. We report the Pfeiffer adapter for all 24 layers
(Serial), specialised AdapterDrop (Rücklé et al., 2021) that inserts SA for the
last 13 layers, and AA^{uni} (Moosavi et al., 2022) without its rational
activation function with 13 selected layers (Adaptable Adapter). We run our
AUTOPEFT under the comparable search space of 24 layers and approximately match
the size of Serial.

* Table 6: The search space of the AUTOPEFT. Each insertion layer has a Boolean
decision for inserting the PEFT modules. The 0 size of submodules indicates that
we exclude the corresponding submodule from the configuration. The total number
of configurations for BERT_{base}: 2^{12} × 11 × 11 × 11 ≈ 5 × 10^{6} and for
BERT/RoBERTa_{large}: 2^{24} × 12 × 12 × 12 ≈ 3 × 10^{10}.

* Table 7: The AUTOPEFT-found configurations reported in Table 1 using
BERT_{base}. The average of fine-tuned parameters (%) of AUTOPEFT^{task} _{Avg.}
is calculated by (1.42 + 3.86 + 1.06 + 0.29 + 1.42 + 0.30 + 1.42 + 1.42)/8 =
1.40, where we transfer the best-found configurations to SST-2, QNLI, QQP, and
MNLI as their best per-task configurations for achieving the best trade-off
between task performance and efficiency.

* Table 8: The AUTOPEFT-found configurations reported in Table 5 using
BERT_{large}.

* Table 9: The AUTOPEFT-found configurations reported in Table 4 using
RoBERTa_{large}. The average of fine-tuned parameters (%) of AUTOPEFT^{task}
_{Avg.} is calculated by (0.03 + 0.25 + 0.25 + 2.36 + 2.36 + 0.03)/6 = 0.88,
where we transfer the best-found AUTOPEFT^{CoLA} to SST-2 and AUTOPEFT^{RTE} to
QNLI as their best per-task configurations for achieving the best trade-off
between performance and efficiency.

## 1 Introduction and Motivation

Pretrained language models (PLM) are used in downstream tasks via the standard
transfer learning paradigm, where they get fine-tuned for particular tasks
(Devlin et al., 2019; Liu et al., 2019b).

This achieves state-of-the-art results in a wide spectrum of NLP tasks, becoming
a prevalent modelling paradigm in NLP (Raffel et al., 2020). Fine-tuning the
PLMs typically requires a full update of their original parameters (i.e. the
so-called full-model fine-tuning (FFT)); however, this is 1) computationally
expensive and also 2) storage-wise expensive as it requires saving a separate
full model copy for each task-tuned model. With the ever-growing size of the
PLMs (Brown et al., 2020; Sanh et al., 2022), the cost of full model FT becomes
a major bottleneck, due to its increasing demands as well as computational (time
and space) non-efficiency. Parameter-Efficient Fine-Tuning (PEFT) delivers a
solution for alleviating the issues with full-model FT (Houlsby et al., 2019).
By freezing the majority of pretrained weights of PLMs, PEFT approaches only
update a small portion of parameters for efficiently adapting the PLM to a new
downstream task. Recent studies have shown that PEFT can achieve competitive
task performance while being modular, adaptable, and preventing catastrophic
forgetting in comparison to traditional FFT (Wang et al., 2022; Pfeiffer et al.,
2023).

Recent developments have created diverse PEFT modules with distinctive
characteristics (Pfeiffer et al., 2020b; Li and Liang, 2021), with one of the
two main aims in focus: 1) to improve task performance over other PEFT
approaches while maintaining the same parameter budget as the competitor PEFT
methods; or 2) to maintain task performance while reducing the parameter budget
needed. Existing PEFT modules, optimising for one of the two aims, have been
successfully applied to transfer learning tasks (Chen et al., 2022b; Pfeiffer et
al., 2022). However, different tasks, with different complexity, show distinct
sensitivity to the allocated parameter budget and even to the chosen PEFT
approach (He et al., 2022). At the same time, most PEFT applications are limited
to a single PEFT architecture (e.g. serial adapters, prefix-tuning) with fixed
decisions on its components (e.g. hidden size dimensionality, insertion layers)
resulting in potentially suboptimal PEFT configurations across many tasks.
Therefore, in this work, we propose a new, versatile and unified framework that
automatically searches for improved and task-adapted PEFT configurations, aiming
to effectively balance between the two (often colliding goals) of improving
performance and keeping the desired low parameter budget for PEFT.

While recent research has started exploring more dynamic PEFT configurations,
the prior studies remain limited across several dimensions, including how they
define the configuration search space. Namely, they typically focus only on a
single PEFT architecture (e.g. adapters) or their simple combinations, or a
single property (e.g. insertion layers - where to insert the module); the
readers are referred to a short overview later in §3. Here, we propose a unified
and more comprehensive framework for improved configuration search. It covers
multiple standard PEFT modules (serial adapters, parallel adapters, and
prefix-tuning) as building blocks, combined with the critical parameter
budget-related decisions: the size of each constituent module and the insertion
layers for the modules.

Our defined comprehensive search space is huge; as a consequence, traversing it
effectively and efficiently is extremely challenging. To enable search over the
large configuration space, we thus propose the novel AUTOPEFT framework. It
automatically configures multiple PEFT modules along with their
efficiency-oriented design decisions, relying on a high-dimensional Bayesian
optimisation (BO) approach. Crucially, within the search space, we propose a
multi-objective optimisation which learns to simultaneously balance between
maximising the searched configurations' task performance and parameter
efficiency. We conduct extensive experiments on the standard GLUE and SuperGLUE
benchmarks (Wang et al., 2018, 2019). We first study the transferability of the
AUTOPEFT-searched architecture by running AUTOPEFT on a single task with a
low-fidelity proxy (aiming to reduce computational cost), followed by
transferring the found architecture to other tasks. Experimental results show
that this architecture can outperform existing PEFT baselines while achieving
on-par performance with the standard FFT. Further slight gains can be achieved
with a larger computation budget for training, where we run AUTOPEFT per each
single task to find a task-adapted PEFT configuration. As demonstrated in Figure
1, AUTOPEFT is able to find configurations that offer a solid trade-off between
task performance and parameter efficiency, even outperforming FFT. We also
provide ablation studies over the search space, validating that the AUTOPEFT
framework is versatile and portable to different search spaces.

Contributions. 1) We propose the AUTOPEFT search space containing diverse and
expressive combinations of PEFT configurations from three representative PEFT
modules as foundational building blocks and the binary decisions concerning
Transformer layers for inserting these modules as searchable dimensions. 2) To
navigate the vast AUTOPEFT search space and to discover a set of transferable
PEFT configurations that optimally trade performance against cost across various
parameter ranges in a single run, we further propose an effective search method
based on multi-dimensional Bayesian optimisation. 3) We demonstrate that the
one-time search cost of AUTOPEFT is cheap, and AUTOPEFT yields task-shareable
configurations, outperforming existing PEFT modules while being transferable
across tasks. The AUTOPEFT framework can also be easily extended to other and
new PEFT modules.

## 2 ^{AUTO}PEFT Framework

## 2.1 Designing the AUTOPEFT Search Space

Inspired by the success of neural architecture search (NAS), we similarly start
by designing a large and expressive configuration space. We additionally provide
the motivation behind each decision to include a particular module and its
components in the configuration space, along with a mathematical formulation.

The search space is known to be one of the most important factors in the
performance of the configurations to be discovered subsequently (Ru et al.,
2020; Xie et al., 2019; Li and Talwalkar, 2019; Dong and Yang, 2020; Yang et
al., 2020). In order to simultaneously maximise task performance along with
parameter efficiency, it is necessary to first define a 'parameter-reducible'
search space, where each dimension within the space potentially contributes to
reducing the parameter budget. Similarly, each dimension might potentially bring
a positive impact on the task performance without introducing redundancy in the
space (Wan et al., 2022). Therefore, as shown in Figure 2, we propose the search
space with representative PEFT modules, as follows, spanning a plethora of
(non-redundant) configurations.

PEFT Modules. Inspired by common practices in NAS of using known well-performing
modules as building blocks, we include three distinctive PEFT designs to
efficiently adapt different forwarding stages of hidden states in the PLM
layers. We combine Serial Adapters (SA), Parallel Adapters (PA), and
Prefix-Tuning (PT) as the three representative modules in the search space as
the building blocks, where the PT module adapts the multi-head attention layer,
and SA and PA interact with the FFN layer (Figure 2). Each configuration makes
two decisions on each of the PEFT modules in the insertion layer: the binary
decision on whether it is 'switched' on or off, and, when it is switched on, its
actual module size (see the next paragraph). As we empirically validate later,
the resultant search space spanned by the building blocks is extremely
expressive and flexible, and enables the discovery of configurations that
outscore any of the individual building blocks.

Size. Previous studies show that PEFT methods are highly sensitive to the number
of tunable parameters: adaptively setting their capacity in accordance with the
target task is therefore essential for achieving good performance (Chen et al.,
2022a). The number of tunable parameters depends on each particular module. The
additional parameters introduced by both SA and PA are dominated by their
bottleneck dimension D. Similarly, the size of the PT module is defined by its
prefix length L_{PT}. Thus, we introduce a searchable dimension for each of
D_{SA}, D_{PA}, and L_{PT} whose possible values span from 0, which indicates
the module is 'switched off' or disabled, to D_{h}, where D_{h} is the
dimensionality of the output embedding of the PLM (e.g. D_{h}=768 for
BERT_{base}).

Insertion Layers. Prior work has also shown that different layers in the PLMs
store different semantic information (Vuli´c et al., 2020), where the higher
layers produce more task-specific and contextualised representations (Tenney et
al., 2019). Therefore, adapting all layers may potentially result in overfitting
to the target task while being suboptimal. We then introduce another set of
searchable dimensions which control the 'insertion' decision at each layer l_{i}
- the aim is to search for configurations that are high-performing yet
parsimonious in inserting PEFT modules.

Combining PEFT Modules. The SA module and the PA module share a bottleneck
architecture. The SA receives hidden states from the FFN output as its inputs,
adapts it with a down-projection matrix W^{down} SA ∈ R^{Dh×DSA}, which is
followed by a non-linear activation function, and an up-projection matrix W^{up}
_{SA} ∈ R^{DSA×Dh}:

f_{SA}(h) = ReLU(hW^{down} SA )W^{up} _{SA}. (1)

PA, on the other hand, receives its inputs from hidden states before the FFN
layer with the same formulation:

f_{PA}(x) = ReLU(xW^{down} PA )W^{up} _{PA}. (2)

Therefore, it is able to act in parallel with the SA without interference. Note
that the FFN hidden states h = F(x) contain the task-specific bias learned in
its pretrained weights. Therefore, by combining SA with PA, the following
composition of functions is achieved:

f_{SAPA}(x) =ReLU(F(x)W^{down} SA )W^{up} SA +ReLU(xW^{down} PA )W^{up} _{PA}.
(3)

The final composition should provide an effective adaptation to both
bias-influence hidden states and the original inputs before the pretrained FFN
layer.(The PA module also acts as the low-rank reparametrization of the learned
SA together with the frozen FFN layer to further match the intrinsic
dimensionality of the target task.)

Further, applying PEFT modules to interact both with FFNs and multi-head
attention should have a positive impact on task performance (Mao et al., 2022;
He et al., 2022). PT learns two prefix vectors, P_{k} and P_{v} ∈ R^{LPT×Dh},
that are concatenated with the original multi-head attention's key and value
vectors, which efficiently adapts the multi-head attention layer to fit the
target task. We thus finally combine the SA and the PA (i.e. SAPA from above)
with PT. In sum, the overview of the dimensions spanning the final configuration
space is provided in Figure 2. The combination of the different 'configuration
dimensions' outlined above gives rise to a total of e.g. 5,451,776 possible
configurations with BERT_{base} and ∼ 3×10^{10} configurations with
RoBERTa_{large} (i.e. the number of configurations is
2^{|l|}×|D_{SA}|×|D_{PA}|×|L_{PT}|). While a large search space is crucial for
expressiveness and to ensure that good-performing configurations are contained,
it also increases the difficulty for search strategies to navigate the search
space well while remaining sample- and thus computationally efficient.
Furthermore, in the PEFT setting, we are also often interested in discovering a
family of configurations that trade off between performance and efficiency for
general application in various scenarios with different resource constraints,
thus giving rise to a multi-objective optimisation problem where we
simultaneously aim to maximise performance while minimising costs. In what
follows, we propose a search framework that satisfies all those criteria.

## 2.2 Pareto-Optimal Configuration Search

The ultimate goal of AUTOPEFT is to discover promising PEFT configuration(s)
from the expressive search space designed in §2.1, which is itself challenging.
In this paper, we focus on an even more challenging but practical goal: instead
of aiming to find a single, best-performing PEFT configuration, we aim to
discover a family of Pareto-optimal PEFT configurations that trade performance
against parameter-efficiency (or parameter cost) optimally: one of the most
impactful use cases of PEFT is its ability to allow fine-tuning of massive
language models even with modest computational resources, and thus we argue that
searching Pareto-optimal configurations is key as it allows tailored user- and
scenario-specific PEFT deployment depending on the computational budget.

To this end, we adopt a Bayesian optimisation (BO) approach, illustrated in
Figure 3. On a high level, BO consists of a surrogate model that sequentially
approximates the objective function based on the observations so far, and an
acquisition function, for exploitation-exploration trade-off that is optimised
at each iteration to actively select the next configuration to evaluate. For a
detailed overview of BO, we refer the readers to Garnett (2023) and Frazier
(2018). We argue that BO is particularly desirable, as 1) BO is sample-efficient
and zeroth-order. It treats the model as a black box and requires no
differentiable objectives, allowing cost-efficient optimisation without assuming
structures of the objectives nor the model itself, and BO also has proven
success in NAS and automated machine learning in general (Snoek et al., 2012;
White et al., 2021a; Ru et al., 2021; Kandasamy et al., 2018); 2) BO may
generalise to the multi-objective setup in a search space-agnostic manner while
the competing methods, such as supernet-based NAS methods, are typically used to
discover a single best-performing configuration (Eriksson et al., 2021;
Izquierdo et al., 2021); and 3) BO is more parallelisable and during search, its
largest memory use, which is particularly important for PEFT given its main
promise on parameter efficiency, is upper-bounded by the largest PEFT
configuration in the search space. Competing methods such as those relying on
over-parameterised supernets typically involve a much larger memory burden.

Adapting BO to the high-dimensional and combinatorial AUTOPEFT search space is
non-trivial. To address the challenges, we adopt the SAAS-GP (Eriksson and
Jankowiak, 2021) model as the surrogate function: SAAS-GP places strong,
sparsity-inducing priors to alleviate the difficulty in modelling
high-dimensional data by assuming that despite the high nominal dimensionality,
the effective dimensionality is much lower - this assumption is shown to be the
case in NAS (Wan et al., 2022), and we expect similar findings in our particular
case. For the acquisition function, we use the noisy expected hypervolume
improvement (NEHVI) (Daulton et al., 2021) to handle the multi-objective
setting. Lastly, we additionally use low-fidelity approximations, a popular
low-cost performance estimation strategy in NAS (Elsken et al., 2019), to manage
the search cost: at search-time, instead of fine-tuning each candidate PEFT
configuration in full, we only fine-tune with a much smaller number of
iterations (5% of full) - this is possible as we are only interested in the
relative ranking (rather than the performance itself) of the different
configurations during the search. Consistent with NAS literature, we also find
the low-fidelity estimate to provide a reliable ranking, with the
best-performing configurations in low fidelity also performing the best under
fine-tuning with the full iterations. As we will show in §5, using the
low-fidelity search pipeline, in combination with the strong transferability of
the discovered configurations, AUTOPEFT only incurs an additional one-off, 1.9%
of the total GLUE fine-tuning cost, but delivers significant performance gains.

## 3 Related Work

PEFT Methods in NLP. Standard PEFT methods can be divided into two main groups
(Pfeiffer et al., 2023). 1) Some methods fine-tune a small portion of pretrained
parameters (Zhao et al., 2020; Guo et al., 2021). For instance, Ben Zaken et al.
(2022) propose to fine-tune the PLM's bias terms, while Sung et al. (2021) and
Ansell et al. (2022) fine-tune sparse subnetworks withing the original PLM for a
particular task. 2) Other methods fine-tune an additional set of parameters (Liu
et al., 2022). Since there is no interference with the pretrained parameters,
this class of PEFT modules, besides offering strong task performance, is
arguably more modular; we thus focus on this class of PEFT methods in this work.
The original adapter modules (Houlsby et al., 2019; Pfeiffer et al., 2020b) have
a bottleneck serial architecture which can be inserted into every Transformer
layer, see Figure 2. LoRA (Hu et al., 2022a) assumes the low-rank intrinsic
dimensionality of the target task and performs low-rank updates (Mahabadi et
al., 2021). Li and Liang (2021) propose the Prefix-Tuning method that appends a
learnable vector to the attention heads at each Transformer layer. Similarly,
prompt-tuning (Lester et al., 2021) only appends this vector to the input
embedding. UniPELT (Mao et al., 2022) integrates multiple PEFT modules with a
dynamic gating mechanism. He et al. (2022) provide a unified formulation of
existing PEFT modules and propose a parallel adapter module, along with a
combined 'Mix-and-Match Adapter (MAM)' architecture that blends parallel
adapters and prefix-tuning. Wang et al. (2022) propose the
mixture-of-adaptations (AdaMix) combined architecture that leverages weight
averaging for a mixture of adapters.

Optimising Parameter Efficiency in PEFT. Recent work further aims to optimise
the parameter efficiency of existing PEFT modules while maintaining task
performance. The standard approach is to insert (typically serial) adapters into
all Transformer layers, which still requires a sizeable parameter budget. Rücklé
et al. (2021) address this question by performing random dropout of adapters
from lower-level layers, displaying only a small decrease in task performance.
Adaptable Adapters (AA) (Moosavi et al., 2022) generalise this idea by learning
gates that switch on or off adapters in particular Transformer layers. Neural
Architecture Search (NAS) methods aim to automate the design of neural net
architectures themselves, and NAS has seen great advances recently, with
performance often surpassing human expert-designed architectures in various
tasks (Zoph and Le, 2017; Ren et al., 2021; Elsken et al., 2019). Concerning NLP
tasks and PEFT, Hu et al. (2022b) propose S_{3}PET, which adapts Differentiable
Architecture Search (DARTS) (Liu et al., 2019a) to learn the positions for
inserting the PEFT modules. Concurrent works (Valipour et al., 2022; Zhang et
al., 2023) also approach the same problem by dynamic budget allocation
mechanisms on a single PEFT module within a limited search space. This field
still lacks a compact solution for automatically configuring a complex space of
PEFT modules (Chen et al., 2023).

Our method, discussed in detail in §2, offers a spectrum of advantages over
related PEFT works. Relying on multi-objective optimisation, unlike DARTS, we
can automatically discover a family of configurations at different parameter
efficiency levels in a single search run, effectively balancing between task
performance and parameter efficiency, without the need to set the 'parameter
budget' in advance; similarly, we enable an automatic search over multiple
constituent modules over the desirable range of parameter budget and effective
layers, whereas previous work can only support one architecture per each search
run. Further, previous work indicated that weight-sharing NAS such as DARTS may
suffer with the reliability of prediction (White et al., 2021b), large memory
usage due to supernet construction (which could be particularly problematic for
PEFT given the emphasis on memory efficiency), and, as discussed in §2, its
success often hinges heavily on the design of the actual search space. While
weight-sharing NAS is often perceived to be more computationally efficient, as
we discussed in §2.2 and will show empirically in §5, AUTOPEFT can be similar,
if not more, efficient in discovering effective PEFT configurations even in
terms of search costs while arguably more parameter-efficient.

## 4 Experimental Setup

Evaluation Data. We follow prior PEFT research and base our evaluation on the
standard and established GLUE and SuperGLUE benchmarks. For GLUE, we include 4
types of text classification tasks, including linguistic acceptability: CoLA;
similarity and paraphrase: STS-B, MRPC, QQP; sentiment analysis: SST-2; natural
language inference: RTE, QNLI, MNLI. We exclude WNLI following previous work
(Houlsby et al., 2019; Mao et al., 2022). We also include CB, COPA, WiC, and
BoolQ from SuperGLUE to further validate the transferability of AUTOPEFT-found
configuration across different tasks and datasets.

Baselines. We compare the performance of the AUTOPEFT-found configurations to
the standard full model FT and each individual PEFT module (SA, PA, PT) from the
search space used in their default setup from respective original work. We also
compare with the LoRA module, to provide a comparison to low-rank decomposition
methods. In order to provide comparisons with recently proposed methods that
also integrate multiple PEFT modules (see §3), we further include the UniPELT
and the MAM adapter in their default settings. We reproduce AdaMix for a
comparison to a mixture of homogeneous adaptations. In ablations on insertion
layers, we also include the Adaptable Adapter (AA) as a baseline that proposes a
differentiable gate learning method to select the insertion layer for PEFT
modules (i.e. serial adapters originally).

Implementation Details. Following previous work on the GLUE benchmark, we report
the best GLUE dev set performance (Ben Zaken et al., 2022) and use 20 training
epochs with an early stopping scheme of 10 epochs for all per-task experiments.
We use AdapterHub (Pfeiffer et al., 2020a) as the codebase and conduct extensive
experiments with the uncased BERT_{base} (Devlin et al., 2019) as the main
backbone model. We report main experiments with the mean and standard deviation
over 5 different random seeds. Following Pfeiffer et al. (2020b), we use a
recommended learning rate of 10^{−4} for all PEFT experiments. We use the
learning rate of 2 × 10^{−5} for full model FT according to Mao et al. (2022).
We use the batch size of 32 and 16 for all BERT and RoBERTa experiments,
respectively. The optimiser settings for each PEFT module follow the default
settings in AdapterHub (Pfeiffer et al., 2020a). We implement the BO search
algorithm in BoTorch (Balandat et al., 2020) and use the recommended settings
from Eriksson and Jankowiak (2021) for the surrogate. For acquisition function
optimisation, we use a local search method similar to previous literature with a
similar setup (Wan et al., 2021; Eriksson et al., 2021): at each search
iteration (after the initial randomly sampled points), we collect the
Pareto-optimal architectures up to this point. From this collection of
Pareto-optimal architectures, we perform a local search by evaluating the
acquisition function values of their neighbours, and move the current point to a
neighbour with a higher acquisition function value and this process is repeated
until convergence. Due to the relatively noisy nature of the problem, we use 100
random initialisation points for all experiments followed by 100 BO iterations.
We further show results using RoBERTa_{large} (Liu et al., 2019b) in Table 4,
which shows findings that are consistent with the BERT_{base}. In experiments
with RoBERTa_{large} as the underlying PLM, we report the RTE results with a
learning rate of 2 × 10^{−5} for_{ AUTO}PEFT^{MRPC} and_{ AUTO}PEFT^{CoLA};
10^{−4} for AUTOPEFT^{RTE}.

## 5 Results and Discussion

Discussion of Main Results. The main results are summarised in Table 1 where we
evaluate the AUTOPEFT-found configurations searched from RTE, the most
low-resource and challenging task, on the full GLUE suite. For simplicity, we
report a single configuration that leads to the highest task performance in a
predefined, user-specified parameter budget from the discovered Pareto-optimal
set in Table 1, whereas the full Pareto-optimal set is evaluated in Figure 4.
First, using only 0.76% of parameters, AUTOPEFT^{RTE} outperforms all the PEFT
baselines (more than 2% on RTE). The AUTOPEFT-found configuration also
outperforms the full-model FT baseline on the RTE task by more than 1%. These
results indicate the effectiveness of the AUTOPEFT framework in optimising both
task performance and parameter efficiency. Transferring the RTE-based
configurations to other tasks, we find that strong performance is maintained
across the target tasks, with more benefits on the medium-resource tasks (MRPC,
STS-B, CoLA), but the configuration remains competitive also for higher-resource
tasks (e.g. QQP, MNLI).

Table 2 specifies the composition of the found configuration, indicating the
exact task-active layers while allocating more parameter budget to the efficient
and effective PA module. On average, the_{ AUTO}PEFT^{RTE} configuration shows a
comparable fine-tuning performance (83.17), to FFT (83.15), by only updating
0.76% of parameters. With strong transferability across similar tasks, AUTOPEFT
provides distinct advantages in parameter efficiency; the search algorithm
itself coupled with transfer becomes more sample-efficient within limited
training resources.

Scalability to More Tasks and Efficiency. We next 'stress-test' the ability of
AUTOPEFT-found configuration in a more challenging scenario, carrying out
experiments on a completely new set of dissimilar tasks. Table 3 reports the
results of transferring AUTOPEFT^{RTE} from Table 1 to four SuperGLUE tasks. In
terms of parameter efficiency, we observe consistent patterns as in Table 1
before, where our plug-and-play PEFT configuration outperforms existing PEFT
baselines by a substantial margin (2%) on average while being comparable to the
costly full-model FT.(With the^{ AUTO}PEFT-found off-the-shelf configuration,)
In terms of search cost, we recall that through the use of low-fidelity proxy
and the strong transferability,_{ AUTO}PEFT^{RTE} in Table 1 only requires an
additional, one-off 1.9% in terms of training time (or equivalently the number
of fine-tuning steps) of that of single-seed training of the GLUE training sets.
Furthermore, Figure 5 demonstrates the robustness of our framework to the choice
of the source task to search on. Therefore, our framework is task-agnostic with
a cheap one-time cost but yields 'permanent' improvement towards all efficiency
metrics for PEFT: space, time, and memory.

Per-Task Configuration Search. We further conduct full-resource per-task
AUTOPEFT searches. While naturally more expensive, we argue this setup is useful
if, for example, one is interested in finding absolutely the best configurations
for that particular task and where search cost is less of a concern. Due to
computational constraints, we search per-task on RTE, MPRC, STS-B and CoLA then
port the small set of best configurations to the remaining higher-resource tasks
(SST-2, QNLI, QQP, MNLI). We observe consistent gains in all tasks we search on
over the best-performing PEFT baselines, e.g. MRPC (87.16% (best baseline) to
87.45% (ours)) and CoLA (60.13% to 60.92%), and also the transferred
configuration _{AUTO}PEFT^{RTE} in Table 1. One interpretation is that while
configurations are highly transferable, the optimal configurations may
nonetheless differ slightly across tasks such that while transferred AUTOPEFT
configurations (e.g. the one reported in Table 1) perform well, searching
per-task per-this requires no additional search cost and enables a more
efficient and effective tuning approach for new tasks.

forms the best. Crucially, we also find per-task AUTOPEFT in this setup to even
outperform FFT, despite only using 1.4% of all parameters, except for the
high-resources task where we mostly perform on par; this is consistent with our
observations that similar to the baselines, due to the richness of training
resources, the performance may be mostly saturated and PEFT methods often
achieve on-par performance to FFT at most.

Analysing the 'Behaviour' of Bayesian Optimisation. Figure 6 shows the
distribution of AUTOPEFT-found configurations when we conduct its search
experiment on RTE. Recalling that the search strategy (§2.2) starts with the
random initialisation, we compare the behaviours of the random explorations and
the BO-suggested configurations: whereas the random search baseline is purely
exploratory and discovers less parameter-efficient configurations, BO succeeds
in discovering configurations towards the regions with improved parameter
efficiency. BO eventually discovers a rich family of PEFT configurations across
a wide range of parameters, whereas previous approaches typically fail to
explore the entire Pareto front. This is a critical strength motivating our BO
search strategy.

Ablation of the Configuration Space. To provide a finer-grained analysis of
factors that bring positive impact to AUTOPEFT, we ablate the AUTOPEFT search
space from the full configuration space: 1) to the basic enumeration of the
bottleneck size D_{SA} of the SA only (the SA space). We then include the
Transformer layer and the SA size together into the search space (the SA-Layer
space) to validate the usefulness of using layer selection as one configuration
dimension. We can then also expand the search space by adding another module
(e.g. PA yields the SA-PA-Layer space). Figure 7 plots the performance over the
ablated configuration spaces and over different parameter budgets. Several key
findings emerge. First, combining multiple single PEFT modules has a positive
impact on AUTOPEFT in general (c.f. full AUTOPEFT vs. SA-PA-Layer vs. SA-Layer).
Relying on layer selection also brings benefits (c.f. SA vs. SA-Layer). The
comparison also indicates that leaving out some Transformer layers while
increasing the capacity of the PEFT module is a straightforward method to
improve the parameter efficiency and task performance of the PEFT module within
a fixed parameter budget. The ablation results also demonstrate that AUTOPEFT is
search space-agnostic, capable of effectively operating over configuration
spaces of different granularity.

Layer Selection. The ability to disable some PEFT layers altogether is a key
novelty of the AUTOPEFT search space, and to further compare different layer
selection approaches, we conduct a controlled experiment with the SA module on
BERT_{large} (24 Transformer layers) under a predefined parameter budget. In
Table 5, we compare against AdapterDrop, which simply drops the adapters for the
first 11 layers while doubling their bottleneck sizes, and, within the same
architecture, we also include the Adaptable Adapter with selected layers from
switch learning (3 and 10 layers from the first 12 and the other 12 layers,
respectively). We show that AUTOPEFT outperforms existing layer selection
baselines activating fewer PEFT layers, leading to better parameter efficiency
(12.5% fewer parameters in relative terms) yet achieving better performance. It
indicates that selecting the best insertion layer is non-trivial, and AUTOPEFT
can efficiently learn the correlation between layers.

## 6 Conclusion

We proposed AUTOPEFT, a novel search framework for automatically configuring
parameter-efficient fine-tuning (PEFT) modules of pretrained language models.
AUTOPEFT features both a large and expressive, newly designed configuration
search space and an effective search method featuring Bayesian optimisation that
discovers a Pareto-optimal set of novel PEFT configurations with promising
performance-efficiency trade-offs. Empirically, we demonstrated that
AUTOPEFT-discovered configurations transfer strongly across different GLUE and
SuperGLUE tasks, outperforming a variety of strong PEFT baselines and being
competitive to full model fine-tuning.

## Limitations and Future Work

AUTOPEFT search inevitably incurs a search cost since it requires iterative
optimisation at search time. However, we mitigate this by 1) using a
low-fidelity proxy of 1-epoch training, and 2) leveraging strong transferability
by generalising from low-resource and thus quick-to-train tasks. While the
search itself can be seen as a one-time cost yielding a permanent
well-performing and shareable configuration for particular tasks, we plan to
delve deeper into further optimising the search cost in future work.
Furthermore, while we conduct extensive experiments on the search space that
contains three existing PEFT modules as building blocks, novel PEFT modules may
emerge. However, AUTOPEFT framework is general where we may easily integrate
these forthcoming new modules. We defer thorough investigations to future work.

## A Supplemental Material: Technical Details

PEFT Modules: Architectures and Setup. We implement the serial adapter
architecture (SA) following the setup of Pfeiffer et al. (2020b). The parallel
adapter (PA) architecture is the same as the one proposed by He et al. (2022),
where a scaling factor of 4 is implemented in all PA experiments. The
prefix-tuning (PT) architecture has an intermediate MLP with a bottleneck size
of 800, which is trained the same way as in the original wor (Li and Liang,
2021). We also use the default setting for LoRA with a scaling of 8 and a rank
of 8. We reproduce the experimental results with the reported setup of the MAM
adapter He et al. (2022) and UniPELT (Mao et al., 2022). We reproduce the AdaMix
results with the reported hyperparameter setup from the original work (Wang et
al., 2022) in 20 epochs. In the experiments of Figure 4, we control the
bottleneck size D_{SA} and D_{PA} for SA and PA baselines, respectively, while
keeping other setups unchanged to discover their performance across the
parameter budget. Similarly, we control the prefix length L_{PT} for
prefix-tuning and the rank r of LoRA without changing other setups.

AUTOPEFT Search Setup. We implement the BO algorithm in BoTorch (Balandat et
al., 2020). We use the Matern 5/2 kernel as the covariance function, and for the
Monte Carlo sampling settings of SAAS-BO (Eriksson and Jankowiak, 2021), we use
a warm-up step of 256, the number of samples to retain as 128, and thinning as
16. For the optimisation of the acquisition function, to adapt to the discrete
setup, we use a local search method similar to previous literature involving
similar setup (Wan et al., 2021; Eriksson et al., 2021): at each search
iteration (after the initial randomly sampled points), we collect the
Pareto-optimal architectures up to this point. From this collection of
Pareto-optimal architectures, we perform a local search by evaluating the
acquisition function values of their neighbours, and move the current point to a
neighbour with a higher acquisition function value and this process is repeated
until convergence (which is a local minimum in terms of acquisition function),
or 100 evaluations in acquisition function value are reached. At each search
iteration, we restart this process 10 times and select the top candidate for the
query (in this case, fine-tuning) for the next iteration. For all BO
experiments, we use 200 total evaluations; given the noisy nature of the
problem, we use a relatively large number of random initialisation points (100)
to ensure that the search results are not overly sensitive to initialisation. We
use the same hyperparameter settings as described for all experiments conducted
in this paper.

Calculation of Fine-tuned Parameters. The uncased BERT_{base} model (109M) has
12 Transformer layers with a hidden dimension size of 768. The uncased
BERT_{large} model (335M) and RoBERTa_{large} (355M) both have 24 layers with a
hidden dimension size of 1, 024. For both SA and PA, their fine-tuned parameters
are computed by 2 × D_{adapter} × D_{h} × |l|, where D_{h} represents the
corresponding hidden dimension of the selected model, and |l| refers to the
total selected number of insertion layers. Similarly, we calculate the
fine-tuned parameters of PT by 2 × L_{PT} × D_{h} × |l|. Thus, the number of
fine-tuned parameters of the AUTOPEFT-found configurations is a summation of
individual PEFT modules' parameters. We report the default fine-tuned parameters
for the remaining PEFT modules as defined in their original papers.

## B Search Space and Discovered Architectures

We analyse the learned configurations in terms of the selected layers over
different parameter scales in Table 2. They show a common trend in selecting the
higher Transformer layers to insert the PEFT modules, which coincides with
previous findings that the higher layer contains richer task-specific
representations, and introducing PEFT modules to these layers is more efficient
than other layers. With the AUTOPEFT-found configurations reported in Table 2,
we hope future PEFT research and applications can benefit from the architecture
design similar to_{ AUTO}PEFT^{RTE} that we find the most transferable across
tasks.
